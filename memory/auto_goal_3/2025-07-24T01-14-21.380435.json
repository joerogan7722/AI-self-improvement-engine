{
  "cycle": null,
  "goal_id": "auto_goal_3",
  "description": "Improve architecture: Plugin architecture and parallel processing improvements",
  "current_code": "# File: src/ai_self_ext_engine/core/role_orchestrator.py\n\"\"\"\nAdvanced Role Orchestrator with Adaptive Role Sequencing and Meta-Learning\n\nThis orchestrator implements advanced feedback loops by:\n1. Dynamically determining optimal role execution order\n2. Learning from role performance patterns\n3. Adapting role configurations based on feedback\n4. Facilitating inter-role communication and coordination\n\"\"\"\n\nimport time\nimport logging\nfrom typing import Dict, List, Any, Optional, Tuple, Type\nfrom dataclasses import dataclass, field\nfrom collections import defaultdict, deque\nimport json\n\nfrom ai_self_ext_engine.core.role import Context, RoleFeedback, FeedbackType, RoleMetrics, AdaptiveRole\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass RoleExecutionPlan:\n    \"\"\"Plan for executing roles with adaptive sequencing.\"\"\"\n    role_sequence: List[str]\n    parallel_groups: List[List[str]] = field(default_factory=list)\n    conditional_roles: Dict[str, List[str]] = field(default_factory=dict)\n    estimated_duration: float = 0.0\n    confidence_score: float = 0.0\n\n\n@dataclass\nclass RolePerformanceSnapshot:\n    \"\"\"Snapshot of role performance metrics.\"\"\"\n    role_name: str\n    success_rate: float\n    avg_execution_time: float\n    effectiveness_score: float\n    feedback_quality: float\n    adaptability_score: float\n    timestamp: float\n\n\nclass RoleOrchestrator:\n    \"\"\"\n    Advanced orchestrator that manages role execution with adaptive feedback loops.\n    \n    Features:\n    - Dynamic role sequencing based on performance\n    - Meta-learning from execution patterns\n    - Feedback-driven role configuration\n    - Inter-role communication coordination\n    - Performance prediction and optimization\n    \"\"\"\n    \n    def __init__(self, max_history_size: int = 100):\n        self.registered_roles: Dict[str, AdaptiveRole] = {}\n        self.execution_history: deque = deque(maxlen=max_history_size)\n        self.role_performance_history: Dict[str, List[RolePerformanceSnapshot]] = defaultdict(list)\n        self.role_dependencies: Dict[str, List[str]] = {}\n        self.meta_learning_insights: List[Dict[str, Any]] = []\n        \n        # Adaptive parameters\n        self.learning_rate = 0.1\n        self.performance_weights = {\n            \"success_rate\": 0.3,\n            \"execution_time\": 0.2,\n            \"effectiveness\": 0.25,\n            \"feedback_quality\": 0.15,\n            \"adaptability\": 0.1\n        }\n        \n    def register_role(self, role: AdaptiveRole, dependencies: Optional[List[str]] = None):\n        \"\"\"Register a role with the orchestrator.\"\"\"\n        self.registered_roles[role.name] = role\n        self.role_dependencies[role.name] = dependencies or []\n        logger.info(f\"Registered role: {role.name} with dependencies: {dependencies}\")\n    \n    def execute_adaptive_workflow(self, context: Context, goal_hint: Optional[str] = None) -> Context:\n        \"\"\"\n        Execute an adaptive workflow that learns and improves over time.\n        \n        Args:\n            context: Current execution context\n            goal_hint: Optional hint about the goal type for better role selection\n        \n        Returns:\n            Updated context with results from all executed roles\n        \"\"\"\n        workflow_start_time = time.time()\n        \n        # 1. Generate adaptive execution plan\n        execution_plan = self._generate_adaptive_execution_plan(context, goal_hint)\n        logger.info(f\"Generated execution plan: {execution_plan.role_sequence}\")\n        \n        # 2. Execute roles according to the adaptive plan\n        updated_context = self._execute_planned_workflow(context, execution_plan)\n        \n        # 3. Analyze workflow performance and extract insights\n        workflow_metrics = self._analyze_workflow_performance(\n            context, updated_context, execution_plan, workflow_start_time\n        )\n        \n        # 4. Update meta-learning models\n        self._update_meta_learning(execution_plan, workflow_metrics, goal_hint)\n        \n        # 5. Generate system-wide feedback and improvements\n        self._generate_system_feedback(updated_context, workflow_metrics)\n        \n        return updated_context\n    \n    def _generate_adaptive_execution_plan(self, context: Context, goal_hint: Optional[str] = None) -> RoleExecutionPlan:\n        \"\"\"Generate an execution plan adapted to current context and historical performance.\"\"\"\n        \n        # Analyze context to determine role requirements\n        required_roles = self._determine_required_roles(context, goal_hint)\n        \n        # Get optimal sequencing based on performance history\n        optimal_sequence = self._compute_optimal_sequence(required_roles, context)\n        \n        # Identify parallel execution opportunities\n        parallel_groups = self._identify_parallel_opportunities(optimal_sequence)\n        \n        # Add conditional roles based on context\n        conditional_roles = self._determine_conditional_roles(context)\n        \n        # Estimate execution time and confidence\n        estimated_duration = self._estimate_execution_duration(optimal_sequence)\n        confidence_score = self._calculate_plan_confidence(optimal_sequence, context)\n        \n        plan = RoleExecutionPlan(\n            role_sequence=optimal_sequence,\n            parallel_groups=parallel_groups,\n            conditional_roles=conditional_roles,\n            estimated_duration=estimated_duration,\n            confidence_score=confidence_score\n        )\n        \n        return plan\n    \n    def _execute_planned_workflow(self, context: Context, plan: RoleExecutionPlan) -> Context:\n        \"\"\"Execute the planned workflow with adaptive monitoring.\"\"\"\n        \n        updated_context = context\n        executed_roles = []\n        \n        for role_name in plan.role_sequence:\n            if role_name not in self.registered_roles:\n                logger.warning(f\"Role {role_name} not registered, skipping\")\n                continue\n            \n            role = self.registered_roles[role_name]\n            \n            try:\n                # Pre-execution: Prepare role with latest feedback\n                self._prepare_role_for_execution(role, updated_context, executed_roles)\n                \n                # Execute role\n                role_start_time = time.time()\n                updated_context = role.run(updated_context)\n                execution_time = time.time() - role_start_time\n                \n                # Post-execution: Record performance and update feedback\n                self._record_role_execution(role_name, execution_time, True, updated_context)\n                executed_roles.append(role_name)\n                \n                # Check for early termination conditions\n                if updated_context.should_abort:\n                    logger.info(f\"Workflow terminated early after {role_name}\")\n                    break\n                \n                # Adaptive decision: Should we skip remaining roles?\n                if self._should_skip_remaining_roles(updated_context, plan, executed_roles):\n                    logger.info(\"Skipping remaining roles based on adaptive decision\")\n                    break\n                    \n            except Exception as e:\n                logger.error(f\"Role {role_name} failed: {e}\")\n                self._record_role_execution(role_name, 0, False, updated_context)\n                \n                # Decide whether to continue or abort based on failure type\n                if self._should_abort_on_failure(role_name, e, updated_context):\n                    logger.error(\"Aborting workflow due to critical role failure\")\n                    updated_context.should_abort = True\n                    break\n        \n        return updated_context\n    \n    def _determine_required_roles(self, context: Context, goal_hint: Optional[str] = None) -> List[str]:\n        \"\"\"Determine which roles are required based on context and goal.\"\"\"\n        \n        required_roles = []\n        \n        # Base roles always needed\n        base_roles = [\"ProblemIdentificationRole\", \"EnhancedRefineRole\", \"TestRole\", \"SelfReviewRole\"]\n        \n        # Goal-specific role selection\n        if goal_hint:\n            if \"refactor\" in goal_hint.lower():\n                required_roles.extend([\"SemanticRefactorRole\", \"CodeGraphRole\"])\n            elif \"test\" in goal_hint.lower():\n                required_roles.extend([\"TestGenerationRole\", \"CoverageAnalysisRole\"])\n            elif \"document\" in goal_hint.lower():\n                required_roles.extend([\"DocumentationRole\", \"DocValidationRole\"])\n        \n        # Context-driven role selection\n        if context.current_code and len(context.current_code) > 10000:\n            required_roles.append(\"CodeComplexityRole\")\n        \n        if len(context.learning_insights) > 10:\n            required_roles.append(\"InsightAnalysisRole\")\n        \n        # Combine and deduplicate\n        all_required = list(set(base_roles + required_roles))\n        \n        # Filter to only registered roles\n        available_roles = [role for role in all_required if role in self.registered_roles]\n        \n        logger.info(f\"Required roles: {available_roles}\")\n        return available_roles\n    \n    def _compute_optimal_sequence(self, required_roles: List[str], context: Context) -> List[str]:\n        \"\"\"Compute optimal role execution sequence based on performance history.\"\"\"\n        \n        # Start with dependency-based ordering\n        sequence = self._topological_sort(required_roles)\n        \n        # Apply performance-based optimizations\n        sequence = self._optimize_sequence_for_performance(sequence, context)\n        \n        return sequence\n    \n    def _topological_sort(self, roles: List[str]) -> List[str]:\n        \"\"\"Sort roles based on dependencies.\"\"\"\n        sorted_roles = []\n        visited = set()\n        temp_visited = set()\n        \n        def visit(role):\n            if role in temp_visited:\n                # Circular dependency detected, handle gracefully\n                logger.warning(f\"Circular dependency detected involving {role}\")\n                return\n            if role in visited:\n                return\n            \n            temp_visited.add(role)\n            for dependency in self.role_dependencies.get(role, []):\n                if dependency in roles:\n                    visit(dependency)\n            temp_visited.remove(role)\n            visited.add(role)\n            sorted_roles.append(role)\n        \n        for role in roles:\n            if role not in visited:\n                visit(role)\n        \n        return sorted_roles\n    \n    def _optimize_sequence_for_performance(self, sequence: List[str], context: Context) -> List[str]:\n        \"\"\"Optimize sequence based on historical performance patterns.\"\"\"\n        \n        # Calculate role performance scores\n        role_scores = {}\n        for role_name in sequence:\n            role_scores[role_name] = self._calculate_role_performance_score(role_name)\n        \n        # Apply learning-based optimizations\n        if len(self.execution_history) > 10:\n            sequence = self._apply_learned_optimizations(sequence, role_scores, context)\n        \n        return sequence\n    \n    def _calculate_role_performance_score(self, role_name: str) -> float:\n        \"\"\"Calculate comprehensive performance score for a role.\"\"\"\n        \n        history = self.role_performance_history.get(role_name, [])\n        if not history:\n            return 0.5  # Default score for new roles\n        \n        # Get recent performance data\n        recent_snapshots = history[-10:]  # Last 10 executions\n        \n        if not recent_snapshots:\n            return 0.5\n        \n        # Calculate weighted score\n        total_score = 0.0\n        total_weight = 0.0\n        \n        for snapshot in recent_snapshots:\n            age_factor = max(0.1, 1.0 - (time.time() - snapshot.timestamp) / (7 * 24 * 3600))  # Decay over week\n            \n            snapshot_score = (\n                snapshot.success_rate * self.performance_weights[\"success_rate\"] +\n                (1.0 - min(1.0, snapshot.avg_execution_time / 60.0)) * self.performance_weights[\"execution_time\"] +\n                snapshot.effectiveness_score * self.performance_weights[\"effectiveness\"] +\n                snapshot.feedback_quality * self.performance_weights[\"feedback_quality\"] +\n                snapshot.adaptability_score * self.performance_weights[\"adaptability\"]\n            )\n            \n            total_score += snapshot_score * age_factor\n            total_weight += age_factor\n        \n        return total_score / total_weight if total_weight > 0 else 0.5\n    \n    def _apply_learned_optimizations(self, sequence: List[str], role_scores: Dict[str, float], \n                                   context: Context) -> List[str]:\n        \"\"\"Apply optimizations learned from execution history.\"\"\"\n        \n        # Analyze successful execution patterns\n        successful_patterns = self._extract_successful_patterns()\n        \n        # Apply pattern-based reordering\n        optimized_sequence = sequence.copy()\n        \n        for pattern in successful_patterns:\n            if self._pattern_matches_current_context(pattern, context):\n                optimized_sequence = self._apply_pattern_optimization(optimized_sequence, pattern)\n        \n        return optimized_sequence\n    \n    def _extract_successful_patterns(self) -> List[Dict[str, Any]]:\n        \"\"\"Extract successful execution patterns from history.\"\"\"\n        \n        patterns = []\n        \n        # Analyze execution history for patterns\n        for execution in self.execution_history:\n            if execution.get(\"success\", False) and execution.get(\"effectiveness_score\", 0) > 0.8:\n                pattern = {\n                    \"sequence\": execution.get(\"role_sequence\", []),\n                    \"context_features\": execution.get(\"context_features\", {}),\n                    \"performance_metrics\": execution.get(\"performance_metrics\", {}),\n                    \"success_rate\": execution.get(\"success_rate\", 0)\n                }\n                patterns.append(pattern)\n        \n        # Sort by performance and return top patterns\n        patterns.sort(key=lambda p: p[\"success_rate\"], reverse=True)\n        return patterns[:5]  # Top 5 patterns\n    \n    def _record_role_execution(self, role_name: str, execution_time: float, \n                             success: bool, context: Context):\n        \"\"\"Record role execution for performance tracking.\"\"\"\n        \n        # Calculate performance metrics\n        effectiveness_score = self._calculate_effectiveness_score(role_name, context, success)\n        feedback_quality = self._assess_feedback_quality(role_name, context)\n        adaptability_score = self._assess_adaptability(role_name, context)\n        \n        snapshot = RolePerformanceSnapshot(\n            role_name=role_name,\n            success_rate=1.0 if success else 0.0,\n            avg_execution_time=execution_time,\n            effectiveness_score=effectiveness_score,\n            feedback_quality=feedback_quality,\n            adaptability_score=adaptability_score,\n            timestamp=time.time()\n        )\n        \n        self.role_performance_history[role_name].append(snapshot)\n        \n        # Keep only recent history\n        if len(self.role_performance_history[role_name]) > 50:\n            self.role_performance_history[role_name] = self.role_performance_history[role_name][-50:]\n    \n    def _update_meta_learning(self, execution_plan: RoleExecutionPlan, \n                            workflow_metrics: Dict[str, Any], goal_hint: Optional[str]):\n        \"\"\"Update meta-learning models based on execution results.\"\"\"\n        \n        insight = {\n            \"timestamp\": time.time(),\n            \"execution_plan\": {\n                \"sequence\": execution_plan.role_sequence,\n                \"estimated_duration\": execution_plan.estimated_duration,\n                \"confidence_score\": execution_plan.confidence_score\n            },\n            \"actual_metrics\": workflow_metrics,\n            \"goal_hint\": goal_hint,\n            \"effectiveness\": workflow_metrics.get(\"overall_effectiveness\", 0.0),\n            \"adaptation_score\": workflow_metrics.get(\"adaptation_score\", 0.0)\n        }\n        \n        self.meta_learning_insights.append(insight)\n        \n        # Keep only recent insights\n        if len(self.meta_learning_insights) > 100:\n            self.meta_learning_insights = self.meta_learning_insights[-100:]\n        \n        # Update learning parameters based on insights\n        self._adjust_learning_parameters(insight)\n    \n    def _generate_system_feedback(self, context: Context, workflow_metrics: Dict[str, Any]):\n        \"\"\"Generate system-wide feedback for continuous improvement.\"\"\"\n        \n        # Generate feedback about workflow effectiveness\n        if workflow_metrics.get(\"overall_effectiveness\", 0.0) > 0.8:\n            feedback = RoleFeedback(\n                from_role=\"RoleOrchestrator\",\n                to_role=None,  # Broadcast\n                feedback_type=FeedbackType.SUCCESS,\n                content={\n                    \"message\": \"High-performing workflow configuration identified\",\n                    \"workflow_metrics\": workflow_metrics,\n                    \"recommendation\": \"Consider this as a template for similar goals\"\n                },\n                timestamp=time.time()\n            )\n            context.add_feedback(feedback)\n        \n        # Generate improvement suggestions\n        improvement_areas = self._identify_improvement_areas(workflow_metrics)\n        for area, suggestion in improvement_areas.items():\n            feedback = RoleFeedback(\n                from_role=\"RoleOrchestrator\",\n                to_role=area,\n                feedback_type=FeedbackType.STRATEGY,\n                content={\n                    \"improvement_area\": area,\n                    \"suggestion\": suggestion,\n                    \"priority\": \"medium\"\n                },\n                timestamp=time.time()\n            )\n            context.add_feedback(feedback)\n    \n    def get_orchestrator_insights(self) -> Dict[str, Any]:\n        \"\"\"Get insights about orchestrator performance and learning.\"\"\"\n        \n        return {\n            \"registered_roles\": list(self.registered_roles.keys()),\n            \"execution_count\": len(self.execution_history),\n            \"meta_learning_insights\": len(self.meta_learning_insights),\n            \"role_performance_summary\": {\n                role: {\n                    \"avg_score\": self._calculate_role_performance_score(role),\n                    \"execution_count\": len(history)\n                }\n                for role, history in self.role_performance_history.items()\n            },\n            \"learning_rate\": self.learning_rate,\n            \"top_performing_sequences\": self._get_top_performing_sequences()\n        }\n    \n    # Helper methods (simplified implementations)\n    \n    def _identify_parallel_opportunities(self, sequence: List[str]) -> List[List[str]]:\n        \"\"\"Identify roles that can be executed in parallel.\"\"\"\n        # Simplified: Return empty for now, would analyze dependencies\n        return []\n    \n    def _determine_conditional_roles(self, context: Context) -> Dict[str, List[str]]:\n        \"\"\"Determine roles that should be executed based on conditions.\"\"\"\n        # Simplified implementation\n        return {}\n    \n    def _estimate_execution_duration(self, sequence: List[str]) -> float:\n        \"\"\"Estimate total execution duration for sequence.\"\"\"\n        total_time = 0.0\n        for role_name in sequence:\n            history = self.role_performance_history.get(role_name, [])\n            if history:\n                avg_time = sum(s.avg_execution_time for s in history[-5:]) / min(5, len(history))\n                total_time += avg_time\n            else:\n                total_time += 30.0  # Default estimate\n        return total_time\n    \n    def _calculate_plan_confidence(self, sequence: List[str], context: Context) -> float:\n        \"\"\"Calculate confidence in the execution plan.\"\"\"\n        confidence_factors = []\n        \n        for role_name in sequence:\n            role_score = self._calculate_role_performance_score(role_name)\n            confidence_factors.append(role_score)\n        \n        return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5\n    \n    def _prepare_role_for_execution(self, role: AdaptiveRole, context: Context, executed_roles: List[str]):\n        \"\"\"Prepare role for execution with latest context and feedback.\"\"\"\n        # This could involve updating role configuration based on context\n        pass\n    \n    def _should_skip_remaining_roles(self, context: Context, plan: RoleExecutionPlan, executed_roles: List[str]) -> bool:\n        \"\"\"Determine if remaining roles should be skipped.\"\"\"\n        # Simplified: Check if goal is achieved\n        return context.accepted and len(executed_roles) >= len(plan.role_sequence) * 0.7\n    \n    def _should_abort_on_failure(self, role_name: str, error: Exception, context: Context) -> bool:\n        \"\"\"Determine if workflow should abort on role failure.\"\"\"\n        # Critical roles that should abort workflow if they fail\n        critical_roles = [\"ProblemIdentificationRole\"]\n        return role_name in critical_roles\n    \n    def _analyze_workflow_performance(self, initial_context: Context, final_context: Context,\n                                    plan: RoleExecutionPlan, start_time: float) -> Dict[str, Any]:\n        \"\"\"Analyze overall workflow performance.\"\"\"\n        return {\n            \"total_duration\": time.time() - start_time,\n            \"roles_executed\": len(plan.role_sequence),\n            \"overall_effectiveness\": 0.8,  # Would calculate based on results\n            \"adaptation_score\": 0.7,  # Would calculate based on adaptations made\n            \"goal_achievement\": final_context.accepted\n        }\n    \n    def _adjust_learning_parameters(self, insight: Dict[str, Any]):\n        \"\"\"Adjust learning parameters based on execution insights.\"\"\"\n        # Simplified: Adjust learning rate based on effectiveness\n        effectiveness = insight.get(\"effectiveness\", 0.5)\n        if effectiveness > 0.8:\n            self.learning_rate = min(0.2, self.learning_rate * 1.05)\n        elif effectiveness < 0.5:\n            self.learning_rate = max(0.05, self.learning_rate * 0.95)\n    \n    def _identify_improvement_areas(self, metrics: Dict[str, Any]) -> Dict[str, str]:\n        \"\"\"Identify areas for improvement.\"\"\"\n        areas = {}\n        if metrics.get(\"total_duration\", 0) > metrics.get(\"estimated_duration\", 0) * 1.5:\n            areas[\"performance\"] = \"Focus on execution speed optimization\"\n        return areas\n    \n    def _calculate_effectiveness_score(self, role_name: str, context: Context, success: bool) -> float:\n        \"\"\"Calculate role effectiveness score.\"\"\"\n        base_score = 0.8 if success else 0.2\n        # Could factor in context changes, feedback quality, etc.\n        return base_score\n    \n    def _assess_feedback_quality(self, role_name: str, context: Context) -> float:\n        \"\"\"Assess quality of feedback generated by role.\"\"\"\n        # Simplified: Count feedback items generated\n        role_feedback = [fb for fb in context.feedback_queue if fb.from_role == role_name]\n        return min(1.0, len(role_feedback) * 0.2)\n    \n    def _assess_adaptability(self, role_name: str, context: Context) -> float:\n        \"\"\"Assess how well role adapted to context and feedback.\"\"\"\n        # Simplified: Return default score\n        return 0.7\n    \n    def _pattern_matches_current_context(self, pattern: Dict[str, Any], context: Context) -> bool:\n        \"\"\"Check if a pattern matches current context.\"\"\"\n        # Simplified pattern matching\n        return True  # Would implement actual pattern matching logic\n    \n    def _apply_pattern_optimization(self, sequence: List[str], pattern: Dict[str, Any]) -> List[str]:\n        \"\"\"Apply pattern-based optimization to sequence.\"\"\"\n        # Simplified: Return original sequence\n        return sequence\n    \n    def _get_top_performing_sequences(self) -> List[Dict[str, Any]]:\n        \"\"\"Get top performing role sequences.\"\"\"\n        # Simplified implementation\n        return []\n\n",
  "todos": [
    {
      "file_path": "src/ai_self_ext_engine/core/role_orchestrator.py",
      "change_type": "modify",
      "description": "Refactor role orchestration logic to support concurrent execution of roles or sub-tasks using asyncio for improved throughput and responsiveness."
    }
  ],
  "patch": "--- a/src/ai_self_ext_engine/core/role_orchestrator.py\n+++ b/src/ai_self_ext_engine/core/role_orchestrator.py\n@@ -10,12 +10,13 @@\n 4. Facilitating inter-role communication and coordination\n \"\"\"\n \n+import asyncio\n import time\n import logging\n-from typing import Dict, List, Any, Optional, Tuple, Type\n+from typing import Dict, List, Any, Optional, Tuple, Type, Union\n from dataclasses import dataclass, field\n from collections import defaultdict, deque\n-import json\n+# import json # Not used\n \n from ai_self_ext_engine.core.role import Context, RoleFeedback, FeedbackType, RoleMetrics, AdaptiveRole\n \n@@ -25,8 +26,7 @@\n @dataclass\n class RoleExecutionPlan:\n     \"\"\"Plan for executing roles with adaptive sequencing.\"\"\"\n-    role_sequence: List[str]\n-    parallel_groups: List[List[str]] = field(default_factory=list)\n+    execution_stages: List[Union[str, List[str]]]\n     conditional_roles: Dict[str, List[str]] = field(default_factory=dict)\n     estimated_duration: float = 0.0\n     confidence_score: float = 0.0\n@@ -67,10 +67,11 @@\n     def register_role(self, role: AdaptiveRole, dependencies: Optional[List[str]] = None):\n         \"\"\"Register a role with the orchestrator.\"\"\"\n         self.registered_roles[role.name] = role\n-        self.role_dependencies[role.name] = dependencies or []\n-        logger.info(f\"Registered role: {role.name} with dependencies: {dependencies}\")\n-    \n-    def execute_adaptive_workflow(self, context: Context, goal_hint: Optional[str] = None) -> Context:\n+        valid_dependencies = [dep for dep in (dependencies or []) if dep in self.registered_roles]\n+        self.role_dependencies[role.name] = valid_dependencies\n+        logger.info(f\"Registered role: {role.name} with dependencies: {valid_dependencies}\")\n+    \n+    async def execute_adaptive_workflow(self, context: Context, goal_hint: Optional[str] = None) -> Context:\n         \"\"\"\n         Execute an adaptive workflow that learns and improves over time.\n         \n@@ -83,10 +84,10 @@\n         workflow_start_time = time.time()\n         \n         # 1. Generate adaptive execution plan\n-        execution_plan = self._generate_adaptive_execution_plan(context, goal_hint)\n-        logger.info(f\"Generated execution plan: {execution_plan.role_sequence}\")\n-        \n-        # 2. Execute roles according to the adaptive plan\n-        updated_context = self._execute_planned_workflow(context, execution_plan)\n+        execution_plan = self._generate_adaptive_execution_plan(context, goal_hint) # This now generates stages\n+        logger.info(f\"Generated execution plan with stages: {execution_plan.execution_stages}\")\n+        \n+        # 2. Execute roles according to the adaptive plan (now async)\n+        updated_context = await self._execute_planned_workflow(context, execution_plan)\n         \n         # 3. Analyze workflow performance and extract insights\n         workflow_metrics = self._analyze_workflow_performance(\n@@ -107,17 +108,13 @@\n         # Get optimal sequencing based on performance history\n-        optimal_sequence = self._compute_optimal_sequence(required_roles, context)\n-        \n-        # Identify parallel execution opportunities\n-        parallel_groups = self._identify_parallel_opportunities(optimal_sequence)\n+        execution_stages = self._compute_optimal_sequence(required_roles, context) # Returns stages\n         \n         # Add conditional roles based on context\n         conditional_roles = self._determine_conditional_roles(context)\n         \n         # Estimate execution time and confidence\n-        estimated_duration = self._estimate_execution_duration(optimal_sequence)\n-        confidence_score = self._calculate_plan_confidence(optimal_sequence, context)\n+        estimated_duration = self._estimate_execution_duration(execution_stages)\n+        confidence_score = self._calculate_plan_confidence(execution_stages, context)\n         \n         plan = RoleExecutionPlan(\n-            role_sequence=optimal_sequence,\n-            parallel_groups=parallel_groups,\n+            execution_stages=execution_stages, # Changed from role_sequence and parallel_groups\n             conditional_roles=conditional_roles,\n             estimated_duration=estimated_duration,\n             confidence_score=confidence_score\n         )\n         \n         return plan\n     \n-    def _execute_planned_workflow(self, context: Context, plan: RoleExecutionPlan) -> Context:\n-        \"\"\"Execute the planned workflow with adaptive monitoring.\"\"\"\n+    async def _execute_planned_workflow(self, context: Context, plan: RoleExecutionPlan) -> Context:\n+        \"\"\"\n+        Execute the planned workflow with adaptive monitoring, supporting async stages.\n+        Context object is assumed to be mutable and changes made by roles (especially in parallel)\n+        are intended to be cumulative or non-conflicting.\n+        \"\"\"\n         \n         updated_context = context\n-        executed_roles = []\n-        \n-        for role_name in plan.role_sequence:\n-            if role_name not in self.registered_roles:\n-                logger.warning(f\"Role {role_name} not registered, skipping\")\n-                continue\n-            \n-            role = self.registered_roles[role_name]\n-            \n-            try:\n-                # Pre-execution: Prepare role with latest feedback\n-                self._prepare_role_for_execution(role, updated_context, executed_roles)\n-                \n-                # Execute role\n-                role_start_time = time.time()\n-                updated_context = role.run(updated_context)\n-                execution_time = time.time() - role_start_time\n-                \n-                # Post-execution: Record performance and update feedback\n-                self._record_role_execution(role_name, execution_time, True, updated_context)\n-                executed_roles.append(role_name)\n-                \n-                # Check for early termination conditions\n-                if updated_context.should_abort:\n-                    logger.info(f\"Workflow terminated early after {role_name}\")\n-                    break\n-                \n-                # Adaptive decision: Should we skip remaining roles?\n-                if self._should_skip_remaining_roles(updated_context, plan, executed_roles):\n-                    logger.info(\"Skipping remaining roles based on adaptive decision\")\n-                    break\n-                    \n-            except Exception as e:\n-                logger.error(f\"Role {role_name} failed: {e}\")\n-                self._record_role_execution(role_name, 0, False, updated_context)\n-                \n-                # Decide whether to continue or abort based on failure type\n-                if self._should_abort_on_failure(role_name, e, updated_context):\n-                    logger.error(\"Aborting workflow due to critical role failure\")\n-                    updated_context.should_abort = True\n-                    break\n+        executed_roles = set() # Track globally executed roles\n+        \n+        for stage in plan.execution_stages:\n+            if isinstance(stage, str): # Single role execution stage\n+                role_name = stage\n+                if role_name not in self.registered_roles:\n+                    logger.warning(f\"Role {role_name} not registered, skipping\")\n+                    continue\n+                \n+                role = self.registered_roles[role_name]\n+                \n+                try:\n+                    # Pre-execution: Prepare role with latest feedback\n+                    self._prepare_role_for_execution(role, updated_context, list(executed_roles))\n+                    \n+                    # Execute role\n+                    role_start_time = time.time()\n+                    # Assume role.run modifies `updated_context` in place, or returns the same object.\n+                    _ = await role.run(updated_context) # AWAIT HERE\n+                    execution_time = time.time() - role_start_time\n+                    \n+                    # Post-execution: Record performance and update feedback\n+                    self._record_role_execution(role_name, execution_time, True, updated_context)\n+                    executed_roles.add(role_name)\n+                    \n+                    # Check for early termination conditions\n+                    if updated_context.should_abort:\n+                        logger.info(f\"Workflow terminated early after {role_name}\")\n+                        break\n+                    \n+                    # Adaptive decision: Should we skip remaining roles?\n+                    if self._should_skip_remaining_roles(updated_context, plan, list(executed_roles)):\n+                        logger.info(\"Skipping remaining roles based on adaptive decision\")\n+                        break\n+                        \n+                except Exception as e:\n+                    logger.error(f\"Role {role_name} failed: {e}\")\n+                    self._record_role_execution(role_name, 0, False, updated_context)\n+                    \n+                    # Decide whether to continue or abort based on failure type\n+                    if self._should_abort_on_failure(role_name, e, updated_context):\n+                        logger.error(\"Aborting workflow due to critical role failure\")\n+                        updated_context.should_abort = True\n+                        break\n+            \n+            elif isinstance(stage, list): # Parallel roles execution stage\n+                tasks = []\n+                stage_role_names = [] # Keep track of role names within this parallel stage for result mapping\n+                \n+                for role_name in stage:\n+                    if role_name not in self.registered_roles:\n+                        logger.warning(f\"Role {role_name} not registered in parallel group, skipping\")\n+                        continue\n+                    \n+                    role = self.registered_roles[role_name]\n+                    \n+                    self._prepare_role_for_execution(role, updated_context, list(executed_roles))\n+\n+                    # Wrap role execution in a coroutine that handles logging, metrics, and errors\n+                    async def run_single_role_task(r_name: str, r_instance: AdaptiveRole, current_context: Context) -> Tuple[str, float, bool]:\n+                        try:\n+                            start_time = time.time()\n+                            _ = await r_instance.run(current_context) # AWAIT and assume in-place context modification\n+                            exec_time = time.time() - start_time\n+                            return r_name, exec_time, True\n+                        except Exception as e:\n+                            logger.error(f\"Parallel role {r_name} failed: {e}\")\n+                            return r_name, 0, False\n+                    \n+                    tasks.append(run_single_role_task(role_name, role, updated_context))\n+                    stage_role_names.append(role_name)\n+\n+                if tasks:\n+                    results = await asyncio.gather(*tasks, return_exceptions=True) # return_exceptions to allow other tasks to complete\n+                    \n+                    for i, res in enumerate(results):\n+                        role_name = stage_role_names[i]\n+                        if isinstance(res, Exception):\n+                            logger.error(f\"Parallel role {role_name} execution resulted in an exception: {res}\")\n+                            self._record_role_execution(role_name, 0, False, updated_context)\n+                            if self._should_abort_on_failure(role_name, res, updated_context):\n+                                logger.error(\"Aborting workflow due to critical parallel role failure in stage\")\n+                                updated_context.should_abort = True\n+                                break # Break from processing this stage's results\n+                        else:\n+                            r_name_res, exec_time, success = res\n+                            self._record_role_execution(r_name_res, exec_time, success, updated_context)\n+                            executed_roles.add(r_name_res)\n+                \n+                if updated_context.should_abort: # Check if any parallel role caused abort\n+                    break\n+                \n+                if self._should_skip_remaining_roles(updated_context, plan, list(executed_roles)):\n+                    logger.info(\"Skipping remaining roles based on adaptive decision after parallel stage\")\n+                    break\n         \n         return updated_context\n     \n@@ -194,15 +236,15 @@\n     def _compute_optimal_sequence(self, required_roles: List[str], context: Context) -> List[str]:\n         \"\"\"Compute optimal role execution sequence based on performance history.\"\"\"\n         \n-        # Start with dependency-based ordering\n-        sequence = self._topological_sort(required_roles)\n-        \n-        # Apply performance-based optimizations\n-        sequence = self._optimize_sequence_for_performance(sequence, context)\n-        \n-        return sequence\n-    \n-    def _topological_sort(self, roles: List[str]) -> List[str]:\n+        # 1. Start with dependency-based ordering (linear sequence first)\n+        linear_sequence = self._topological_sort(required_roles)\n+        \n+        # 2. Apply performance-based optimizations (still on linear sequence)\n+        optimized_linear_sequence = self._optimize_sequence_for_performance(linear_sequence, context)\n+        \n+        # 3. Convert the optimized linear sequence into execution stages (sequential or parallel groups)\n+        execution_stages = self._create_execution_stages_from_sequence(optimized_linear_sequence)\n+        \n+        return execution_stages\n+    \n+    def _topological_sort(self, roles: List[str]) -> List[str]: # This method is fine, it produces a linear sort\n         \"\"\"Sort roles based on dependencies.\"\"\"\n         sorted_roles = []\n         visited = set()\n@@ -214,10 +256,10 @@\n                 return\n             if role in visited:\n                 return\n-            \n+\n             temp_visited.add(role)\n             for dependency in self.role_dependencies.get(role, []):\n-                if dependency in roles:\n+                if dependency in roles and dependency not in visited: # Ensure dependency is in the current set of roles and not already processed\n                     visit(dependency)\n             temp_visited.remove(role)\n             visited.add(role)\n@@ -226,6 +268,66 @@\n         for role in roles:\n             if role not in visited:\n                 visit(role)\n+        \n+        # The standard topological sort can produce different valid orders.\n+        # To make it more deterministic and follow typical execution,\n+        # sort the roles added to `sorted_roles` if there are multiple choices.\n+        # For simplicity, returning as is, but this might influence parallel stage grouping.\n         \n         return sorted_roles\n     \n     def _optimize_sequence_for_performance(self, sequence: List[str], context: Context) -> List[str]:\n         \"\"\"Optimize sequence based on historical performance patterns.\"\"\"\n-        \n+        # This part remains similar, it optimizes the linear sequence.\n         # Calculate role performance scores\n         role_scores = {}\n         for role_name in sequence:\n             role_scores[role_name] = self._calculate_role_performance_score(role_name)\n         \n         # Apply learning-based optimizations\n+        # Current _apply_learned_optimizations is a placeholder. It should actually reorder 'sequence'.\n         if len(self.execution_history) > 10:\n             sequence = self._apply_learned_optimizations(sequence, role_scores, context)\n         \n         return sequence\n     \n+    def _create_execution_stages_from_sequence(self, sequence: List[str]) -> List[Union[str, List[str]]]:\n+        \"\"\"\n+        Converts a linear, topologically sorted sequence of roles into execution stages,\n+        grouping independent roles into parallel stages (levels).\n+        \"\"\"\n+        if not sequence:\n+            return []\n+\n+        # Build graph and calculate in-degrees for roles *within this sequence*\n+        graph = defaultdict(list) # Adjacency list: predecessor -> [successors]\n+        in_degree = {role_name: 0 for role_name in sequence}\n+        \n+        for role_name in sequence:\n+            for dependency in self.role_dependencies.get(role_name, []):\n+                if dependency in sequence: # Only consider dependencies within the current `sequence`\n+                    # `role_name` depends on `dependency`, so `dependency` is a predecessor of `role_name`.\n+                    graph[dependency].append(role_name)\n+                    in_degree[role_name] += 1\n+        \n+        # Initialize queue with roles that have no *internal* dependencies in this subgraph\n+        ready_queue = deque(sorted([role for role in sequence if in_degree[role] == 0])) # Sort for deterministic stages\n+        \n+        execution_stages: List[Union[str, List[str]]] = []\n+        \n+        while ready_queue:\n+            current_stage_roles = sorted(list(ready_queue)) # Sort for deterministic output of parallel group\n+            ready_queue.clear() # Process all roles in this \"level\"\n+\n+            if len(current_stage_roles) > 1:\n+                execution_stages.append(current_stage_roles)\n+            else:\n+                execution_stages.append(current_stage_roles[0])\n+\n+            for role_name in current_stage_roles:\n+                for successor in graph[role_name]:\n+                    in_degree[successor] -= 1\n+                    if in_degree[successor] == 0:\n+                        ready_queue.append(successor)\n+        \n+        # Check for cycles or unprocessible roles (shouldn't happen with a valid topological sort as input)\n+        unprocessed_roles = [role for role, degree in in_degree.items() if degree > 0]\n+        if unprocessed_roles:\n+            logger.warning(f\"Circular dependencies detected or some roles could not be scheduled for parallelization: {unprocessed_roles}. Adding them as sequential fallback.\")\n+            for role in unprocessed_roles:\n+                execution_stages.append(role) # Add as individual sequential stages\n+\n+        logger.debug(f\"Generated execution stages from linear sequence: {execution_stages}\")\n+        return execution_stages\n+\n     def _calculate_role_performance_score(self, role_name: str) -> float:\n         \"\"\"Calculate comprehensive performance score for a role.\"\"\"\n         \n@@ -345,26 +445,39 @@\n     \n     # Helper methods (simplified implementations)\n     \n-    def _identify_parallel_opportunities(self, sequence: List[str]) -> List[List[str]]:\n-        \"\"\"Identify roles that can be executed in parallel.\"\"\"\n-        # Simplified: Return empty for now, would analyze dependencies\n-        return []\n+    # _identify_parallel_opportunities method removed as its logic is integrated into\n+    # _create_execution_stages_from_sequence and RoleExecutionPlan now directly holds stages.\n     \n     def _determine_conditional_roles(self, context: Context) -> Dict[str, List[str]]:\n         \"\"\"Determine roles that should be executed based on conditions.\"\"\"\n         # Simplified implementation\n         return {}\n     \n-    def _estimate_execution_duration(self, sequence: List[str]) -> float:\n-        \"\"\"Estimate total execution duration for sequence.\"\"\"\n+    def _estimate_execution_duration(self, execution_stages: List[Union[str, List[str]]]) -> float:\n+        \"\"\"Estimate total execution duration for execution stages.\"\"\"\n         total_time = 0.0\n-        for role_name in sequence:\n-            history = self.role_performance_history.get(role_name, [])\n-            if history:\n-                avg_time = sum(s.avg_execution_time for s in history[-5:]) / min(5, len(history))\n-                total_time += avg_time\n-            else:\n-                total_time += 30.0  # Default estimate\n+        for stage in execution_stages:\n+            if isinstance(stage, str): # Sequential stage\n+                role_name = stage\n+                history = self.role_performance_history.get(role_name, [])\n+                if history:\n+                    avg_time = sum(s.avg_execution_time for s in history[-5:]) / min(5, len(history))\n+                    total_time += avg_time\n+                else:\n+                    total_time += 30.0  # Default estimate for unknown roles\n+            elif isinstance(stage, list): # Parallel stage\n+                max_stage_time = 0.0\n+                for role_name in stage:\n+                    history = self.role_performance_history.get(role_name, [])\n+                    if history:\n+                        avg_time = sum(s.avg_execution_time for s in history[-5:]) / min(5, len(history))\n+                        max_stage_time = max(max_stage_time, avg_time)\n+                    else:\n+                        max_stage_time = max(max_stage_time, 30.0) # Default estimate\n+                total_time += max_stage_time # Add the duration of the longest role in the parallel stage\n+            \n+        # Add a small overhead for coordination if needed, but keeping it simple for now\n+\n         return total_time\n     \n-    def _calculate_plan_confidence(self, sequence: List[str], context: Context) -> float:\n+    def _calculate_plan_confidence(self, execution_stages: List[Union[str, List[str]]], context: Context) -> float:\n         \"\"\"Calculate confidence in the execution plan.\"\"\"\n         confidence_factors = []\n         \n-        for role_name in sequence:\n-            role_score = self._calculate_role_performance_score(role_name)\n-            confidence_factors.append(role_score)\n+        for stage in execution_stages:\n+            roles_in_stage = [stage] if isinstance(stage, str) else stage\n+            for role_name in roles_in_stage:\n+                role_score = self._calculate_role_performance_score(role_name)\n+                confidence_factors.append(role_score)\n         \n         return sum(confidence_factors) / len(confidence_factors) if confidence_factors else 0.5\n     \n@@ -373,9 +486,17 @@\n         # This could involve updating role configuration based on context\n         pass\n     \n-    def _should_skip_remaining_roles(self, context: Context, plan: RoleExecutionPlan, executed_roles: List[str]) -> bool:\n+    def _should_skip_remaining_roles(self, context: Context, plan: RoleExecutionPlan, executed_roles: List[str]) -> bool: # executed_roles is now a list conversion from set\n         \"\"\"Determine if remaining roles should be skipped.\"\"\"\n         # Simplified: Check if goal is achieved\n-        return context.accepted and len(executed_roles) >= len(plan.role_sequence) * 0.7\n+\n+        # Calculate total roles from stages for comparison\n+        total_roles_in_plan = 0\n+        for stage in plan.execution_stages:\n+            if isinstance(stage, str):\n+                total_roles_in_plan += 1\n+            else: # list of roles\n+                total_roles_in_plan += len(stage)\n+        \n+        return context.accepted and len(executed_roles) >= total_roles_in_plan * 0.7\n     \n     def _should_abort_on_failure(self, role_name: str, error: Exception, context: Context) -> bool:\n         \"\"\"Determine if workflow should abort on role failure.\"\"\"",
  "test_results": null,
  "accepted": false,
  "should_abort": true,
  "metadata": {}
}