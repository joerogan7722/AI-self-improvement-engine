{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -0,0 +1,30 @@\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+class Engine:\n+    def __init__(self):\n+        \"\"\"Initializes the Engine with necessary components.\"\"\"\n+        # Placeholder for initializing role-specific modules or configurations\n+        logger.info(\"Engine initialized.\")\n+\n+    def problem_identification(self, context: dict) -> dict:\n+        \"\"\"Critique phase: identifies problems or areas for improvement.\"\"\"\n+        logger.info(\"Executing problem identification (critique) phase.\")\n+        # TODO: Implement actual problem identification logic\n+        return {\"problems_identified\": [\"Initial placeholder problem\"]}\n+\n+    def refine(self, context: dict) -> dict:\n+        \"\"\"Refine phase: generates solutions or refinements based on identified problems.\"\"\"\n+        logger.info(\"Executing refine phase.\")\n+        # TODO: Implement actual refinement logic\n+        return {\"refined_output\": \"Placeholder refined output\"}\n+\n+    def test(self, context: dict) -> dict:\n+        \"\"\"Test phase: validates the refined output.\"\"\"\n+        logger.info(\"Executing test phase.\")\n+        # TODO: Implement actual testing logic\n+        return {\"test_results\": \"Placeholder test results: PASSED\"}\n+\n+    def self_review(self, context: dict) -> dict:\n+        \"\"\"Self-review phase: evaluates the entire cycle's outcome.\"\"\"\n+        logger.info(\"Executing self-review phase.\")\n+        # TODO: Implement actual self-review logic\n+        return {\"self_review_feedback\": \"Placeholder self-review feedback\"}\n+\n+    def run_cycle(self, initial_context: dict = None) -> dict:\n+        \"\"\"\n+        Orchestrates the full critique-refine-test-self-review cycle.\n+        \"\"\"\n+        current_context = initial_context if initial_context is not None else {}\n+        logger.info(\"Starting a new execution cycle.\")\n+\n+        # 1. Problem Identification (Critique)\n+        problems = self.problem_identification(current_context)\n+        current_context.update(problems)\n+        logger.debug(f\"Problems identified: {problems}\")\n+\n+        # 2. Refine\n+        refined_output = self.refine(current_context)\n+        current_context.update(refined_output)\n+        logger.debug(f\"Refined output: {refined_output}\")\n+\n+        # 3. Test\n+        test_results = self.test(current_context)\n+        current_context.update(test_results)\n+        logger.debug(f\"Test results: {test_results}\")\n+\n+        # 4. Self-Review\n+        self_review_feedback = self.self_review(current_context)\n+        current_context.update(self_review_feedback)\n+        logger.debug(f\"Self-review feedback: {self_review_feedback}\")\n+\n+        logger.info(\"Execution cycle completed.\")\n+        return current_context\n--- /dev/null\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -0,0 +1,30 @@\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+class Engine:\n+    def __init__(self):\n+        \"\"\"Initializes the Engine with necessary components.\"\"\"\n+        # Placeholder for initializing role-specific modules or configurations\n+        logger.info(\"Engine initialized.\")\n+\n+    def problem_identification(self, context: dict) -> dict:\n+        \"\"\"Critique phase: identifies problems or areas for improvement.\"\"\"\n+        logger.info(\"Executing problem identification (critique) phase.\")\n+        # TODO: Implement actual problem identification logic\n+        return {\"problems_identified\": [\"Initial placeholder problem\"]}\n+\n+    def refine(self, context: dict) -> dict:\n+        \"\"\"Refine phase: generates solutions or refinements based on identified problems.\"\"\"\n+        logger.info(\"Executing refine phase.\")\n+        # TODO: Implement actual refinement logic\n+        return {\"refined_output\": \"Placeholder refined output\"}\n+\n+    def test(self, context: dict) -> dict:\n+        \"\"\"Test phase: validates the refined output.\"\"\"\n+        logger.info(\"Executing test phase.\")\n+        # TODO: Implement actual testing logic\n+        return {\"test_results\": \"Placeholder test results: PASSED\"}\n+\n+    def self_review(self, context: dict) -> dict:\n+        \"\"\"Self-review phase: evaluates the entire cycle's outcome.\"\"\"\n+        logger.info(\"Executing self-review phase.\")\n+        # TODO: Implement actual self-review logic\n+        return {\"self_review_feedback\": \"Placeholder self-review feedback\"}\n+\n+    def run_cycle(self, initial_context: dict = None) -> dict:\n+        \"\"\"\n+        Orchestrates the full critique-refine-test-self-review cycle.\n+        \"\"\"\n+        current_context = initial_context if initial_context is not None else {}\n+        logger.info(\"Starting a new execution cycle.\")\n+\n+        # 1. Problem Identification (Critique)\n+        problems = self.problem_identification(current_context)\n+        current_context.update(problems)\n+        logger.debug(f\"Problems identified: {problems}\")\n+\n+        # 2. Refine\n+        refined_output = self.refine(current_context)\n+        current_context.update(refined_output)\n+        logger.debug(f\"Refined output: {refined_output}\")\n+\n+        # 3. Test\n+        test_results = self.test(current_context)\n+        current_context.update(test_results)\n+        logger.debug(f\"Test results: {test_results}\")\n+\n+        # 4. Self-Review\n+        self_review_feedback = self.self_review(current_context)\n+        current_context.update(self_review_feedback)\n+        logger.debug(f\"Self-review feedback: {self_review_feedback}\")\n+\n+        logger.info(\"Execution cycle completed.\")\n+        return current_context", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- /dev/null\n+++ b/src/ai_self_ext_engine/core/plugin.py\n@@ -0,0 +1,24 @@\n+from abc import ABC, abstractmethod\n+\n+class Plugin(ABC):\n+    \"\"\"\n+    Abstract Base Class for all AI Self-Extending Engine plugins.\n+    Defines the standard interface that all plugins must implement.\n+    \"\"\"\n+\n+    @abstractmethod\n+    def name(self) -> str:\n+        \"\"\"Returns the name of the plugin.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def version(self) -> str:\n+        \"\"\"Returns the version of the plugin.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def initialize(self, config: dict):\n+        \"\"\"Initializes the plugin with given configuration.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def execute(self, context: dict) -> dict:\n+        \"\"\"Executes the core functionality of the plugin given a context.\"\"\"\n+        pass\n--- /dev/null\n+++ b/src/ai_self_ext_engine/core/plugin.py\n@@ -0,0 +1,24 @@\n+from abc import ABC, abstractmethod\n+\n+class Plugin(ABC):\n+    \"\"\"\n+    Abstract Base Class for all AI Self-Extending Engine plugins.\n+    Defines the standard interface that all plugins must implement.\n+    \"\"\"\n+\n+    @abstractmethod\n+    def name(self) -> str:\n+        \"\"\"Returns the name of the plugin.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def version(self) -> str:\n+        \"\"\"Returns the version of the plugin.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def initialize(self, config: dict):\n+        \"\"\"Initializes the plugin with given configuration.\"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def execute(self, context: dict) -> dict:\n+        \"\"\"Executes the core functionality of the plugin given a context.\"\"\"\n+        pass", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -4,9 +4,11 @@\n class Engine:\n     def __init__(self):\n         \"\"\"Initializes the Engine with necessary components.\"\"\"\n         # Placeholder for initializing role-specific modules or configurations\n         logger.info(\"Engine initialized.\")\n+        self.active_goals = {} # To store goals by ID\n+        self.next_goal_id = 1\n \n     def problem_identification(self, context: dict) -> dict:\n         \"\"\"Critique phase: identifies problems or areas for improvement.\"\"\"\n         logger.info(\"Executing problem identification (critique) phase.\")\n@@ -24,26 +26,76 @@\n         \"\"\"Self-review phase: evaluates the entire cycle's outcome.\"\"\"\n         logger.info(\"Executing self-review phase.\")\n         # TODO: Implement actual self-review logic\n         return {\"self_review_feedback\": \"Placeholder self-review feedback\"}\n \n-    def run_cycle(self, initial_context: dict = None) -> dict:\n+    def add_goal(self, initial_context: dict) -> int:\n         \"\"\"\n-        Orchestrates the full critique-refine-test-self-review cycle.\n+        Adds a new goal to the engine and returns its ID.\n         \"\"\"\n-        current_context = initial_context if initial_context is not None else {}\n-        logger.info(\"Starting a new execution cycle.\")\n+        goal_id = self.next_goal_id\n+        self.next_goal_id += 1\n+        # Each goal will have its own context and state\n+        self.active_goals[goal_id] = {\n+            \"id\": goal_id,\n+            \"status\": \"pending\", # e.g., pending, running, completed, failed\n+            \"context\": initial_context if initial_context is not None else {},\n+            \"phases_completed\": [] # To track progress within a goal's cycle\n+        }\n+        logger.info(f\"Goal {goal_id} added with initial context: {initial_context}\")\n+        return goal_id\n \n-        # 1. Problem Identification (Critique)\n-        problems = self.problem_identification(current_context)\n-        current_context.update(problems)\n-        logger.debug(f\"Problems identified: {problems}\")\n+    def _execute_goal_phases(self, goal_id: int):\n+        \"\"\"\n+        Executes the predefined critique-refine-test-self-review cycle for a single goal.\n+        This method is designed to be called for an individual goal, potentially in parallel.\n+        \"\"\"\n+        goal = self.active_goals.get(goal_id)\n+        if not goal:\n+            logger.error(f\"Goal {goal_id} not found for execution.\")\n+            return\n \n-        # 2. Refine\n-        refined_output = self.refine(current_context)\n-        current_context.update(refined_output)\n-        logger.debug(f\"Refined output: {refined_output}\")\n+        current_context = goal[\"context\"]\n+        logger.info(f\"Starting execution for Goal {goal_id}.\")\n+        goal[\"status\"] = \"running\"\n \n-        # 3. Test\n-        test_results = self.test(current_context)\n-        current_context.update(test_results)\n-        logger.debug(f\"Test results: {test_results}\")\n+        try:\n+            # 1. Problem Identification (Critique)\n+            problems = self.problem_identification(current_context)\n+            current_context.update(problems)\n+            goal[\"phases_completed\"].append(\"problem_identification\")\n+            logger.debug(f\"Goal {goal_id} - Problems identified: {problems}\")\n \n-        # 4. Self-Review\n-        self_review_feedback = self.self_review(current_context)\n-        current_context.update(self_review_feedback)\n-        logger.debug(f\"Self-review feedback: {self_review_feedback}\")\n+            # 2. Refine\n+            refined_output = self.refine(current_context)\n+            current_context.update(refined_output)\n+            goal[\"phases_completed\"].append(\"refine\")\n+            logger.debug(f\"Goal {goal_id} - Refined output: {refined_output}\")\n \n-        logger.info(\"Execution cycle completed.\")\n-        return current_context\n+            # 3. Test\n+            test_results = self.test(current_context)\n+            current_context.update(test_results)\n+            goal[\"phases_completed\"].append(\"test\")\n+            logger.debug(f\"Goal {goal_id} - Test results: {test_results}\")\n+\n+            # 4. Self-Review\n+            self_review_feedback = self.self_review(current_context)\n+            current_context.update(self_review_feedback)\n+            goal[\"phases_completed\"].append(\"self_review\")\n+            logger.debug(f\"Goal {goal_id} - Self-review feedback: {self_review_feedback}\")\n+\n+            goal[\"status\"] = \"completed\"\n+            logger.info(f\"Execution for Goal {goal_id} completed.\")\n+        except Exception as e:\n+            goal[\"status\"] = \"failed\"\n+            logger.error(f\"Execution for Goal {goal_id} failed: {e}\")\n+            # Depending on error handling strategy, might re-raise or just log\n+        finally:\n+            # Ensure the goal's context is updated even if an error occurs\n+            goal[\"context\"] = current_context\n+\n+    def run(self):\n+        \"\"\"\n+        Main execution loop for the engine, processing active goals.\n+        This method is the entry point for orchestrating multiple goals.\n+        For now, it processes goals sequentially.\n+        In future iterations, this method would be refactored to\n+        dispatch goals concurrently using a task queue or thread pool.\n+        \"\"\"\n+        if not self.active_goals:\n+            logger.info(\"No active goals to process.\")\n+            return\n+\n+        logger.info(\"Starting engine run to process active goals.\")\n+        # Iterate over a copy of keys to allow modification of self.active_goals during iteration\n+        for goal_id in list(self.active_goals.keys()):\n+            goal = self.active_goals[goal_id]\n+            if goal[\"status\"] == \"pending\":\n+                try:\n+                    self._execute_goal_phases(goal_id)\n+                except Exception as e:\n+                    logger.error(f\"Error processing goal {goal_id}: {e}\")\n+                    # Goal status is already set to 'failed' in _execute_goal_phases\n+            elif goal[\"status\"] == \"completed\":\n+                logger.debug(f\"Goal {goal_id} already completed, skipping.\")\n+            elif goal[\"status\"] == \"failed\":\n+                logger.warning(f\"Goal {goal_id} previously failed, skipping.\")\n+            else: # running, etc.\n+                logger.debug(f\"Goal {goal_id} is in status '{goal['status']}', skipping for now.\")\n+\n+        logger.info(\"Engine run completed.\")\n```", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- /dev/null\n+++ b/src/ai_self_ext_engine/config/config.py\n@@ -0,0 +1,50 @@\n+class Config:\n+    \"\"\"\n+    Centralized configuration for the AI Self-Extending Engine.\n+\n+    This class holds various configuration settings, including details about\n+    external software dependencies, API keys, and other operational parameters.\n+    \"\"\"\n+\n+    # Configuration for external software dependencies.\n+    # This section declares the core external software dependencies that the\n+    # AI engine might interact with or rely upon. This serves as an internal\n+    # representation for future dependency management tool integration (e.g.,\n+    # verifying installation, version compatibility, or triggering external\n+    # tool setup).\n+    EXTERNAL_DEPENDENCIES = [\n+        {\n+            \"name\": \"git\",\n+            \"type\": \"tool\",\n+            \"description\": \"Version control system for source code management.\",\n+            \"version_expected\": \">=2.0.0\",\n+            \"required\": True,\n+            \"purpose\": \"Code retrieval, storage, and versioning for self-modification.\",\n+        },\n+        {\n+            \"name\": \"python\",\n+            \"type\": \"runtime\",\n+            \"description\": \"Python interpreter for executing engine code and scripts.\",\n+            \"version_expected\": \">=3.9\",\n+            \"required\": True,\n+            \"purpose\": \"Core execution environment.\",\n+        },\n+        {\n+            \"name\": \"pip\",\n+            \"type\": \"package_manager\",\n+            \"description\": \"Python package installer.\",\n+            \"version_expected\": \">=20.0\",\n+            \"required\": True,\n+            \"purpose\": \"Installing Python dependencies (e.g., for new plugins).\",\n+        },\n+        {\n+            \"name\": \"poetry\",\n+            \"type\": \"package_manager\",\n+            \"description\": \"Python dependency management and packaging tool.\",\n+            \"version_expected\": \">=1.0.0\",\n+            \"required\": False,\n+            \"purpose\": \"Advanced Python project and dependency management.\",\n+        },\n+        # Add more dependencies as needed, e.g., 'docker', 'pytest', etc.\n+    ]\n+\n```", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- /dev/null\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -0,0 +1,63 @@\n+import logging\n+\n+logger = logging.getLogger(__name__)\n+\n+class Engine:\n+    def __init__(self):\n+        \"\"\"Initializes the Engine with necessary components.\"\"\"\n+        # Placeholder for initializing role-specific modules or configurations\n+        logger.info(\"Engine initialized.\")\n+\n+    def problem_identification(self, context: dict) -> dict:\n+        \"\"\"Critique phase: identifies problems or areas for improvement.\"\"\"\n+        logger.info(\"Executing problem identification (critique) phase.\")\n+        # TODO: Implement actual problem identification logic\n+        return {\"problems_identified\": [\"Initial placeholder problem\"]}\n+\n+    def refine(self, context: dict) -> dict:\n+        \"\"\"Refine phase: generates solutions or refinements based on identified problems.\"\"\"\n+        logger.info(\"Executing refine phase.\")\n+        # TODO: Implement actual refinement logic\n+        return {\"refined_output\": \"Placeholder refined output\"}\n+\n+    def test(self, context: dict) -> dict:\n+        \"\"\"Test phase: validates the refined output.\"\"\"\n+        logger.info(\"Executing test phase.\")\n+        # TODO: Implement actual testing logic\n+        return {\"test_results\": \"Placeholder test results: PASSED\"}\n+\n+    def self_review(self, context: dict) -> dict:\n+        \"\"\"Self-review phase: evaluates the entire cycle's outcome.\"\"\"\n+        logger.info(\"Executing self-review phase.\")\n+        # TODO: Implement actual self-review logic\n+        return {\"self_review_feedback\": \"Placeholder self-review feedback\"}\n+\n+    def run_cycle(self, initial_context: dict = None) -> dict:\n+        \"\"\"\n+        Orchestrates the full critique-refine-test-self-review cycle.\n+        \"\"\"\n+        current_context = initial_context if initial_context is not None else {}\n+        logger.info(\"Starting a new execution cycle.\")\n+\n+        # 1. Problem Identification (Critique)\n+        problems = self.problem_identification(current_context)\n+        current_context.update(problems)\n+        logger.debug(f\"Problems identified: {problems}\")\n+\n+        # 2. Refine\n+        refined_output = self.refine(current_context)\n+        current_context.update(refined_output)\n+        logger.debug(f\"Refined output: {refined_output}\")\n+\n+        # 3. Test\n+        test_results = self.test(current_context)\n+        current_context.update(test_results)\n+        logger.debug(f\"Test results: {test_results}\")\n+\n+        # 4. Self-Review\n+        self_review_feedback = self.self_review(current_context)\n+        current_context.update(self_review_feedback)\n+        logger.debug(f\"Self-review feedback: {self_review_feedback}\")\n+\n+        logger.info(\"Execution cycle completed.\")\n+        return current_context", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- a/src/ai_self_ext_engine/core/plugin.py\n+++ b/src/ai_self_ext_engine/core/plugin.py\n@@ -1,23 +1,52 @@\n from abc import ABC, abstractmethod\n+from typing import Dict, Any\n \n class Plugin(ABC):\n     \"\"\"\n     Abstract Base Class for all AI Self-Extending Engine plugins.\n-    Defines the standard interface that all plugins must implement.\n+    Defines a standardized, extensible interface for plugins, including\n+    lifecycle management and capability declaration.\n     \"\"\"\n \n     @abstractmethod\n     def name(self) -> str:\n-        \"\"\"Returns the name of the plugin.\"\"\"\n+        \"\"\"\n+        Returns the unique name of the plugin.\n+        This name should be descriptive and unique among all loaded plugins.\n+        \"\"\"\n         pass\n \n     @abstractmethod\n     def version(self) -> str:\n-        \"\"\"Returns the version of the plugin.\"\"\"\n+        \"\"\"\n+        Returns the version string of the plugin (e.g., \"1.0.0\").\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def get_capabilities(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Declares the specific functionalities or services this plugin provides.\n+        Returns a dictionary where keys are capability names (e.g., \"code_generation\",\n+        \"test_execution\", \"refinement\") and values are details about the capability\n+        (e.g., supported input/output formats, complexity level, etc.).\n+        \"\"\"\n         pass\n \n     @abstractmethod\n-    def initialize(self, config: dict):\n-        \"\"\"Initializes the plugin with given configuration.\"\"\"\n+    def initialize(self, config: Dict[str, Any]):\n+        \"\"\"\n+        Initializes the plugin with the given configuration.\n+        This method is called once when the plugin is loaded and prepared for use.\n+        \"\"\"\n         pass\n \n     @abstractmethod\n-    def execute(self, context: dict) -> dict:\n-        \"\"\"Executes the core functionality of the plugin given a context.\"\"\"\n+    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n+        \"\"\"\n+        Executes the core functionality of the plugin given a context.\n+        The context dictionary should contain all necessary input data for the plugin\n+        to perform its operation. The method returns a dictionary containing the results.\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def shutdown(self):\n+        \"\"\"\n+        Performs any necessary cleanup before the plugin is unloaded or the engine exits.\n+        This method is called once when the plugin is being shut down.\n+        \"\"\"\n         pass", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -4,9 +4,11 @@\n class Engine:\n     def __init__(self):\n         \"\"\"Initializes the Engine with necessary components.\"\"\"\n         # Placeholder for initializing role-specific modules or configurations\n         logger.info(\"Engine initialized.\")\n+        self.active_goals = {} # To store goals by ID\n+        self.next_goal_id = 1\n \n     def problem_identification(self, context: dict) -> dict:\n         \"\"\"Critique phase: identifies problems or areas for improvement.\"\"\"\n         logger.info(\"Executing problem identification (critique) phase.\")\n@@ -24,26 +26,76 @@\n         \"\"\"Self-review phase: evaluates the entire cycle's outcome.\"\"\"\n         logger.info(\"Executing self-review phase.\")\n         # TODO: Implement actual self-review logic\n         return {\"self_review_feedback\": \"Placeholder self-review feedback\"}\n \n-    def run_cycle(self, initial_context: dict = None) -> dict:\n+    def add_goal(self, initial_context: dict) -> int:\n         \"\"\"\n-        Orchestrates the full critique-refine-test-self-review cycle.\n+        Adds a new goal to the engine and returns its ID.\n         \"\"\"\n-        current_context = initial_context if initial_context is not None else {}\n-        logger.info(\"Starting a new execution cycle.\")\n+        goal_id = self.next_goal_id\n+        self.next_goal_id += 1\n+        # Each goal will have its own context and state\n+        self.active_goals[goal_id] = {\n+            \"id\": goal_id,\n+            \"status\": \"pending\", # e.g., pending, running, completed, failed\n+            \"context\": initial_context if initial_context is not None else {},\n+            \"phases_completed\": [] # To track progress within a goal's cycle\n+        }\n+        logger.info(f\"Goal {goal_id} added with initial context: {initial_context}\")\n+        return goal_id\n \n-        # 1. Problem Identification (Critique)\n-        problems = self.problem_identification(current_context)\n-        current_context.update(problems)\n-        logger.debug(f\"Problems identified: {problems}\")\n+    def _execute_goal_phases(self, goal_id: int):\n+        \"\"\"\n+        Executes the predefined critique-refine-test-self-review cycle for a single goal.\n+        This method is designed to be called for an individual goal, potentially in parallel.\n+        \"\"\"\n+        goal = self.active_goals.get(goal_id)\n+        if not goal:\n+            logger.error(f\"Goal {goal_id} not found for execution.\")\n+            return\n \n-        # 2. Refine\n-        refined_output = self.refine(current_context)\n-        current_context.update(refined_output)\n-        logger.debug(f\"Refined output: {refined_output}\")\n+        current_context = goal[\"context\"]\n+        logger.info(f\"Starting execution for Goal {goal_id}.\")\n+        goal[\"status\"] = \"running\"\n \n-        # 3. Test\n-        test_results = self.test(current_context)\n-        current_context.update(test_results)\n-        logger.debug(f\"Test results: {test_results}\")\n+        try:\n+            # 1. Problem Identification (Critique)\n+            problems = self.problem_identification(current_context)\n+            current_context.update(problems)\n+            goal[\"phases_completed\"].append(\"problem_identification\")\n+            logger.debug(f\"Goal {goal_id} - Problems identified: {problems}\")\n \n-        # 4. Self-Review\n-        self_review_feedback = self.self_review(current_context)\n-        current_context.update(self_review_feedback)\n-        logger.debug(f\"Self-review feedback: {self_review_feedback}\")\n+            # 2. Refine\n+            refined_output = self.refine(current_context)\n+            current_context.update(refined_output)\n+            goal[\"phases_completed\"].append(\"refine\")\n+            logger.debug(f\"Goal {goal_id} - Refined output: {refined_output}\")\n \n-        logger.info(\"Execution cycle completed.\")\n-        return current_context\n+            # 3. Test\n+            test_results = self.test(current_context)\n+            current_context.update(test_results)\n+            goal[\"phases_completed\"].append(\"test\")\n+            logger.debug(f\"Goal {goal_id} - Test results: {test_results}\")\n+\n+            # 4. Self-Review\n+            self_review_feedback = self.self_review(current_context)\n+            current_context.update(self_review_feedback)\n+            goal[\"phases_completed\"].append(\"self_review\")\n+            logger.debug(f\"Goal {goal_id} - Self-review feedback: {self_review_feedback}\")\n+\n+            goal[\"status\"] = \"completed\"\n+            logger.info(f\"Execution for Goal {goal_id} completed.\")\n+        except Exception as e:\n+            goal[\"status\"] = \"failed\"\n+            logger.error(f\"Execution for Goal {goal_id} failed: {e}\")\n+            # Depending on error handling strategy, might re-raise or just log\n+        finally:\n+            # Ensure the goal's context is updated even if an error occurs\n+            goal[\"context\"] = current_context\n+\n+    def run(self):\n+        \"\"\"\n+        Main execution loop for the engine, processing active goals.\n+        This method is the entry point for orchestrating multiple goals.\n+        For now, it processes goals sequentially.\n+        In future iterations, this method would be refactored to\n+        dispatch goals concurrently using a task queue or thread pool.\n+        \"\"\"\n+        if not self.active_goals:\n+            logger.info(\"No active goals to process.\")\n+            return\n+\n+        logger.info(\"Starting engine run to process active goals.\")\n+        # Iterate over a copy of keys to allow modification of self.active_goals during iteration\n+        for goal_id in list(self.active_goals.keys()):\n+            goal = self.active_goals[goal_id]\n+            if goal[\"status\"] == \"pending\":\n+                try:\n+                    self._execute_goal_phases(goal_id)\n+                except Exception as e:\n+                    logger.error(f\"Error processing goal {goal_id}: {e}\")\n+                    # Goal status is already set to 'failed' in _execute_goal_phases\n+            elif goal[\"status\"] == \"completed\":\n+                logger.debug(f\"Goal {goal_id} already completed, skipping.\")\n+            elif goal[\"status\"] == \"failed\":\n+                logger.warning(f\"Goal {goal_id} previously failed, skipping.\")\n+            else: # running, etc.\n+                logger.debug(f\"Goal {goal_id} is in status '{goal['status']}', skipping for now.\")\n+\n+        logger.info(\"Engine run completed.\")", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -1,13 +1,105 @@\n import logging\n+import subprocess\n+import sys\n+import re\n+\n+from ai_self_ext_engine.config.config import Config\n \n logger = logging.getLogger(__name__)\n \n class Engine:\n     def __init__(self):\n         \"\"\"Initializes the Engine with necessary components.\"\"\"\n         # Placeholder for initializing role-specific modules or configurations\n         logger.info(\"Engine initialized.\")\n         self.active_goals = {} # To store goals by ID\n         self.next_goal_id = 1\n+        self._verify_core_dependencies() # Perform dependency check on initialization\n+\n+    def _compare_versions(self, found_version_str: str, expected_version_str: str) -> bool:\n+        \"\"\"\n+        Compares a found version string against an expected version string.\n+        Supports simple '>=X.Y.Z' comparisons by splitting on dots and comparing numerically.\n+        \"\"\"\n+        def parse_version_tuple(version_str):\n+            # Clean up the version string to get just the numbers\n+            match = re.search(r'(\\d+(?:\\.\\d+)*)', version_str)\n+            if match:\n+                return tuple(map(int, match.group(1).split('.')))\n+            return (0,) # Default for unparsable, effectively min version\n+\n+        if not expected_version_str:\n+            return True # No specific version expected, so any found version is fine\n+\n+        # Handle specific tool output formats for cleaning\n+        cleaned_found_version = found_version_str.strip()\n+        if cleaned_found_version.lower().startswith(\"python\"):\n+            cleaned_found_version = cleaned_found_version.split(\" \")[0] # E.g., \"3.9.7 (main, ...)\" -> \"3.9.7\"\n+        elif cleaned_found_version.lower().startswith(\"pip\"):\n+             # E.g., \"pip 21.0.1 from ...\" -> \"21.0.1\"\n+             cleaned_found_version = cleaned_found_version.split(\" \")[1]\n+        elif cleaned_found_version.lower().startswith(\"poetry\"):\n+             # E.g., \"Poetry (version 1.2.3)\" or \"Poetry version 1.2.3\"\n+             match = re.search(r'version\\s+(\\d+(?:\\.\\d+)*)', cleaned_found_version, re.IGNORECASE)\n+             if match:\n+                cleaned_found_version = match.group(1)\n+             else:\n+                cleaned_found_version = cleaned_found_version.split()[-1] # Fallback if regex fails\n+        elif \"version\" in cleaned_found_version.lower():\n+            # Generic case for tools like \"git version X.Y.Z\"\n+            match = re.search(r'version\\s+(\\d+(?:\\.\\d+)*)', cleaned_found_version, re.IGNORECASE)\n+            if match:\n+                cleaned_found_version = match.group(1)\n+            else:\n+                # Just look for any version pattern\n+                match = re.search(r'(\\d+(?:\\.\\d+)*)', cleaned_found_version)\n+                if match:\n+                    cleaned_found_version = match.group(1)\n+\n+        found_parts = parse_version_tuple(cleaned_found_version)\n+\n+        if expected_version_str.startswith(\">=\"):\n+            expected_parts = parse_version_tuple(expected_version_str[2:])\n+            # Pad the shorter tuple with zeros to match length for comparison\n+            max_len = max(len(found_parts), len(expected_parts))\n+            found_parts = found_parts + (0,) * (max_len - len(found_parts))\n+            expected_parts = expected_parts + (0,) * (max_len - len(expected_parts))\n+            return found_parts >= expected_parts\n+        else: # For exact match or if no operator\n+            expected_parts = parse_version_tuple(expected_version_str)\n+            return found_parts == expected_parts\n+\n+    def _verify_core_dependencies(self) -> dict:\n+        \"\"\"\n+        Programmatically checks for the presence and status of critical external packages\n+        and tools required for the engine's operation.\n+        Reports their status based on Config.EXTERNAL_DEPENDENCIES.\n+        \"\"\"\n+        logger.info(\"Verifying core dependencies...\")\n+        dependency_report = {}\n+\n+        for dep in Config.EXTERNAL_DEPENDENCIES:\n+            name = dep[\"name\"]\n+            dep_type = dep[\"type\"]\n+            version_expected = dep.get(\"version_expected\", \"\")\n+            required = dep.get(\"required\", False)\n+\n+            found_status = \"NOT_FOUND\"\n+            found_version = \"N/A\"\n+            is_satisfied = False\n+            details = \"\"\n+\n+            try:\n+                if dep_type == \"tool\":\n+                    # Check if the tool executable exists in PATH\n+                    path_cmd = [\"where\", name] if sys.platform == \"win32\" else [\"which\", name]\n+                    proc_path = subprocess.run(path_cmd, capture_output=True, text=True, check=False, shell=True if sys.platform == \"win32\" else False)\n+\n+                    if proc_path.returncode == 0:\n+                        found_status = \"FOUND\"\n+                        # Try to get the version using '--version' flag, which is common for many tools\n+                        try:\n+                            version_cmd = [name, \"--version\"]\n+                            proc_version = subprocess.run(version_cmd, capture_output=True, text=True, check=False)\n+                            if proc_version.returncode == 0:\n+                                found_version = proc_version.stdout.strip().split('\\n')[0]\n+                                is_satisfied = self._compare_versions(found_version, version_expected)\n+                                details = f\"Found at: {proc_path.stdout.strip().splitlines()[0]}\"\n+                            else:\n+                                details = f\"Found executable at '{proc_path.stdout.strip().splitlines()[0]}', but failed to get version with '{' '.join(version_cmd)}': {proc_version.stderr.strip()}\"\n+                                is_satisfied = False # Can't verify version if we can't get it\n+                        except FileNotFoundError: # Should not happen if proc_path succeeded, but for safety\n+                            details = f\"Found executable at '{proc_path.stdout.strip().splitlines()[0]}', but '{name}' command for version check failed to execute.\"\n+                            is_satisfied = False\n+                        except Exception as e:\n+                            details = f\"Found executable at '{proc_path.stdout.strip().splitlines()[0]}', but error during version check: {e}\"\n+                            is_satisfied = False\n+                    else:\n+                        details = f\"Executable '{name}' not found in PATH.\"\n+\n+                elif dep_type == \"runtime\" and name == \"python\":\n+                    found_version = sys.version.split(\" \")[0] # E.g., \"3.9.7 (main, ...)\" -> \"3.9.7\"\n+                    found_status = \"FOUND\"\n+                    is_satisfied = self._compare_versions(found_version, version_expected)\n+                    details = f\"Running Python version: {found_version}\"\n+\n+                elif dep_type == \"package_manager\":\n+                    cmd = [sys.executable, \"-m\", name, \"--version\"]\n+                    proc = subprocess.run(cmd, capture_output=True, text=True, check=False)\n+                    if proc.returncode == 0:\n+                        found_status = \"FOUND\"\n+                        found_version = proc.stdout.strip().split('\\n')[0]\n+                        is_satisfied = self._compare_versions(found_version, version_expected)\n+                        details = f\"Installed via Python: {found_version}\"\n+                    else:\n+                        details = f\"Command failed or package manager not found: {proc.stderr.strip()}\"\n+                else:\n+                    found_status = \"UNSUPPORTED_TYPE\"\n+                    details = f\"Dependency type '{dep_type}' is not supported for verification.\"\n+\n+            except FileNotFoundError:\n+                found_status = \"NOT_FOUND\"\n+                details = f\"Executable '{name}' not found.\"\n+            except Exception as e:\n+                found_status = \"ERROR_CHECKING\"\n+                details = f\"An error occurred while checking '{name}': {e}\"\n+\n+            report_entry = {\n+                \"name\": name,\n+                \"type\": dep_type,\n+                \"required\": required,\n+                \"version_expected\": version_expected,\n+                \"version_found\": found_version,\n+                \"status\": found_status,\n+                \"is_satisfied\": is_satisfied,\n+                \"details\": details,\n+            }\n+            dependency_report[name] = report_entry\n+\n+            log_level = logger.info\n+            if required and not is_satisfied:\n+                log_level = logger.critical # Use critical for required dependencies not satisfied\n+            elif not is_satisfied:\n+                log_level = logger.warning # Warning for optional dependencies not satisfied\n+            \n+            log_level(f\"Dependency '{name}' ({dep_type}): Status='{found_status}', Version Found='{found_version}', Expected='{version_expected}', Satisfied={is_satisfied}, Required={required}. Details: {details}\")\n+\n+        return dependency_report\n+\n     def problem_identification(self, context: dict) -> dict:\n         \"\"\"Critique phase: identifies problems or areas for improvement.\"\"\"\n         logger.info(\"Executing problem identification (critique) phase.\")\n```", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -34,7 +34,7 @@\n         # Ensure core directories exist for the project structure\n         Path(self.config.engine.code_dir).mkdir(parents=True, exist_ok=True)\n \n-        self.roles = self._load_roles(config.roles)\n+        self.roles: Dict[str, Role] = self._load_roles(config.roles)\n         self.plugins = self._load_plugins(config.plugins)\n \n     def _load_roles(self, role_configs: List[RoleConfig]) -> List[Role]:\n@@ -42,7 +42,7 @@\n         \"\"\"\n         Dynamically loads and instantiates roles based on the role_configs.\n         \"\"\"\n-        loaded_roles: List[Role] = []\n+        loaded_roles: Dict[str, Role] = {}\n         for role_conf in role_configs:\n             try:\n                 module = import_module(role_conf.module)\n@@ -51,11 +51,11 @@\n                 if role_conf.class_name == \"RefineRole\":\n                     loaded_roles.append(\n                         role_class(\n                             self.config, self.model_client, self.learning_log\n                         )\n                     )\n                 else:\n-                    loaded_roles.append(\n+                    loaded_roles[role_conf.class_name] = (\n                         role_class(self.config, self.model_client)\n                     )\n             except (ImportError, AttributeError, TypeError) as e:\n@@ -124,15 +124,29 @@\n                 context.accepted = False\n                 context.should_abort = False\n \n-                # Execute roles\n-                for role in self.roles:\n+                # Execute roles in a fixed sequence for the critique-refine-test-self-review cycle\n+                cycle_roles_order = [\n+                    \"CritiqueRole\",\n+                    \"RefineRole\",\n+                    \"TestRole\",\n+                    \"SelfReviewRole\",\n+                ]\n+\n+                for role_name in cycle_roles_order:\n+                    role = self.roles.get(role_name)\n+                    if not role:\n+                        self.logger.error(\n+                            f\"Required role '{role_name}' not loaded. Aborting goal '{goal.goal_id}'.\"\n+                        )\n+                        context.should_abort = True\n+                        break # Break from role loop, will then check context.should_abort below\n+\n                     self.logger.info(\n-                        \"Executing role: %s\", role.__class__.__name__\n+                        \"Executing role: %s\", role_name\n                     )\n                     context = role.run(context)\n                     if context.should_abort:\n                         self.logger.warning(\n-                            \"Role %s requested abort. Stopping attempt.\",\n-                            role.__class__.__name__,\n+                            \"Role %s requested abort. Stopping attempt.\", role_name\n                         )\n                         break", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- a/src/ai_self_ext_engine/core/plugin.py\n+++ b/src/ai_self_ext_engine/core/plugin.py\n@@ -1,28 +1,59 @@\n-from abc import abstractmethod\n-from typing import Any, Optional, Protocol # Import Protocol\n+from abc import ABC, abstractmethod\n+from typing import Dict, Any\n \n-class Plugin(Protocol): # Change to Protocol\n+class Plugin(ABC):\n     \"\"\"\n-    Protocol for all plugins in the self-extending engine.\n-    Plugins provide specific capabilities, such as language support or tool integration.\n+    Abstract Base Class for all AI Self-Extending Engine plugins.\n+    Defines a standardized, extensible interface for plugins, including\n+    lifecycle management and capability declaration.\n     \"\"\"\n+\n     @abstractmethod\n-    def detect(self, code: str) -> bool:\n+    def name(self) -> str:\n         \"\"\"\n-        Detects if the plugin is applicable to the given code.\n+        Returns the unique name of the plugin.\n+        This name should be descriptive and unique among all loaded plugins.\n         \"\"\"\n         pass\n \n     @abstractmethod\n-    def execute(self, command: str, **kwargs) -> Any:\n+    def version(self) -> str:\n         \"\"\"\n-        Executes a command specific to the plugin's capability.\n+        Returns the version string of the plugin (e.g., \"1.0.0\").\n         \"\"\"\n         pass\n \n     @abstractmethod\n-    def test(self, code: str, tests: Optional[str] = None) -> Any:\n+    def get_capabilities(self) -> Dict[str, Any]:\n         \"\"\"\n-        Runs tests for the given code using the plugin's testing framework.\n+        Declares the specific functionalities or services this plugin provides.\n+        Returns a dictionary where keys are capability names (e.g., \"code_generation\",\n+        \"test_execution\", \"refinement\") and values are details about the capability\n+        (e.g., supported input/output formats, complexity level, etc.).\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def initialize(self, config: Dict[str, Any]):\n+        \"\"\"\n+        Initializes the plugin with the given configuration.\n+        This method is called once when the plugin is loaded and prepared for use.\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n+        \"\"\"\n+        Executes the core functionality of the plugin given a context.\n+        The context dictionary should contain all necessary input data for the plugin\n+        to perform its operation. The method returns a dictionary containing the results.\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def shutdown(self):\n+        \"\"\"\n+        Performs any necessary cleanup before the plugin is unloaded or the engine exits.\n         \"\"\"\n         pass\n```", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -79,77 +79,84 @@\n                 raise  # Re-raise to stop execution\n         return loaded_plugins\n \n+    def _execute_single_goal_cycle(self, goal: Goal) -> bool:\n+        \"\"\"\n+        Executes a complete self-improvement cycle for a single goal,\n+        including multiple attempts if necessary.\n+        This method is designed to be potentially run concurrently for independent goals.\n+        Returns True if the goal was successfully completed, False otherwise.\n+        \"\"\"\n+        self.logger.info(\n+            \"\\n--- Processing Goal: %s - %s ---\",\n+            goal.goal_id,\n+            goal.description,\n+        )\n+\n+        context = Context(code_dir=self.config.engine.code_dir, goal=goal)\n+        loaded_snapshot = self.snapshot_store.load_latest(goal.goal_id)\n+        if loaded_snapshot:\n+            loaded_snapshot.goal = goal\n+            context = loaded_snapshot\n+            self.logger.info(\n+                \"Resuming goal '%s' from previous snapshot.\",\n+                goal.goal_id,\n+            )\n+        else:\n+            self.logger.info(\n+                \"Starting new attempt for goal '%s'.\",\n+                goal.goal_id,\n+            )\n+            context.todos = []\n+\n+        for attempt in range(self.config.engine.max_cycles):\n+            # Ensure goal is correctly typed and up-to-date within context for each attempt\n+            current_goal = cast(Goal, context.goal)\n+            self.logger.info(\n+                \"\\n--- Goal '%s' Attempt %s/%s ---\",\n+                current_goal.goal_id,\n+                attempt + 1,\n+                self.config.engine.max_cycles,\n+            )\n+\n+            # Reset transient states\n+            context.patch = None\n+            context.test_results = None\n+            context.review = None\n+            context.accepted = False\n+            context.should_abort = False\n+\n+            # Execute roles\n+            for role in self.roles:\n+                self.logger.info(\n+                    \"Executing role: %s\", role.__class__.__name__\n+                )\n+                context = role.run(context)\n+                if context.should_abort:\n+                    self.logger.warning(\n+                        \"Role %s requested abort. Stopping attempt.\",\n+                        role.__class__.__name__,\n+                    )\n+                    break  # Break from role loop\n+\n+            self.snapshot_store.record(context)\n+\n+            # Record learning entry\n+            learning_entry = create_learning_entry(\n+                goal=current_goal.description,\n+                patch=context.patch or \"\",\n+                test_results=context.test_results or {},\n+                review=context.review or \"\",\n+                success=context.accepted,\n+            )\n+            self.learning_log.record_entry(learning_entry)\n+\n+            if context.accepted:\n+                self.logger.info(\n+                    \"Goal '%s' completed in %s attempts.\",\n+                    current_goal.goal_id,\n+                    attempt + 1,\n+                )\n+                return True  # Goal completed successfully\n+            elif context.should_abort:\n+                self.logger.warning(\n+                    \"Goal '%s' aborted after %s attempts.\",\n+                    current_goal.goal_id,\n+                    attempt + 1,\n+                )\n+                return False  # Goal aborted before completion\n+\n+        self.logger.warning(\n+            \"Goal '%s' failed to complete within %s attempts.\",\n+            goal.goal_id,\n+            self.config.engine.max_cycles,\n+        )\n+        return False  # Goal failed to complete within max_cycles\n+\n     def run_cycles(self):\n         \"\"\"\n         Main loop for the self-improvement process.\n+        Orchestrates goal processing. Designed to be extendable for concurrent processing.\n         \"\"\"\n         self.logger.info(\"Starting self-improvement engine cycles...\")\n \n         while True:\n             goal = self.goal_manager.next_goal()\n             if not goal:\n                 self.logger.info(\"No more pending goals. Exiting.\")\n                 break\n \n-            context = Context(code_dir=self.config.engine.code_dir, goal=goal)\n-            goal = cast(Goal, context.goal)  # Ensure goal is correctly typed\n-            self.logger.info(\n-                \"\\n--- Processing Goal: %s - %s ---\",\n-                goal.goal_id,\n-                goal.description,\n-            )\n-\n-            loaded_snapshot = self.snapshot_store.load_latest(goal.goal_id)\n-            if loaded_snapshot:\n-                loaded_snapshot.goal = goal\n-                context = loaded_snapshot\n-                self.logger.info(\n-                    \"Resuming goal '%s' from previous snapshot.\",\n-                    goal.goal_id,\n-                )\n-            else:\n-                self.logger.info(\n-                    \"Starting new attempt for goal '%s'.\",\n-                    goal.goal_id,\n-                )\n-                context.todos = []\n-\n-            for attempt in range(self.config.engine.max_cycles):\n-                goal = cast(Goal, context.goal)  # Ensure goal is not None\n-                self.logger.info(\n-                    \"\\n--- Goal '%s' Attempt %s/%s ---\",\n-                    goal.goal_id,\n-                    attempt + 1,\n-                    self.config.engine.max_cycles,\n-                )\n-\n-                # Reset transient states\n-                context.patch = None\n-                context.test_results = None\n-                context.review = None\n-                context.accepted = False\n-                context.should_abort = False\n-\n-                # Execute roles\n-                for role in self.roles:\n-                    self.logger.info(\n-                        \"Executing role: %s\", role.__class__.__name__\n-                    )\n-                    context = role.run(context)\n-                    if context.should_abort:\n-                        self.logger.warning(\n-                            \"Role %s requested abort. Stopping attempt.\",\n-                            role.__class__.__name__,\n-                        )\n-                        break\n-\n-                self.snapshot_store.record(context)\n-\n-                # Record learning entry\n-                learning_entry = create_learning_entry(\n-                    goal=goal.description,\n-                    patch=context.patch or \"\",\n-                    test_results=context.test_results or {},\n-                    review=context.review or \"\",\n-                    success=context.accepted,\n-                )\n-                self.learning_log.record_entry(learning_entry)\n-\n-                if context.accepted:\n-                    self.goal_manager.mark_done(goal.goal_id)\n-                    self.logger.info(\n-                        \"Goal '%s' completed in %s attempts.\",\n-                        goal.goal_id,\n-                        attempt + 1,\n-                    )\n-                    break\n-                elif context.should_abort:\n-                    self.logger.warning(\n-                        \"Goal '%s' aborted after %s attempts.\",\n-                        goal.goal_id,\n-                        attempt + 1,\n-                    )\n-                    break  # Move to the next pending goal\n+            # Process the current goal. In a future iteration, this call could be\n+            # dispatched to a thread pool or process pool for concurrent execution.\n+            goal_completed = self._execute_single_goal_cycle(goal)\n+\n+            if goal_completed:\n+                self.goal_manager.mark_done(goal.goal_id)\n+            # If not completed (aborted or max_cycles reached), the goal remains pending\n+            # or its status is implicitly handled by the next_goal() logic, allowing\n+            # it to be picked up again or eventually skipped.\n```", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- a/src/ai_self_ext_engine/code_synthesizer.py\n+++ b/src/ai_self_ext_engine/code_synthesizer.py\n@@ -20,11 +20,13 @@\n         # In a real-world scenario, this would ideally be loaded from a file\n         # in a separate 'prompts' directory as per the engine's config.\n         self.PROMPT_TEMPLATE = \"\"\"\n You are an expert AI software engineer. Your task is to propose an initial self-improvement or code change based on a given goal and the current codebase.\n Your output MUST be a unified diff patch. If no changes are needed, output an empty string.\n+In addition to the core code changes, if your proposed changes introduce new external dependencies, you MUST also include a diff for a 'requirements.txt' file at the end of your patch, listing any new or updated package dependencies. If no new external dependencies are required, do not include a 'requirements.txt' diff.\n \n Goal: {goal_description}\n \n Current Codebase:\n ```\n {current_code}\n ```\n \n-Based on the Goal, generate a unified diff patch to improve the Current Codebase. Focus on the core change needed to address the goal.\n+Based on the Goal, generate a unified diff patch to improve the Current Codebase. Focus on the core change needed to address the goal and any associated dependency updates.\n Do not include any conversational text or explanations. Provide only the patch.\n \"\"\"\n \n```", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -34,7 +34,7 @@\n         # Ensure core directories exist for the project structure\n         Path(self.config.engine.code_dir).mkdir(parents=True, exist_ok=True)\n \n-        self.roles = self._load_roles(config.roles)\n+        self.roles: Dict[str, Role] = self._load_roles(config.roles)\n         self.plugins = self._load_plugins(config.plugins)\n \n     def _load_roles(self, role_configs: List[RoleConfig]) -> List[Role]:\n@@ -42,7 +42,7 @@\n         \"\"\"\n         Dynamically loads and instantiates roles based on the role_configs.\n         \"\"\"\n-        loaded_roles: List[Role] = []\n+        loaded_roles: Dict[str, Role] = {}\n         for role_conf in role_configs:\n             try:\n                 module = import_module(role_conf.module)\n@@ -51,7 +51,7 @@\n                         role_class(\n                             self.config, self.model_client, self.learning_log\n                         )\n                     )\n                 else:\n-                    loaded_roles.append(\n+                    loaded_roles[role_conf.class_name] = (\n                         role_class(self.config, self.model_client)\n                     )\n             except (ImportError, AttributeError, TypeError) as e:\n@@ -124,15 +124,29 @@\n                 context.accepted = False\n                 context.should_abort = False\n \n-                # Execute roles\n-                for role in self.roles:\n+                # Execute roles in a fixed sequence for the critique-refine-test-self-review cycle\n+                cycle_roles_order = [\n+                    \"CritiqueRole\",\n+                    \"RefineRole\",\n+                    \"TestRole\",\n+                    \"SelfReviewRole\",\n+                ]\n+\n+                for role_name in cycle_roles_order:\n+                    role = self.roles.get(role_name)\n+                    if not role:\n+                        self.logger.error(\n+                            f\"Required role '{role_name}' not loaded. Aborting goal '{goal.goal_id}'.\"\n+                        )\n+                        context.should_abort = True\n+                        break # Break from role loop, will then check context.should_abort below\n+\n                     self.logger.info(\n-                        \"Executing role: %s\", role.__class__.__name__\n+                        \"Executing role: %s\", role_name\n                     )\n                     context = role.run(context)\n                     if context.should_abort:\n                         self.logger.warning(\n-                            \"Role %s requested abort. Stopping attempt.\",\n-                            role.__class__.__name__,\n+                            \"Role %s requested abort. Stopping attempt.\", role_name\n                         )\n                         break", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- a/src/ai_self_ext_engine/core/plugin.py\n+++ b/src/ai_self_ext_engine/core/plugin.py\n@@ -1,28 +1,59 @@\n-from abc import abstractmethod\n-from typing import Any, Optional, Protocol # Import Protocol\n+from abc import ABC, abstractmethod\n+from typing import Dict, Any\n \n-class Plugin(Protocol): # Change to Protocol\n+class Plugin(ABC):\n     \"\"\"\n-    Protocol for all plugins in the self-extending engine.\n-    Plugins provide specific capabilities, such as language support or tool integration.\n+    Abstract Base Class for all AI Self-Extending Engine plugins.\n+    Defines a standardized, extensible interface for plugins, including\n+    lifecycle management and capability declaration.\n     \"\"\"\n+\n     @abstractmethod\n-    def detect(self, code: str) -> bool:\n+    def name(self) -> str:\n         \"\"\"\n-        Detects if the plugin is applicable to the given code.\n+        Returns the unique name of the plugin.\n+        This name should be descriptive and unique among all loaded plugins.\n         \"\"\"\n         pass\n \n     @abstractmethod\n-    def execute(self, command: str, **kwargs) -> Any:\n+    def version(self) -> str:\n         \"\"\"\n-        Executes a command specific to the plugin's capability.\n+        Returns the version string of the plugin (e.g., \"1.0.0\").\n         \"\"\"\n         pass\n \n     @abstractmethod\n-    def test(self, code: str, tests: Optional[str] = None) -> Any:\n+    def get_capabilities(self) -> Dict[str, Any]:\n         \"\"\"\n-        Runs tests for the given code using the plugin's testing framework.\n+        Declares the specific functionalities or services this plugin provides.\n+        Returns a dictionary where keys are capability names (e.g., \"code_generation\",\n+        \"test_execution\", \"refinement\") and values are details about the capability\n+        (e.g., supported input/output formats, complexity level, etc.).\n         \"\"\"\n         pass\n+\n+    @abstractmethod\n+    def initialize(self, config: Dict[str, Any]):\n+        \"\"\"\n+        Initializes the plugin with the given configuration.\n+        This method is called once when the plugin is loaded and prepared for use.\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def execute(self, context: Dict[str, Any]) -> Dict[str, Any]:\n+        \"\"\"\n+        Executes the core functionality of the plugin given a context.\n+        The context dictionary should contain all necessary input data for the plugin\n+        to perform its operation. The method returns a dictionary containing the results.\n+        \"\"\"\n+        pass\n+\n+    @abstractmethod\n+    def shutdown(self):\n+        \"\"\"\n+        Performs any necessary cleanup before the plugin is unloaded or the engine exits.\n+        \"\"\"\n+        pass", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py\n@@ -79,77 +79,84 @@\n                 raise  # Re-raise to stop execution\n         return loaded_plugins\n \n+    def _execute_single_goal_cycle(self, goal: Goal) -> bool:\n+        \"\"\"\n+        Executes a complete self-improvement cycle for a single goal,\n+        including multiple attempts if necessary.\n+        This method is designed to be potentially run concurrently for independent goals.\n+        Returns True if the goal was successfully completed, False otherwise.\n+        \"\"\"\n+        self.logger.info(\n+            \"\\n--- Processing Goal: %s - %s ---\",\n+            goal.goal_id,\n+            goal.description,\n+        )\n+\n+        context = Context(code_dir=self.config.engine.code_dir, goal=goal)\n+        loaded_snapshot = self.snapshot_store.load_latest(goal.goal_id)\n+        if loaded_snapshot:\n+            loaded_snapshot.goal = goal\n+            context = loaded_snapshot\n+            self.logger.info(\n+                \"Resuming goal '%s' from previous snapshot.\",\n+                goal.goal_id,\n+            )\n+        else:\n+            self.logger.info(\n+                \"Starting new attempt for goal '%s'.\",\n+                goal.goal_id,\n+            )\n+            context.todos = []\n+\n+        for attempt in range(self.config.engine.max_cycles):\n+            # Ensure goal is correctly typed and up-to-date within context for each attempt\n+            current_goal = cast(Goal, context.goal)\n+            self.logger.info(\n+                \"\\n--- Goal '%s' Attempt %s/%s ---\",\n+                current_goal.goal_id,\n+                attempt + 1,\n+                self.config.engine.max_cycles,\n+            )\n+\n+            # Reset transient states\n+            context.patch = None\n+            context.test_results = None\n+            context.review = None\n+            context.accepted = False\n+            context.should_abort = False\n+\n+            # Execute roles\n+            for role in self.roles:\n+                self.logger.info(\n+                    \"Executing role: %s\", role.__class__.__name__\n+                )\n+                context = role.run(context)\n+                if context.should_abort:\n+                    self.logger.warning(\n+                        \"Role %s requested abort. Stopping attempt.\",\n+                        role.__class__.__name__,\n+                    )\n+                    break  # Break from role loop\n+\n+            self.snapshot_store.record(context)\n+\n+            # Record learning entry\n+            learning_entry = create_learning_entry(\n+                goal=current_goal.description,\n+                patch=context.patch or \"\",\n+                test_results=context.test_results or {},\n+                review=context.review or \"\",\n+                success=context.accepted,\n+            )\n+            self.learning_log.record_entry(learning_entry)\n+\n+            if context.accepted:\n+                self.logger.info(\n+                    \"Goal '%s' completed in %s attempts.\",\n+                    current_goal.goal_id,\n+                    attempt + 1,\n+                )\n+                return True  # Goal completed successfully\n+            elif context.should_abort:\n+                self.logger.warning(\n+                    \"Goal '%s' aborted after %s attempts.\",\n+                    current_goal.goal_id,\n+                    attempt + 1,\n+                )\n+                return False  # Goal aborted before completion\n+\n+        self.logger.warning(\n+            \"Goal '%s' failed to complete within %s attempts.\",\n+            goal.goal_id,\n+            self.config.engine.max_cycles,\n+        )\n+        return False  # Goal failed to complete within max_cycles\n+\n     def run_cycles(self):\n         \"\"\"\n         Main loop for the self-improvement process.\n+        Orchestrates goal processing. Designed to be extendable for concurrent processing.\n         \"\"\"\n         self.logger.info(\"Starting self-improvement engine cycles...\")\n \n         while True:\n             goal = self.goal_manager.next_goal()\n             if not goal:\n                 self.logger.info(\"No more pending goals. Exiting.\")\n                 break\n \n-            context = Context(code_dir=self.config.engine.code_dir, goal=goal)\n-            goal = cast(Goal, context.goal)  # Ensure goal is correctly typed\n-            self.logger.info(\n-                \"\\n--- Processing Goal: %s - %s ---\",\n-                goal.goal_id,\n-                goal.description,\n-            )\n-\n-            loaded_snapshot = self.snapshot_store.load_latest(goal.goal_id)\n-            if loaded_snapshot:\n-                loaded_snapshot.goal = goal\n-                context = loaded_snapshot\n-                self.logger.info(\n-                    \"Resuming goal '%s' from previous snapshot.\",\n-                    goal.goal_id,\n-                )\n-            else:\n-                self.logger.info(\n-                    \"Starting new attempt for goal '%s'.\",\n-                    goal.goal_id,\n-                )\n-                context.todos = []\n-\n-            for attempt in range(self.config.engine.max_cycles):\n-                goal = cast(Goal, context.goal)  # Ensure goal is not None\n-                self.logger.info(\n-                    \"\\n--- Goal '%s' Attempt %s/%s ---\",\n-                    goal.goal_id,\n-                    attempt + 1,\n-                    self.config.engine.max_cycles,\n-                )\n-\n-                # Reset transient states\n-                context.patch = None\n-                context.test_results = None\n-                context.review = None\n-                context.accepted = False\n-                context.should_abort = False\n-\n-                # Execute roles\n-                for role in self.roles:\n-                    self.logger.info(\n-                        \"Executing role: %s\", role.__class__.__name__\n-                    )\n-                    context = role.run(context)\n-                    if context.should_abort:\n-                        self.logger.warning(\n-                            \"Role %s requested abort. Stopping attempt.\",\n-                            role.__class__.__name__,\n-                        )\n-                        break\n-\n-                self.snapshot_store.record(context)\n-\n-                # Record learning entry\n-                learning_entry = create_learning_entry(\n-                    goal=goal.description,\n-                    patch=context.patch or \"\",\n-                    test_results=context.test_results or {},\n-                    review=context.review or \"\",\n-                    success=context.accepted,\n-                )\n-                self.learning_log.record_entry(learning_entry)\n-\n-                if context.accepted:\n-                    self.goal_manager.mark_done(goal.goal_id)\n-                    self.logger.info(\n-                        \"Goal '%s' completed in %s attempts.\",\n-                        goal.goal_id,\n-                        attempt + 1,\n-                    )\n-                    break\n-                elif context.should_abort:\n-                    self.logger.warning(\n-                        \"Goal '%s' aborted after %s attempts.\",\n-                        goal.goal_id,\n-                        attempt + 1,\n-                    )\n-                    break  # Move to the next pending goal\n+            # Process the current goal. In a future iteration, this call could be\n+            # dispatched to a thread pool or process pool for concurrent execution.\n+            goal_completed = self._execute_single_goal_cycle(goal)\n+\n+            if goal_completed:\n+                self.goal_manager.mark_done(goal.goal_id)\n+            # If not completed (aborted or max_cycles reached), the goal remains pending\n+            # or its status is implicitly handled by the next_goal() logic, allowing\n+            # it to be picked up again or eventually skipped.", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- a/src/ai_self_ext_engine/config/config.py\n+++ b/src/ai_self_ext_engine/config/config.py\n@@ -23,5 +23,10 @@\n plugins: {}\n logging:\n \n-\n-\n+CORE_DEPENDENCIES = [\n+    \"PyYAML\",\n+    \"python-gemini-sdk\",\n+    # Add other core dependencies here as the engine evolves.\n+    # These are dependencies that the engine itself relies on.\n+    # This list can inform self-extension and dependency management.\n+]", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- a/src/ai_self_ext_engine/core/plugin.py\n+++ b/src/ai_self_ext_engine/core/plugin.py", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- a/src/ai_self_ext_engine/code_synthesizer.py\n+++ b/src/ai_self_ext_engine/code_synthesizer.py", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- a/src/ai_self_ext_engine/core/plugin.py\n+++ b/src/ai_self_ext_engine/core/plugin.py", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- a/src/ai_self_ext_engine/package_smoke_test.py\n+++ b/src/ai_self_ext_engine/package_smoke_test.py", "test_results": {}, "review": "", "success": false}
{"goal": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py", "test_results": {}, "review": "", "success": false}
{"goal": "Refine the plugin architecture to make it more extensible and easier to add new capabilities.", "patch": "--- a/src/ai_self_ext_engine/core/plugin.py\n+++ b/src/ai_self_ext_engine/core/plugin.py", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "", "test_results": {}, "review": "", "success": false}
{"goal": "Investigate the possibility of executing independent goals in parallel to improve performance.", "patch": "--- a/src/ai_self_ext_engine/core/engine.py\n+++ b/src/ai_self_ext_engine/core/engine.py", "test_results": {}, "review": "", "success": false}
{"goal": "Adopt a dependency management tool (e.g., poetry or pip-tools) to manage dependencies more effectively.", "patch": "--- a/src/ai_self_ext_engine/config/config.py\n+++ b/src/ai_self_ext_engine/config/config.py", "test_results": {}, "review": "", "success": false}
