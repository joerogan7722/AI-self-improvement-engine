{
  "cycle": null,
  "goal_id": "auto_goal_1",
  "description": "Improve refactoring: Found 1 code smells requiring attention",
  "current_code": "# File: src/ai_self_ext_engine/roles/refine.py\nfrom ai_self_ext_engine.learning_log import LearningLog\n\n\nif TYPE_CHECKING:\n    from ai_self_ext_engine.todo_schema import Todo\n\n\nlogger = logging.getLogger(__name__)\n\n\nclass RefineRole(Role):\n    def __init__(\n        self,\n        config: MainConfig,\n        model_client: ModelClient,\n        learning_log: LearningLog,\n    ):\n        self.config = config\n        self.model_client = model_client\n        self.learning_log = learning_log\n        self.prompt_template_path = (\n            Path(config.engine.prompts_dir) / \"patch_generation.tpl\"\n        )\n\n    def run(self, context: Context) -> Context:\n        if not context.todos:\n            logger.info(\"RefineRole: No todos in context. Skipping.\")\n            return context\n\n        logger.info(\n            \"RefineRole: Generating and applying patch for todos: %s\",\n            context.todos,\n        )\n\n        try:\n            # Read code for each todo and combine\n            code_context_for_llm = self._read_code_for_todos(context.todos)\n            context.current_code = code_context_for_llm  # Update context\n\n            # Load prompt template from file\n            if not self.prompt_template_path.exists():\n                raise FileNotFoundError(\n                    f\"Prompt template not found at {self.prompt_template_path}\"\n                )\n\n            prompt_template = self.prompt_template_path.read_text(\n                encoding=\"utf-8\"\n            )\n\n            # Format todos for the prompt\n            todos_formatted = \"\\n\".join(\n                [\n                    f\"- File: {todo.get('file_path', 'N/A')}, \"\n                    f\"Type: {todo.get('change_type', 'modify')}, \"\n                    \"Description: \"\n                    f\"{todo.get('description', 'No description')}\"\n                    for todo in context.todos\n                ]\n            )\n            # Load and format learning examples\n            learning_examples = self._format_learning_examples()\n\n            prompt = prompt_template.format(\n                current_code=code_context_for_llm,\n                todos=todos_formatted,\n                learning_examples=learning_examples,\n            )\n\n            raw_patch_response = self.model_client.call_model(\n                self.config.model.model_name, prompt=prompt\n            ).strip()\n\n            # Extract patch using the new delimiters\n            patch = self._extract_patch_from_response(raw_patch_response)\n\n            context.patch = patch\n            logger.debug(\"RefineRole: Generated patch:\\n%s\", patch)\n\n            if patch:\n                # Normalize line endings and strip trailing whitespace\n                normalized_patch = patch.replace('\\r\\n', '\\n')\n                normalized_patch = '\\n'.join(line.rstrip() for line in normalized_patch.splitlines())\n\n                # Use the actual current working directory as cwd for git apply\n                if self._apply_patch(normalized_patch, os.getcwd()):\n                    logger.info(\"RefineRole: Patch applied successfully.\")\n                else:\n                    logger.error(\"RefineRole: Failed to apply patch. Aborting.\")\n                    context.should_abort = True\n            else:\n                logger.info(\"RefineRole: No valid patch generated. Skipping application.\")\n\n",
  "todos": [
    {
      "file_path": "src/ai_self_ext_engine/roles/refine.py",
      "change_type": "modify",
      "description": "Refactor identified code smell within the refinement logic to improve refactoring capabilities.",
      "line_start": 10,
      "line_end": 100
    }
  ],
  "patch": "--- a/src/ai_self_ext_engine/roles/refine.py\n+++ b/src/ai_self_ext_engine/roles/refine.py\n@@ -1,7 +1,9 @@\n+import os\n+from typing import TYPE_CHECKING\n from ai_self_ext_engine.learning_log import LearningLog\n \n \n-if TYPE_CHECKING:\n+if TYPE_CHECKING:\n     from ai_self_ext_engine.todo_schema import Todo\n \n \n@@ -34,57 +36,90 @@\n         )\n \n         try:\n-            # Read code for each todo and combine\n+            # Step 1: Read relevant code for the todos\n             code_context_for_llm = self._read_code_for_todos(context.todos)\n             context.current_code = code_context_for_llm  # Update context\n \n-            # Load prompt template from file\n-            if not self.prompt_template_path.exists():\n-                raise FileNotFoundError(\n-                    f\"Prompt template not found at {self.prompt_template_path}\"\n-                )\n-\n-            prompt_template = self.prompt_template_path.read_text(\n-                encoding=\"utf-8\"\n-            )\n-\n-            # Format todos for the prompt\n-            todos_formatted = \"\\n\".join(\n-                [\n-                    f\"- File: {todo.get('file_path', 'N/A')}, \"\n-                    f\"Type: {todo.get('change_type', 'modify')}, \"\n-                    \"Description: \"\n-                    f\"{todo.get('description', 'No description')}\"\n-                    for todo in context.todos\n-                ]\n-            )\n-            # Load and format learning examples\n-            learning_examples = self._format_learning_examples()\n-\n-            prompt = prompt_template.format(\n-                current_code=code_context_for_llm,\n-                todos=todos_formatted,\n-                learning_examples=learning_examples,\n-            )\n-\n-            raw_patch_response = self.model_client.call_model(\n-                self.config.model.model_name, prompt=prompt\n-            ).strip()\n-\n-            # Extract patch using the new delimiters\n+            # Step 2: Prepare the full prompt content\n+            prompt_content = self._prepare_prompt_content(code_context_for_llm, context.todos)\n+ \n+            # Step 3: Call the model and extract the raw patch\n+            raw_patch_response = self._call_model_for_patch(prompt_content)\n             patch = self._extract_patch_from_response(raw_patch_response)\n \n             context.patch = patch\n             logger.debug(\"RefineRole: Generated patch:\\n%s\", patch)\n \n+            # Step 4: Apply the generated patch if valid\n             if patch:\n-                # Normalize line endings and strip trailing whitespace\n-                normalized_patch = patch.replace('\\r\\n', '\\n')\n-                normalized_patch = '\\n'.join(line.rstrip() for line in normalized_patch.splitlines())\n-\n-                # Use the actual current working directory as cwd for git apply\n-                if self._apply_patch(normalized_patch, os.getcwd()):\n-                    logger.info(\"RefineRole: Patch applied successfully.\")\n-                else:\n-                    logger.error(\"RefineRole: Failed to apply patch. Aborting.\")\n-                    context.should_abort = True\n+                self._apply_generated_patch(patch, context)\n             else:\n                 logger.info(\"RefineRole: No valid patch generated. Skipping application.\")\n+\n+        except FileNotFoundError as e:\n+            logger.error(f\"RefineRole: Prompt template error: {e}\")\n+            context.should_abort = True\n+        except Exception as e:\n+            logger.error(f\"RefineRole: An unexpected error occurred during patch generation or application: {e}\", exc_info=True)\n+            context.should_abort = True\n+        return context\n+\n+    def _prepare_prompt_content(self, current_code: str, todos: list['Todo']) -> str:\n+        \"\"\"\n+        Loads the prompt template and formats it with current code, todos, and learning examples.\n+        \"\"\"\n+        if not self.prompt_template_path.exists():\n+            raise FileNotFoundError(\n+                f\"Prompt template not found at {self.prompt_template_path}\"\n+            )\n+\n+        prompt_template = self.prompt_template_path.read_text(encoding=\"utf-8\")\n+\n+        todos_formatted = \"\\n\".join(\n+            [f\"- File: {todo.get('file_path', 'N/A')}, Type: {todo.get('change_type', 'modify')}, Description: {todo.get('description', 'No description')}\"\n+             for todo in todos]\n+        )\n+        learning_examples = self._format_learning_examples()\n+\n+        return prompt_template.format(\n+            current_code=current_code,\n+            todos=todos_formatted,\n+            learning_examples=learning_examples,\n+        )\n+\n+    def _call_model_for_patch(self, prompt_content: str) -> str:\n+        \"\"\"\n+        Calls the language model with the prepared prompt and returns the raw response.\n+        \"\"\"\n+        return self.model_client.call_model(\n+            self.config.model.model_name, prompt=prompt_content\n+        ).strip()\n+\n+    def _apply_generated_patch(self, patch: str, context: Context):\n+        \"\"\"\n+        Normalizes the patch and attempts to apply it to the codebase.\n+        Updates context.should_abort based on application success.\n+        \"\"\"\n+        # Normalize line endings and strip trailing whitespace\n+        normalized_patch = patch.replace('\\r\\n', '\\n')\n+        normalized_patch = '\\n'.join(line.rstrip() for line in normalized_patch.splitlines())\n+\n+        # Use the actual current working directory as cwd for git apply\n+        if self._apply_patch(normalized_patch, os.getcwd()):\n+            logger.info(\"RefineRole: Patch applied successfully.\")\n+        else:\n+            logger.error(\"RefineRole: Failed to apply patch. Aborting.\")\n+            context.should_abort = True",
  "test_results": null,
  "accepted": false,
  "should_abort": true,
  "metadata": {}
}