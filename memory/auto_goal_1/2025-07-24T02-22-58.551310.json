{
  "cycle": null,
  "goal_id": "auto_goal_1",
  "description": "Improve refactoring: Found 1 code smells requiring attention",
  "current_code": "# File: src/ai_self_ext_engine/roles/enhanced_refine.py\n\"\"\"\nEnhanced Refine Role with advanced feedback loops and role communication.\n\nThis role demonstrates the next generation of autonomous AI improvement by:\n1. Learning from feedback from other roles\n2. Providing structured feedback to improve the overall system\n3. Adapting its approach based on effectiveness metrics\n4. Contributing to the system's learning insights\n\"\"\"\n\nimport time\nimport logging\nfrom typing import List, Dict, Any\nfrom pathlib import Path\nfrom datetime import datetime\n\nfrom ai_self_ext_engine.core.role import Role, Context, RoleFeedback, FeedbackType, RoleMetrics\nfrom ai_self_ext_engine.model_client import ModelClient, ModelCallError\nfrom ai_self_ext_engine.config import MainConfig\nfrom ai_self_ext_engine.learning_log import LearningLog\n\nlogger = logging.getLogger(__name__)\n\n\nclass EnhancedRefineRole(Role):\n    \"\"\"\n    Enhanced version of RefineRole with advanced feedback loops and adaptive behavior.\n    \n    This role can:\n    - Learn from feedback provided by other roles\n    - Adapt its patch generation based on previous effectiveness\n    - Provide structured feedback to improve other roles\n    - Track and report its own performance metrics\n    \"\"\"\n\n    def __init__(self, config: MainConfig, model_client: ModelClient, learning_log: LearningLog):\n        self.config = config\n        self.model_client = model_client\n        self.learning_log = learning_log\n        self.prompt_template_path = (\n            Path(config.engine.prompts_dir) / \"patch_generation.tpl\"\n        )\n        \n        # Enhanced capabilities\n        self.adaptation_history = []\n        self.effectiveness_trends = []\n        self.feedback_integration_count = 0\n\n    def run(self, context: Context) -> Context:\n        \"\"\"\n        Enhanced run method with feedback integration and adaptive behavior.\n        \"\"\"\n        start_time = time.time()\n        role_name = self.__class__.__name__\n        \n        logger.info(f\"{role_name}: Starting enhanced refinement process\")\n        \n        try:\n            # 1. Process feedback from other roles\n            self._process_incoming_feedback(context, role_name)\n            \n            # 2. Adapt approach based on historical effectiveness\n            approach_adaptations = self._adapt_approach_from_history(context)\n            \n            # 3. Generate patch with enhanced context\n            patch_result = self._generate_enhanced_patch(context, approach_adaptations)\n            \n            # 4. Provide feedback to other roles\n            self._provide_feedback_to_other_roles(context, patch_result)\n            \n            # 5. Record execution metrics\n            execution_time = time.time() - start_time\n            self._record_performance_metrics(context, role_name, execution_time, patch_result)\n            \n            # 6. Add learning insights\n            self._contribute_learning_insights(context, patch_result, approach_adaptations)\n            \n            logger.info(f\"{role_name}: Enhanced refinement completed successfully\")\n            return context\n            \n        except Exception as e:\n            logger.error(f\"{role_name}: Enhanced refinement failed: {e}\")\n            \n            # Even in failure, provide feedback about what went wrong\n            error_feedback = RoleFeedback(\n                from_role=role_name,\n                to_role=None,  # Broadcast to all roles\n                feedback_type=FeedbackType.ERROR,\n                message=f\"RefineRole encountered error: {str(e)}\",\n                data={\"error_type\": type(e).__name__, \"context_state\": str(context.accepted)},\n                priority=\"high\"\n            )\n            context.add_feedback(error_feedback)\n            \n            return context\n\n    def _process_incoming_feedback(self, context: Context, role_name: str):\n        \"\"\"Process feedback from other roles to improve performance\"\"\"\n        feedback_list = context.get_feedback_for_role(role_name)\n        \n        if not feedback_list:\n            return\n            \n        logger.info(f\"Processing {len(feedback_list)} feedback items\")\n        \n        for feedback in feedback_list:\n            if feedback.feedback_type == FeedbackType.SUGGESTION:\n                self._integrate_suggestion(feedback)\n            elif feedback.feedback_type == FeedbackType.ERROR:\n                self._learn_from_error_feedback(feedback)\n            elif feedback.feedback_type == FeedbackType.METRIC:\n                self._analyze_performance_feedback(feedback)\n                \n        self.feedback_integration_count += len(feedback_list)\n\n    def _adapt_approach_from_history(self, context: Context) -> Dict[str, Any]:\n        \"\"\"Adapt refinement approach based on historical effectiveness\"\"\"\n        adaptations = {\n            \"use_conservative_approach\": False,\n            \"increase_context_analysis\": False,\n            \"focus_on_incremental_changes\": False,\n            \"emphasize_testing\": False\n        }\n        \n        # Analyze recent effectiveness\n        recent_effectiveness = context.get_role_effectiveness(self.__class__.__name__)\n        \n        if recent_effectiveness < 60:  # Poor performance\n            adaptations[\"use_conservative_approach\"] = True\n            adaptations[\"focus_on_incremental_changes\"] = True\n            logger.info(\"Adapting to conservative approach due to low effectiveness\")\n            \n        elif recent_effectiveness > 85:  # Excellent performance\n            adaptations[\"increase_context_analysis\"] = True\n            logger.info(\"Increasing analysis depth due to high effectiveness\")\n        \n        # Learn from learning log\n        recent_entries = self.learning_log.get_recent_entries(5)\n        failed_entries = [entry for entry in recent_entries if not entry.success]\n        \n        if len(failed_entries) > 2:\n            adaptations[\"emphasize_testing\"] = True\n            logger.info(\"Emphasizing testing due to recent failures\")\n        \n        self.adaptation_history.append({\n            \"timestamp\": datetime.now().isoformat(),\n            \"adaptations\": adaptations,\n            \"trigger_effectiveness\": recent_effectiveness\n        })\n        \n        return adaptations\n\n    def _generate_enhanced_patch(self, context: Context, adaptations: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Generate patch with enhanced context and adaptations\"\"\"\n        \n        # Build enhanced prompt with adaptations\n        enhancement_context = self._build_enhancement_context(context, adaptations)\n        \n        try:\n            # Use the existing patch generation logic but with enhanced context\n            # This would integrate with the existing RefineRole implementation\n            \n            patch_result = {\n                \"patch_generated\": True,\n                \"adaptations_applied\": adaptations,\n                \"enhancement_context\": enhancement_context,\n                \"confidence_level\": self._calculate_confidence_level(context, adaptations)\n            }\n            \n            # Set the patch in context (simplified for this example)\n            if context.todos:\n                context.patch = f\"# Enhanced patch with adaptations: {adaptations}\\n# TODO: Implement actual patch generation logic\"\n            \n            return patch_result\n            \n        except Exception as e:\n            logger.error(f\"Enhanced patch generation failed: {e}\")\n            return {\"patch_generated\": False, \"error\": str(e)}\n\n    def _provide_feedback_to_other_roles(self, context: Context, patch_result: Dict[str, Any]):\n        \"\"\"Provide structured feedback to other roles\"\"\"\n        \n        # Feedback to TestRole\n        test_feedback = RoleFeedback(\n            from_role=self.__class__.__name__,\n            to_role=\"TestRole\",\n            feedback_type=FeedbackType.SUGGESTION,\n            message=\"Focus testing on areas with high complexity changes\",\n            data={\n                \"patch_confidence\": patch_result.get(\"confidence_level\", 0.5),\n                \"suggested_test_focus\": [\"error_handling\", \"edge_cases\"]\n            },\n            priority=\"medium\"\n        )\n        context.add_feedback(test_feedback)\n        \n        # Feedback to SelfReviewRole\n        review_feedback = RoleFeedback(\n            from_role=self.__class__.__name__,\n            to_role=\"SelfReviewRole\", \n            feedback_type=FeedbackType.SUGGESTION,\n            message=\"Pay attention to architectural consistency\",\n            data={\n                \"adaptations_used\": patch_result.get(\"adaptations_applied\", {}),\n                \"review_priorities\": [\"maintainability\", \"performance_impact\"]\n            },\n            priority=\"medium\"\n        )\n        context.add_feedback(review_feedback)\n        \n        # Broadcast feedback about system learning\n        if self.feedback_integration_count > 0:\n            learning_feedback = RoleFeedback(\n                from_role=self.__class__.__name__,\n                to_role=None,  # Broadcast\n                feedback_type=FeedbackType.SUCCESS,\n                message=f\"Successfully integrated {self.feedback_integration_count} feedback items\",\n                data={\"integration_count\": self.feedback_integration_count},\n                priority=\"low\"\n            )\n            context.add_feedback(learning_feedback)\n\n    def _record_performance_metrics(self, context: Context, role_name: str, \n                                   execution_time: float, patch_result: Dict[str, Any]):\n        \"\"\"Record detailed performance metrics\"\"\"\n        \n        success_rate = 1.0 if patch_result.get(\"patch_generated\", False) else 0.0\n        effectiveness_score = patch_result.get(\"confidence_level\", 0.5) * 100\n        \n        metrics = RoleMetrics(\n            role_name=role_name,\n            execution_time=execution_time,\n            success_rate=success_rate,\n            effectiveness_score=effectiveness_score,\n            resource_usage={\n                \"feedback_items_processed\": self.feedback_integration_count,\n                \"adaptations_applied\": len(patch_result.get(\"adaptations_applied\", {})),\n                \"learning_entries_analyzed\": len(self.learning_log.get_recent_entries(5))\n            },\n            improvement_suggestions=self._generate_self_improvement_suggestions()\n        )\n        \n        context.update_role_metrics(metrics)\n        self.effectiveness_trends.append(effectiveness_score)\n\n    def _contribute_learning_insights(self, context: Context, patch_result: Dict[str, Any], \n                                    adaptations: Dict[str, Any]):\n        \"\"\"Add learning insights from this execution\"\"\"\n        \n        if patch_result.get(\"patch_generated\", False):\n            context.add_learning_insight(\n                f\"RefineRole: Successfully applied {sum(adaptations.values())} adaptations\"\n            )\n            \n        if self.feedback_integration_count > 0:\n            context.add_learning_insight(\n                f\"RefineRole: Integrated feedback from {self.feedback_integration_count} sources\"\n            )\n            \n        # Analyze effectiveness trends\n        if len(self.effectiveness_trends) >= 3:\n            recent_trend = self.effectiveness_trends[-3:]\n            if all(recent_trend[i] <= recent_trend[i+1] for i in range(len(recent_trend)-1)):\n                context.add_learning_insight(\n                    \"RefineRole: Showing consistent improvement trend in effectiveness\"\n                )\n\n    # Helper methods\n    \n    def _integrate_suggestion(self, feedback: RoleFeedback):\n        \"\"\"Integrate a suggestion from another role\"\"\"\n        logger.info(f\"Integrating suggestion: {feedback.message}\")\n        # Implementation would adapt behavior based on suggestion\n    \n    def _learn_from_error_feedback(self, feedback: RoleFeedback):\n        \"\"\"Learn from error feedback to avoid similar issues\"\"\"\n        logger.warning(f\"Learning from error: {feedback.message}\")\n        # Implementation would adjust approach to avoid similar errors\n    \n    def _analyze_performance_feedback(self, feedback: RoleFeedback):\n        \"\"\"Analyze performance metrics feedback\"\"\"\n        logger.info(f\"Analyzing performance feedback: {feedback.message}\")\n        # Implementation would adjust performance parameters\n    \n    def _build_enhancement_context(self, context: Context, adaptations: Dict[str, Any]) -> str:\n        \"\"\"Build enhanced context string for patch generation\"\"\"\n        return f\"Adaptations: {adaptations}, Learning insights: {len(context.learning_insights)}\"\n    \n    def _calculate_confidence_level(self, context: Context, adaptations: Dict[str, Any]) -> float:\n        \"\"\"Calculate confidence level for the generated patch\"\"\"\n        base_confidence = 0.7\n        \n        # Adjust based on adaptations\n        if adaptations.get(\"use_conservative_approach\", False):\n            base_confidence += 0.1\n        if adaptations.get(\"emphasize_testing\", False):\n            base_confidence += 0.05\n            \n        return min(1.0, base_confidence)\n    \n    def _generate_self_improvement_suggestions(self) -> List[str]:\n        \"\"\"Generate suggestions for improving this role\"\"\"\n        suggestions = []\n        \n        if len(self.effectiveness_trends) > 5:\n            avg_effectiveness = sum(self.effectiveness_trends[-5:]) / 5\n            if avg_effectiveness < 70:\n                suggestions.append(\"Consider more aggressive learning from feedback\")\n                suggestions.append(\"Analyze recent failure patterns more deeply\")\n        \n        if self.feedback_integration_count == 0:\n            suggestions.append(\"Improve feedback processing mechanisms\")\n            \n        return suggestions\n\n",
  "todos": [
    {
      "file_path": "src/ai_self_ext_engine/roles/enhanced_refine.py",
      "change_type": "modify",
      "description": "Enhance the `EnhancedRefine` role to specifically identify and apply refactoring strategies to resolve code smells detected during analysis, focusing on the most critical identified smell."
    }
  ],
  "patch": "--- a/src/ai_self_ext_engine/roles/enhanced_refine.py\n+++ b/src/ai_self_ext_engine/roles/enhanced_refine.py\n@@ -107,19 +107,59 @@\n     def _generate_enhanced_patch(self, context: Context, adaptations: Dict[str, Any]) -> Dict[str, Any]:\n         \"\"\"Generate patch with enhanced context and adaptations\"\"\"\n         \n+        refactoring_instruction = None\n         # Build enhanced prompt with adaptations\n-        enhancement_context = self._build_enhancement_context(context, adaptations)\n+        # 1. Identify and prioritize code smells from context\n+        critical_smell = self._identify_and_prioritize_smells(context)\n+        if critical_smell:\n+            refactoring_instruction = self._craft_refactoring_instruction(critical_smell)\n+            logger.info(f\"Identified critical code smell: {critical_smell.get('type')}. Generating refactoring instruction.\")\n+\n+        enhancement_context = self._build_enhancement_context(context, adaptations, refactoring_instruction)\n         \n         try:\n-            # Use the existing patch generation logic but with enhanced context\n-            # This would integrate with the existing RefineRole implementation\n-            \n+            # Read the prompt template\n+            if not self.prompt_template_path.exists():\n+                logger.error(f\"Prompt template file not found: {self.prompt_template_path}\")\n+                return {\"patch_generated\": False, \"error\": \"Prompt template not found\"}\n+            \n+            prompt_template_content = self.prompt_template_path.read_text()\n+\n+            # Assume context.goal exists and has a description for the overall task\n+            goal_description = context.goal.description if context.goal else \"No specific goal provided.\"\n+            \n+            # Assume current_code_snapshot holds the relevant code to be modified/analyzed.\n+            # This might be specific files or the entire codebase relevant to the goal.\n+            # A more robust system would involve a CodeReader role or direct file access based on context.todos\n+            current_code_snapshot = context.code_snapshot if context.code_snapshot else \"\" \n+            if not current_code_snapshot and context.todos:\n+                # Fallback: if code_snapshot is not directly available, try to get from the first todo's file_path\n+                if context.todos[0].file_path and Path(context.todos[0].file_path).exists():\n+                    try:\n+                        current_code_snapshot = Path(context.todos[0].file_path).read_text()\n+                        logger.debug(f\"Using content of {context.todos[0].file_path} as current_code_snapshot.\")\n+                    except Exception as file_read_e:\n+                        logger.warning(f\"Could not read file {context.todos[0].file_path} for current_code_snapshot: {file_read_e}\")\n+                        current_code_snapshot = \"\"\n+                else:\n+                    logger.warning(\"No code snapshot available in context and target file not found for todos.\")\n+\n+            # Format the prompt using the template and collected information\n+            full_prompt = prompt_template_content.format(\n+                goal_description=goal_description,\n+                current_code=current_code_snapshot,\n+                enhancement_context=enhancement_context\n+            )\n+\n+            # Call the model to generate the patch\n+            response_text = self.model_client.call_model(\n+                model_name=self.config.model.model_name,\n+                prompt=full_prompt\n+            ).strip()\n+\n+            # Basic validation of the generated patch format\n+            if response_text.startswith(\"---\") and \"diff\" in response_text:\n+                context.patch = response_text\n+                logger.info(\"Successfully generated enhanced patch from model.\")\n+                patch_generated = True\n+            else:\n+                logger.warning(f\"Model response did not appear to be a valid diff. Response start: '{response_text[:50]}'\")\n+                context.patch = \"\" # Clear patch if invalid or invalid format\n+                patch_generated = False\n+\n             patch_result = {\n-                \"patch_generated\": True,\n+                \"patch_generated\": patch_generated,\n                 \"adaptations_applied\": adaptations,\n                 \"enhancement_context\": enhancement_context,\n                 \"confidence_level\": self._calculate_confidence_level(context, adaptations)\n             }\n             \n-            # Set the patch in context (simplified for this example)\n-            if context.todos:\n-                context.patch = f\"# Enhanced patch with adaptations: {adaptations}\\n# TODO: Implement actual patch generation logic\"\n-            \n+            if refactoring_instruction:\n+                patch_result[\"refactoring_applied\"] = True\n+                patch_result[\"code_smell_addressed\"] = critical_smell\n+\n             return patch_result\n             \n         except Exception as e:\n@@ -220,13 +260,49 @@\n         logger.info(f\"Analyzing performance feedback: {feedback.message}\")\n         # Implementation would adjust performance parameters\n     \n-    def _build_enhancement_context(self, context: Context, adaptations: Dict[str, Any]) -> str:\n+    def _build_enhancement_context(self, context: Context, adaptations: Dict[str, Any], \n+                                  refactoring_instruction: Optional[str] = None) -> str:\n         \"\"\"Build enhanced context string for patch generation\"\"\"\n-        return f\"Adaptations: {adaptations}, Learning insights: {len(context.learning_insights)}\"\n+        context_parts = [\n+            f\"Adaptations: {adaptations}\",\n+            f\"Learning insights: {len(context.learning_insights)}\"\n+        ]\n+        if refactoring_instruction:\n+            context_parts.append(f\"Refactoring Focus: {refactoring_instruction}\")\n+        return \", \".join(context_parts)\n     \n     def _calculate_confidence_level(self, context: Context, adaptations: Dict[str, Any]) -> float:\n         \"\"\"Calculate confidence level for the generated patch\"\"\"\n         base_confidence = 0.7\n         \n+        # Adjust based on refactoring effort\n+        critical_smell = self._identify_and_prioritize_smells(context)\n+        if critical_smell:\n+            base_confidence += 0.15 # Higher confidence if addressing a known critical smell\n+        \n         # Adjust based on adaptations\n         if adaptations.get(\"use_conservative_approach\", False):\n             base_confidence += 0.1\n         if adaptations.get(\"emphasize_testing\", False):\n             base_confidence += 0.05\n+        \n+        return min(1.0, base_confidence)\n+    \n+    def _identify_and_prioritize_smells(self, context: Context) -> Optional[Dict[str, Any]]:\n+        \"\"\"\n+        Identifies and prioritizes code smells from the context.\n+        Assumes code smells are stored in context.metadata under 'code_smells_detected'.\n+        Each smell is expected to be a dict with at least 'type' and 'description',\n+        and optionally 'severity' (e.g., 'critical', 'high', 'medium', 'low').\n+        \"\"\"\n+        smells = context.metadata.get(\"code_smells_detected\", [])\n+        if not smells:\n+            return None\n+\n+        # Prioritize: 'critical' > 'high' > 'medium' > 'low'. If no severity, default to 'medium'.\n+        priorities = {'critical': 4, 'high': 3, 'medium': 2, 'low': 1}\n+        sorted_smells = sorted(smells, key=lambda s: priorities.get(s.get('severity', 'medium').lower(), 0), reverse=True)\n+\n+        if sorted_smells:\n+            logger.debug(f\"Prioritized code smell for refactoring: {sorted_smells[0].get('type')} with severity {sorted_smells[0].get('severity', 'medium')}\")\n+            return sorted_smells[0]\n+        return None\n+\n+    def _craft_refactoring_instruction(self, smell: Dict[str, Any]) -> str:\n+        \"\"\"\n+        Crafts a specific instruction for the model to refactor the identified code smell.\n+        \"\"\"\n+        smell_type = smell.get(\"type\", \"unknown smell type\")\n+        file_path = smell.get(\"file\", \"an unspecified file\")\n+        line_info = f\" at line {smell['line']}\" if 'line' in smell else \"\"\n+        description = smell.get(\"description\", \"A code quality issue.\")\n+\n+        instruction = (\n+            f\"Please identify and apply a refactoring strategy to address the following code smell: \"\n+            f\"Type: '{smell_type}'. Location: '{file_path}{line_info}'. Description: '{description}'. \"\n+            \"Focus on resolving this specific, most critical issue detected, making the code cleaner, more modular, and maintainable.\"\n+        )\n+        return instruction",
  "test_results": null,
  "accepted": false,
  "should_abort": true,
  "metadata": {}
}