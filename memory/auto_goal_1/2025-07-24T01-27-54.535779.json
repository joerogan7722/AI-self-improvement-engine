{
  "cycle": null,
  "goal_id": "auto_goal_1",
  "description": "Improve refactoring: Found 1 code smells requiring attention",
  "current_code": "# File: src/ai_self_ext_engine/roles/self_review.py\nfrom pathlib import Path\nimport json\nfrom ai_self_ext_engine.core.role import Role, Context\nfrom ai_self_ext_engine.model_client import ModelClient, ModelCallError\nfrom ai_self_ext_engine.config import MainConfig\nimport subprocess\nimport os\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nclass SelfReviewRole(Role):\n    \"\"\"\n    Role responsible for evaluating the acceptance of changes based on\n    a code review from an LLM and test results.\n    \"\"\"\n    def __init__(self, config: MainConfig, model_client: ModelClient):\n        self.config = config\n        self.model_client = model_client\n        self.prompt_template_path = Path(config.engine.prompts_dir) / \"self_review.tpl\"\n\n    def run(self, context: Context) -> Context:\n        if context.should_abort or not context.patch:\n            logger.info(\"SelfReviewRole: Context aborted or no patch to review. Skipping self-review.\")\n            return context\n\n        logger.info(\"SelfReviewRole: Reviewing generated patch...\")\n\n        try:\n            # Load prompt template\n            if not self.prompt_template_path.exists():\n                raise FileNotFoundError(f\"Prompt template not found at {self.prompt_template_path}\")\n            \n            prompt_template = self.prompt_template_path.read_text(encoding=\"utf-8\")\n\n            # Format todos for the prompt\n            todos_formatted = \"\\n\".join([\n                f\"- File: {todo.get('file_path', 'N/A')}, Type: {todo.get('change_type', 'modify')}, Description: {todo.get('description', 'No description')}\"\n                for todo in context.todos\n            ])\n            \n            prompt = prompt_template.format(\n                todos=todos_formatted,\n                current_code=context.current_code,\n                patch=context.patch\n            )\n            \n            response_text = self.model_client.call_model(\n                self.config.model.model_name,\n                prompt=prompt\n            )\n\n            # Parse the review from the LLM\n            review = json.loads(response_text)\n            patch_accepted = review.get(\"patch_accepted\", False)\n            feedback = review.get(\"feedback\", \"No feedback provided.\")\n            \n            logger.info(\"SelfReviewRole: Review feedback: %s\", feedback)\n            \n            # Evaluate based on test results and LLM review\n            tests_passed = context.test_results and context.test_results.get(\"passed\", False)\n            \n            if patch_accepted and tests_passed:\n                logger.info(\"SelfReviewRole: Patch accepted by review and tests passed. Changes are accepted.\")\n                context.accepted = True\n            else:\n                logger.info(\"SelfReviewRole: Patch not accepted or tests failed. Reverting changes.\")\n                context.accepted = False\n                self._git_reset_all(os.getcwd())\n                context.should_abort = True\n\n        except (ModelCallError, json.JSONDecodeError, FileNotFoundError) as e:\n            logger.error(\"SelfReviewRole: Error during self-review: %s\", e)\n            context.accepted = False\n            self._git_reset_all(os.getcwd())\n            context.should_abort = True\n        except Exception as e:\n            logger.exception(\"SelfReviewRole: An unexpected error occurred: %s\", e)\n            context.accepted = False\n            self._git_reset_all(os.getcwd())\n            context.should_abort = True\n            \n        return context\n\n    def _git_reset_all(self, cwd: str):\n        \"\"\"Resets all changes in the git repository.\"\"\"\n        try:\n            logger.info(\"SelfReviewRole: Resetting git repository in %s...\", cwd)\n            subprocess.run(\n                [\"git\", \"reset\", \"--hard\"],\n                check=True,\n                cwd=cwd,\n                capture_output=True,\n            )\n            logger.info(\"SelfReviewRole: Git reset successful.\")\n        except subprocess.CalledProcessError:\n            logger.exception(\"SelfReviewRole: Error resetting git.\")\n        except Exception:\n            logger.exception(\"SelfReviewRole: An unexpected error occurred during git reset.\")\n\n\n",
  "todos": [
    {
      "file_path": "src/ai_self_ext_engine/roles/self_review.py",
      "change_type": "modify",
      "description": "Enhance the `self_review` role to incorporate explicit code smell detection and reporting mechanisms for refactoring opportunities."
    }
  ],
  "patch": "--- a/src/ai_self_ext_engine/roles/self_review.py\n+++ b/src/ai_self_ext_engine/roles/self_review.py\n@@ -32,15 +32,25 @@\n                 for todo in context.todos\n             ])\n             \n-            prompt = prompt_template.format(\n+            # Construct the base prompt with existing variables\n+            base_prompt = prompt_template.format(\n                 todos=todos_formatted,\n                 current_code=context.current_code,\n                 patch=context.patch\n             )\n             \n+            # Add specific instructions for code smell detection\n+            prompt = base_prompt + \"\\n\\n\" \\\n+                     \"Additionally, analyze the 'current_code' and the 'patch' for any general code smells, \" \\\n+                     \"design flaws, or refactoring opportunities. Report these in a 'code_smells' JSON array. \" \\\n+                     \"Each item in the 'code_smells' array should be a dictionary with at least 'description', \" \\\n+                     \"'file', and 'line' (if applicable) fields. Also include 'severity' (low, medium, high) \" \\\n+                     \"and 'refactoring_opportunity' (a suggested action) for each smell.\\n\" \\\n+                     \"If no code smells are found, an empty array `[]` should be provided for 'code_smells'.\"\n+\n             response_text = self.model_client.call_model(\n                 self.config.model.model_name,\n                 prompt=prompt\n             )\n \n             # Parse the review from the LLM\n             review = json.loads(response_text)\n             patch_accepted = review.get(\"patch_accepted\", False)\n             feedback = review.get(\"feedback\", \"No feedback provided.\")\n+            \n+            # Extract code smells\n+            code_smells = review.get(\"code_smells\", [])\n+            context.code_smells = code_smells # Store code smells in context\n             \n             logger.info(\"SelfReviewRole: Review feedback: %s\", feedback)\n             \n+            if code_smells:\n+                logger.info(\"SelfReviewRole: Detected %d code smells:\", len(code_smells))\n+                for i, smell in enumerate(code_smells):\n+                    logger.info(\"  Smell %d: %s (File: %s, Line: %s, Severity: %s)\", \n+                                i+1, smell.get(\"description\", \"N/A\"), \n+                                smell.get(\"file\", \"N/A\"), smell.get(\"line\", \"N/A\"), \n+                                smell.get(\"severity\", \"N/A\"))\n+\n             # Evaluate based on test results and LLM review\n             tests_passed = context.test_results and context.test_results.get(\"passed\", False)",
  "test_results": null,
  "accepted": false,
  "should_abort": true,
  "metadata": {}
}