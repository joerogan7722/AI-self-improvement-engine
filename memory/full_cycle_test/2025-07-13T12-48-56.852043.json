{
  "cycle": null,
  "goal_id": "full_cycle_test",
  "description": "Run a full critique-refine-test-self-review cycle to verify end-to-end functionality of the refactored engine.",
  "current_code": "# File: src\\ai_self_ext_engine\\cli.py\nimport argparse\nfrom pathlib import Path\nimport os\nimport sys\nimport yaml\nimport logging # New import\nimport json # New import for JSON formatter\nfrom datetime import datetime # New import for JSON formatter\nfrom pydantic import ValidationError # Import ValidationError\n\nfrom .config import MainConfig, LoggingConfig\nfrom .core.engine import Engine\n\n# Set up a logger for the CLI module\nlogger = logging.getLogger(__name__)\n\nclass JsonFormatter(logging.Formatter):\n    \"\"\"A custom logging formatter that outputs logs in JSON format.\"\"\"\n    def format(self, record):\n        log_record = {\n            \"timestamp\": datetime.fromtimestamp(record.created).isoformat(),\n            \"level\": record.levelname,\n            \"name\": record.name,\n            \"message\": record.getMessage(),\n        }\n        if record.exc_info:\n            log_record[\"exc_info\"] = self.formatException(record.exc_info)\n        if record.stack_info:\n            log_record[\"stack_info\"] = self.formatStack(record.stack_info)\n\n        return json.dumps(log_record)\n\ndef _setup_logging(log_config: LoggingConfig):\n    \"\"\"Configures the root logger based on the provided logging configuration.\"\"\"\n    level_map = {level: getattr(logging, level.upper()) for level in [\"debug\", \"info\", \"warning\", \"error\", \"critical\"]}\n    log_level = level_map.get(log_config.level.lower(), logging.INFO)\n\n    root_logger = logging.getLogger()\n    root_logger.setLevel(log_level)\n    for handler in root_logger.handlers[:]: # Clear existing handlers\n        root_logger.removeHandler(handler)\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stderr)\n    console_handler.setLevel(log_level)\n\n    if log_config.format == \"json\":\n        formatter = JsonFormatter()\n    else:\n        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s')\n\n    console_handler.setFormatter(formatter)\n    root_logger.addHandler(console_handler)\n\n    # File handler (if log_file is specified)\n    if log_config.log_file:\n        log_file_path = Path(log_config.log_file)\n        log_file_path.parent.mkdir(parents=True, exist_ok=True) # Ensure log directory exists\n        file_handler = logging.FileHandler(log_file_path, encoding='utf-8')\n        file_handler.setLevel(log_level)\n        file_handler.setFormatter(formatter)\n        root_logger.addHandler(file_handler)\n\n    logger.info(\"Logging configured to level '%s' with format '%s'. Outputting to console and %s.\", \n                log_config.level, log_config.format, log_config.log_file if log_config.log_file else \"console only\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"AI Self-Extending Engine\")\n    parser.add_argument(\"--config\", type=str, default=\"config/engine_config.yaml\",\n                        help=\"Path to the engine configuration file.\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", \n                        help=\"Enable verbose logging (DEBUG level). Overrides config.\")\n    args = parser.parse_args()\n\n    # Load and validate configuration\n    config: MainConfig\n    try:\n        config_path = Path(args.config)\n        if not config_path.exists():\n            raise FileNotFoundError(f\"Config file not found at {config_path.absolute()}\")\n        \n        with open(config_path, 'r', encoding='utf-8') as f:\n            config_data = yaml.safe_load(f)\n        \n        config = MainConfig(**config_data) # Use MainConfig for validation\n\n        # Override log level if --verbose flag is set\n        if args.verbose:\n            config.logging.level = \"DEBUG\"\n\n        # Configure logging as early as possible after config is loaded\n        _setup_logging(config.logging)\n\n    except FileNotFoundError as e:\n        logger.error(\"Error: Config file not found at %s. %s\", config_path.absolute(), e, exc_info=False)\n        sys.exit(1)\n    except ValidationError as e:\n        logger.error(\"Configuration validation error: %s\", e, exc_info=True)\n        sys.exit(1)\n    except Exception as e:\n        logger.error(\"Error loading or parsing configuration: %s\", e, exc_info=True)\n        sys.exit(1)\n\n    engine = Engine(config)\n    engine.run_cycles()\n\nif __name__ == \"__main__\":\n    main()\n\n\n# File: src\\ai_self_ext_engine\\code_synthesizer.py\nimport logging\nfrom typing import Optional\nfrom pathlib import Path\n\nfrom ..config import MainConfig\nfrom ..model_client import ModelClient, ModelCallError\n\nlogger = logging.getLogger(__name__)\n\nclass CodeSynthesizer:\n    \"\"\"\n    A module responsible for synthesizing initial code improvements or patches\n    based on a given goal and the current codebase.\n    \"\"\"\n    def __init__(self, config: MainConfig, model_client: ModelClient):\n        self.config = config\n        self.model_client = model_client\n\n        # NOTE: Due to patch file location constraints (only modifying files within\n        # 'src/ai_self_ext_engine/'), the prompt template is embedded here.\n        # In a real-world scenario, this would ideally be loaded from a file\n        # in a separate 'prompts' directory as per the engine's config.\n        self.PROMPT_TEMPLATE = \"\"\"\nYou are an expert AI software engineer. Your task is to propose an initial self-improvement or code change based on a given goal and the current codebase.\nYour output MUST be a unified diff patch. If no changes are needed, output an empty string.\n\nGoal: {goal_description}\n\nCurrent Codebase:\n```\n{current_code}\n```\n\nBased on the Goal, generate a unified diff patch to improve the Current Codebase. Focus on the core change needed to address the goal.\nDo not include any conversational text or explanations. Provide only the patch.\n\"\"\"\n\n    def synthesize_initial_patch(self, goal_description: str, current_code: str) -> Optional[str]:\n        \"\"\"\n        Synthesizes an initial patch to address the given goal based on the current codebase.\n\n        Args:\n            goal_description: The description of the goal to achieve.\n            current_code: The concatenated content of the current codebase files.\n\n        Returns:\n            A unified diff patch string, or None if an error occurred or no patch was generated.\n        \"\"\"\n        logger.info(\"CodeSynthesizer: Synthesizing initial patch for goal: '%s'\", goal_description)\n\n        try:\n            prompt = self.PROMPT_TEMPLATE.format(\n                goal_description=goal_description,\n                current_code=current_code\n            )\n\n            response_text = self.model_client.call_model(\n                model_name=self.config.model.model_name,\n                prompt=prompt\n            ).strip()\n\n            if response_text.startswith(\"---\"):\n                logger.debug(\"CodeSynthesizer: Successfully synthesized an initial patch.\")\n                return response_text\n            elif not response_text:\n\n\n# File: src\\ai_self_ext_engine\\config.py\nfrom typing import List, Dict, Any, Optional, Literal\nfrom pydantic import BaseModel, Field, ValidationError, validator\n\nclass EngineSectionConfig(BaseModel):\n    code_dir: str = Field(\"./src\", description=\"Path to the codebase directory relative to project root.\")\n    max_cycles: int = Field(3, description=\"Maximum number of improvement cycles to run.\")\n    memory_path: str = Field(\"./memory\", description=\"Path to the memory/snapshot directory relative to project root.\")\n    goals_path: str = Field(\"goals.json\", description=\"Path to the goals file.\")\n    prompts_dir: str = Field(\"prompts\", description=\"Directory containing prompt templates, relative to project root.\")\n\nclass ModelSectionConfig(BaseModel):\n    api_key_env: str = Field(..., description=\"Environment variable name for the API key.\")\n    model_name: str = Field(\"gemini-2.5-flash\", description=\"Default model name to use.\")\n\nclass RoleConfig(BaseModel):\n    module: str = Field(..., description=\"Module path for the role, e.g., 'roles.problem_identification'.\")\n    class_name: str = Field(..., alias='class', description=\"Class name of the role within the module, e.g., 'ProblemIdentificationRole'.\")\n    prompt_path: str = Field(..., description=\"Path to the prompt template file relative to prompts_dir.\")\n\nclass PluginConfig(BaseModel):\n    entry_point: str = Field(..., description=\"Full import path to the plugin class, e.g., 'plugins.python.PythonPlugin'.\")\n\nclass LoggingConfig(BaseModel):\n    level: str = Field(\"INFO\", description=\"Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL).\")\n    format: str = Field(\"json\", description=\"Logging output format (json or plain).\")\n    log_file: Optional[str] = Field(None, description=\"Optional path to a log file. If not provided, logs go to stderr.\")\n\nclass MainConfig(BaseModel):\n    \"\"\"\n    Main configuration schema for the AI Self-Extending Engine.\n    \"\"\"\n    version: Literal[1] = Field(1, description=\"Version of the configuration schema.\")\n    engine: EngineSectionConfig = Field(..., description=\"Engine core settings.\")\n    model: ModelSectionConfig = Field(..., description=\"Model client settings.\")\n    roles: List[RoleConfig] = Field(..., description=\"List of roles to execute in order.\")\n    plugins: Dict[str, PluginConfig] = Field({}, description=\"Dictionary of plugins, keyed by name.\")\n    logging: LoggingConfig = Field(..., description=\"Logging configuration.\")\n\n    @validator('engine')\n    def validate_engine_max_cycles(cls, v):\n        if v.max_cycles <= 0:\n            raise ValueError('engine.max_cycles must be a positive integer')\n        return v\n\n    class Config:\n        validate_by_name = True # Allow 'class' to be used in RoleConfig\n\n\n# File: src\\ai_self_ext_engine\\goal_manager.py\nimport json\nfrom pathlib import Path\nimport logging # Import logging\nfrom typing import Any, Dict, List, Optional\n\nclass Goal:\n    \"\"\"Represents a single improvement goal.\"\"\"\n    def __init__(self, goal_id: str, description: str, status: str = \"pending\"):\n        self.goal_id = goal_id\n        self.description = description\n        self.status = status\n\n    def to_dict(self) -> Dict[str, Any]:\n        return {\"id\": self.goal_id, \"description\": self.description, \"status\": self.status}\n\nclass GoalManager:\n    \"\"\"Manages the loading, serving, and tracking of improvement goals.\"\"\"\n    def __init__(self, goals_path: str):\n        # Ensure the parent directory for the goals file exists\n        Path(goals_path).parent.mkdir(parents=True, exist_ok=True)\n        self.goals_path = Path(goals_path)\n        self.logger = logging.getLogger(__name__) # New logger\n        self.goals: List[Goal] = []\n        self._load_goals()\n        self._current_goal_index = 0\n\n    def _load_goals(self):\n        \"\"\"Loads goals from the specified JSON file.\"\"\"\n        if not self.goals_path.exists():\n            self.logger.info(f\"Goals file not found at {self.goals_path}. Starting with no goals.\")\n            return\n\n        try:\n            with open(self.goals_path, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                # If data is a list, assume it's directly the list of goal items\n                if isinstance(data, list):\n                    goal_items = data\n                else: # Otherwise, assume it's a dict with a \"goals\" key\n                    goal_items = data.get(\"goals\", [])\n\n                for item in goal_items:\n                    # Map 'id' from JSON to 'goal_id' for Goal constructor\n                    item['goal_id'] = item.pop('id') \n                    self.goals.append(Goal(**item))\n        except json.JSONDecodeError:\n            self.logger.error(f\"Error decoding goals JSON from {self.goals_path}. File might be corrupted.\")\n        except Exception as e:\n            self.logger.error(f\"Error loading goals from {self.goals_path}: {e}\")\n\n    def save_goals(self):\n        \"\"\"Saves the current state of goals back to the JSON file.\"\"\"\n        # Always save as a dictionary with a \"goals\" key\n        data = {\"goals\": [goal.to_dict() for goal in self.goals]}\n        try:\n            with open(self.goals_path, 'w', encoding='utf-8') as f:\n                json.dump(data, f, indent=2)\n        except Exception as e: # Catch any file-related errors\n            self.logger.error(f\"Error saving goals to {self.goals_path}: {e}\")\n\n    def next_goal(self) -> Optional[Goal]:\n        \"\"\"Returns the next pending goal, or None if no more pending goals.\"\"\"\n        while self._current_goal_index < len(self.goals):\n            goal = self.goals[self._current_goal_index]\n            if goal.status == \"pending\":\n                return goal\n            self._current_goal_index += 1\n        return None\n\n    def mark_done(self, goal_id: str):\n        \"\"\"Marks a goal as completed.\"\"\"\n        for goal in self.goals:\n            if goal.goal_id == goal_id:\n                goal.status = \"completed\"\n                self.save_goals()\n                self.logger.info(f\"Goal '{goal_id}' marked as completed.\")\n                return\n        self.logger.warning(f\"Goal '{goal_id}' not found when trying to mark as done.\")\n\n    def add_goal(self, goal: Goal):\n        \"\"\"Adds a new goal to the manager.\"\"\"\n        self.goals.append(goal)\n        self.save_goals()\n        self.logger.info(f\"Added new goal: {goal.goal_id}\")\n\n    def add_goal_from_dict(self, goal_data: Dict[str, Any]):\n        \"\"\"Adds a new goal from a dictionary.\"\"\"\n        self.goals.append(Goal(goal_data[\"id\"], goal_data[\"description\"], goal_data.get(\"status\", \"pending\")))\n        self.save_goals()\n        self.logger.info(f\"Added new goal: {goal_data['id']}\")\n\n\n# File: src\\ai_self_ext_engine\\model_client.py\nimport os\nimport logging # Import logging\nfrom typing import Any, Dict, List, Optional\nfrom google import genai\nfrom .config import ModelSectionConfig # Import ModelSectionConfig\n\nclass ModelCallError(Exception):\n    \"\"\"Custom exception for errors during model calls.\"\"\"\n    pass\n\nclass ModelClient:\n    \"\"\"\n    Handles interactions with the Gemini API for various model calls.\n    \"\"\"\n    def __init__(self, config: ModelSectionConfig): # Accept ModelSectionConfig\n        self.config = config\n        self.logger = logging.getLogger(__name__) # Get logger for ModelClient\n        try:\n            api_key = os.environ.get(self.config.api_key_env)\n            if not api_key:\n                raise ValueError(f\"Environment variable '{self.config.api_key_env}' not set.\")\n            self._client = genai.Client(api_key=api_key)\n        except Exception as e:\n            self.logger.error(\"Error initializing Gemini client: %s\", e)\n            raise ValueError(f\"Error initializing Gemini client: {e}\")\n\n    def call_model(\n        self,\n        model_name: str,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        dry_run: bool = False,\n        **kwargs # For any other model-specific parameters\n    ) -> str:\n        \"\"\"\n        Makes a call to the specified Gemini model with prompt and system prompt.\n        \"\"\"\n        if dry_run:\n            self.logger.info(f\"Dry run: Model '{model_name}' would be called with prompt:\\n{prompt}\")\n            return \"DRY_RUN_RESPONSE\"\n\n        try:\n            # Construct contents based on system_prompt presence\n            contents = []\n            if system_prompt:\n                contents.append({\"role\": \"user\", \"parts\": [{\"text\": system_prompt}]})\n                contents.append({\"role\": \"model\", \"parts\": [{\"text\": \"Okay, I understand.\"}]}) # Standard response to system prompt\n            contents.append({\"role\": \"user\", \"parts\": [{\"text\": prompt}]})\n\n            response = self._client.models.generate_content(\n                model=model_name, # Pass model_name as a keyword argument\n                contents=contents,\n                **kwargs # Pass any remaining kwargs directly to generate_content\n            )\n            \n            if response.text is None:\n                raise ModelCallError(f\"Model '{model_name}' returned no text response.\")\n            \n            return response.text\n\n        except Exception as e:\n            self.logger.error(\"Failed to call model '%s': %s\", model_name, e)\n            raise ModelCallError(f\"Failed to call model '{model_name}': {e}\")\n\n\n# File: src\\ai_self_ext_engine\\package_smoke_test.py\nimport os\nimport tempfile\nfrom pathlib import Path\n\n# Relative imports for the package\nfrom .config import MainConfig, EngineSectionConfig, ModelSectionConfig, LoggingConfig\nfrom .core.engine import Engine\n\ndef run_smoke_test():\n    \"\"\"\n    Executes a basic smoke test for the AI Self-Extending Engine package.\n    This test verifies that the Engine can be instantiated with a minimal\n    configuration and its dependencies can be resolved.\n    It uses temporary directories and mocks API keys to avoid side effects.\n    \"\"\"\n    print(\"Running AI Self-Extending Engine package smoke test...\")\n\n    # Create temporary directories for config paths\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        tmp_path = Path(tmp_dir)\n        temp_goals_path = tmp_path / \"goals.json\"\n        temp_memory_path = tmp_path / \"memory\"\n        temp_code_dir = tmp_path / \"src\"\n        temp_prompts_dir = tmp_path / \"prompts\"\n        temp_prompts_dir.mkdir(parents=True, exist_ok=True) # Ensure it exists for config validation\n\n        # Minimal valid config for instantiation\n        dummy_config_data = {\n            \"version\": 1,\n            \"engine\": {\n                \"code_dir\": str(temp_code_dir),\n                \"max_cycles\": 1,\n                \"memory_path\": str(temp_memory_path),\n                \"goals_path\": str(temp_goals_path),\n                \"prompts_dir\": str(temp_prompts_dir)\n            },\n            \"model\": {\n                \"api_key_env\": \"AI_SELF_EXT_ENGINE_DUMMY_KEY\",\n                \"model_name\": \"dummy-model\"\n            },\n            \"roles\": [], # No actual roles needed for basic instantiation test\n            \"plugins\": {},\n            \"logging\": {\n                \"level\": \"INFO\",\n                \"format\": \"plain\"\n            }\n        }\n\n        # Temporarily set dummy API key to allow ModelClient initialization\n        original_env_value = os.environ.get(dummy_config_data[\"model\"][\"api_key_env\"])\n        os.environ[dummy_config_data[\"model\"][\"api_key_env\"]] = \"sk-dummy-key-for-test-only\"\n\n        try:\n            config = MainConfig(**dummy_config_data)\n            engine = Engine(config)\n            print(\"Engine instantiated successfully.\")\n        finally:\n            # Clean up dummy API key\n            if original_env_value is not None:\n                os.environ[dummy_config_data[\"model\"][\"api_key_env\"]] = original_env_value\n            else:\n                del os.environ[dummy_config_data[\"model\"][\"api_key_env\"]]\n\n    print(\"AI Self-Extending Engine package smoke test completed successfully.\")\n\nif __name__ == \"__main__\":\n    run_smoke_test()\n\n\n# File: src\\ai_self_ext_engine\\simple_test_module.py\ndef add_one(number: int) -> int:\n    return number + 1\n\n\n# File: src\\ai_self_ext_engine\\snapshot_store.py\nimport json\nimport os # Import os\nfrom pathlib import Path\nfrom typing import Any, Dict, Optional\nfrom datetime import datetime\n\n# Assuming Context is defined in ai_self_ext_engine.core.role\nfrom ai_self_ext_engine.core.role import Context\n\nclass SnapshotStore:\n    \"\"\"\n    Manages the storage and retrieval of improvement cycle snapshots.\n    Each snapshot includes code, critiques, and other relevant metadata.\n    \"\"\"\n    def __init__(self, memory_path: str):\n        self.memory_dir = Path(memory_path) # Use relative path\n        self.memory_dir.mkdir(parents=True, exist_ok=True) # Ensure directory exists\n\n    def record(self, context: Context):\n        \"\"\"\n        Records a snapshot of the current context.\n        Each goal will have its own subdirectory.\n        \"\"\"\n        if not context.goal:\n            print(\"Cannot record snapshot: No goal in context.\")\n            return\n\n        goal_snapshot_dir = self.memory_dir / context.goal.goal_id\n        goal_snapshot_dir.mkdir(parents=True, exist_ok=True) # Ensure goal-specific directory exists\n\n        # Sanitize timestamp for filename: replace colons with hyphens\n        timestamp = context.metadata.get(\"timestamp\", datetime.now().isoformat()).replace(\":\", \"-\")\n        snapshot_file_path = os.path.join(str(goal_snapshot_dir), f\"{timestamp}.json\")\n        \n        # Prepare data for serialization\n        snapshot_data = {\n            \"cycle\": context.metadata.get(\"cycle\"),\n            \"goal_id\": context.goal.goal_id,\n            \"description\": context.goal.description,\n            \"current_code\": context.current_code,\n            \"todos\": context.todos,\n            \"patch\": context.patch,\n            \"test_results\": context.test_results,\n            \"accepted\": context.accepted,\n            \"should_abort\": context.should_abort,\n            \"metadata\": context.metadata\n        }\n\n        try:\n            with open(snapshot_file_path, 'w', encoding='utf-8') as f: # Use the new path variable\n                json.dump(snapshot_data, f, indent=2)\n            print(f\"Snapshot recorded for goal '{context.goal.goal_id}' at {snapshot_file_path}\")\n        except Exception as e:\n            print(f\"Error recording snapshot for goal '{context.goal.goal_id}': {e}\")\n\n    def has(self, goal: Any) -> bool:\n        \"\"\"\n        Checks if a snapshot for a given goal already exists.\n        For simplicity, it just checks if the goal's directory exists and is not empty.\n        \"\"\"\n        goal_snapshot_dir = self.memory_dir / goal.goal_id\n        return goal_snapshot_dir.exists() and any(goal_snapshot_dir.iterdir())\n\n    def load_latest(self, goal_id: str) -> Optional[Context]:\n        \"\"\"\n        Loads the latest snapshot for a given goal.\n        (Implementation can be more sophisticated to find actual latest by timestamp)\n        \"\"\"\n        goal_snapshot_dir = self.memory_dir / goal_id\n        if not goal_snapshot_dir.exists():\n            return None\n        \n        # For simplicity, just pick the first json file found\n        for f in goal_snapshot_dir.iterdir():\n            if f.suffix == \".json\":\n                try:\n                    with open(f, 'r', encoding='utf-8') as sf:\n                        data = json.load(sf)\n                        # Reconstruct Context object (simplified)\n                        context = Context(\n                            code_dir=data.get(\"code_dir\", \".\"), # Assuming code_dir is stored\n                            current_code=data.get(\"current_code\"),\n                            goal=None, # Need to load Goal object separately if needed\n                            todos=data.get(\"todos\", []),\n                            patch=data.get(\"patch\"),\n                            test_results=data.get(\"test_results\"),\n                            accepted=data.get(\"accepted\", False),\n                            should_abort=data.get(\"should_abort\", False),\n                            metadata=data.get(\"metadata\", {})\n                        )\n                        return context\n                except Exception as e:\n                    print(f\"Error loading snapshot from {f}: {e}\")\n        return None\n\n\n# File: src\\ai_self_ext_engine\\__init__.py\n# This file makes 'src' a Python package.\n\n\n# File: src\\ai_self_ext_engine\\config\\config.py\n# src/ai_self_ext_engine/config/config.py\n# Default configuration settings for the AI Self-Extending Engine.\n# This can be used as a template for engine_config.yaml.\n\ndefault_config_yaml = \"\"\"\nversion: 1\nengine:\n  code_dir: ./src\n  max_cycles: 3\n  memory_path: ./memory\n  goals_path: goals.json\n  prompts_dir: prompts\nmodel:\n  api_key_env: GEMINI_API_KEY\n  model_name: gemini-2.5-flash\nroles:\n  - module: ai_self_ext_engine.roles.problem_identification\n    class: ProblemIdentificationRole\n    prompt_path: problem_identification.tpl\n  - module: ai_self_ext_engine.roles.refine\n    class: RefineRole\n    prompt_path: patch_generation.tpl\n  - module: ai_self_ext_engine.roles.test\n    class: TestRole\n    prompt_path: N/A # TestRole does not use a prompt template directly\n  - module: ai_self_ext_engine.roles.self_review\n    class: SelfReviewRole\n    prompt_path: N/A # SelfReviewRole does not use a prompt template directly\nplugins: {}\nlogging:\n\n\n# File: src\\ai_self_ext_engine\\core\\engine.py\nfrom typing import List, Optional, Any, Dict\nfrom datetime import datetime\nfrom importlib import import_module\nfrom .role import Context, Role\nfrom .plugin import Plugin # Import Plugin\nfrom ..config import MainConfig, RoleConfig, PluginConfig # Import PluginConfig\nfrom pathlib import Path\nimport logging\n\n\nfrom ..goal_manager import GoalManager, Goal\nfrom ..snapshot_store import SnapshotStore\nfrom ..model_client import ModelClient\n\nclass Engine:\n    \"\"\"\n    Orchestrates the self-improvement process, managing cycles,\n    goals, roles, and snapshots.\n    \"\"\"\n    logger = logging.getLogger(__name__)\n\n    def __init__(self, config: MainConfig):\n        self.config = config\n        self.goal_manager = GoalManager(self.config.engine.goals_path)\n        self.snapshot_store = SnapshotStore(self.config.engine.memory_path)\n        self.model_client = ModelClient(self.config.model)\n\n        # Ensure core directories exist for the project structure\n        Path(self.config.engine.code_dir).mkdir(parents=True, exist_ok=True)\n        # We no longer explicitly create src/core/roles as they are part of the package structure\n        # and should be handled by the packaging system or user's project setup.\n\n        self.roles = self._load_roles(config.roles)\n        self.plugins = self._load_plugins(config.plugins) # Load plugins\n\n    def _load_roles(self, role_configs: List[RoleConfig]) -> List[Role]:\n        \"\"\"\n        Dynamically loads and instantiates roles based on the role_configs in config.\n        \"\"\"\n        loaded_roles: List[Role] = []\n        for role_conf in role_configs:\n            try:\n                # Dynamically import the module\n                module = import_module(role_conf.module)\n                # Get the role class from the module\n                role_class = getattr(module, role_conf.class_name)\n                loaded_roles.append(role_class(self.config, self.model_client))\n            except (ImportError, AttributeError, TypeError) as e:\n                self.logger.exception(\"Error loading role '%s' from module '%s': %s\", role_conf.class_name, role_conf.module, e)\n                raise # Re-raise to stop execution if a critical role can't be loaded\n        return loaded_roles\n\n    def _load_plugins(self, plugin_configs: Dict[str, PluginConfig]) -> Dict[str, Plugin]:\n        \"\"\"\n        Dynamically loads and instantiates plugins based on the plugin_configs in config.\n        \"\"\"\n        loaded_plugins: Dict[str, Plugin] = {}\n        for plugin_name, plugin_conf in plugin_configs.items():\n            try:\n                module_path, class_name = plugin_conf.entry_point.rsplit('.', 1)\n                module = import_module(module_path)\n                plugin_class = getattr(module, class_name)\n                loaded_plugins[plugin_name] = plugin_class(self.config) # Assuming plugin constructor takes config\n            except (ImportError, AttributeError, TypeError) as e:\n                self.logger.exception(\"Error loading plugin '%s' from entry point '%s': %s\", plugin_name, plugin_conf.entry_point, e)\n                raise # Re-raise to stop execution if a critical plugin can't be loaded\n        return loaded_plugins\n\n    def run_cycles(self):\n        \"\"\"\n        Main loop for the self-improvement process.\n        \"\"\"\n        self.logger.info(\"Starting self-improvement engine cycles...\")\n        \n        # Outer loop: Iterate through pending goals\n        while True:\n            # Initialize context for the current goal iteration\n            # A new context object is created for each *new* goal,\n            # but its state might be loaded from a snapshot if resuming.\n            context = Context(code_dir=self.config.engine.code_dir)\n\n            # Get the next pending goal\n            context.goal = self.goal_manager.next_goal()\n            if not context.goal:\n                self.logger.info(\"No more pending goals. Exiting.\")\n                break\n\n            self.logger.info(\"\\n--- Processing Goal: %s - %s ---\", context.goal.goal_id, context.goal.description)\n\n            # Try to load the latest snapshot for this goal to resume progress\n            loaded_snapshot_context = self.snapshot_store.load_latest(context.goal.goal_id)\n            if loaded_snapshot_context:\n                # Preserve the current Goal object from GoalManager (which is the source of truth for 'pending')\n                # and overlay other state from the snapshot.\n                loaded_snapshot_context.goal = context.goal\n                context = loaded_snapshot_context\n                self.logger.info(\"Resuming goal '%s' from previous snapshot.\", context.goal.goal_id)\n            else:\n                self.logger.info(\"Starting new attempt for goal '%s'.\", context.goal.goal_id)\n                # For a newly started goal, ensure todos are fresh (ProblemIdentification will populate them)\n                context.todos = [] \n            \n            # Inner loop: Multiple attempts for the current goal\n            for attempt in range(self.config.engine.max_cycles):\n                self.logger.info(\"\\n--- Goal '%s' Attempt %s/%s ---\", context.goal.goal_id, attempt + 1, self.config.engine.max_cycles)\n\n                # Reset transient states at the beginning of each new attempt\n                context.patch = None\n                context.test_results = None\n                context.accepted = False\n                context.should_abort = False\n                \n                # Update metadata for the current attempt\n                context.metadata[\"current_attempt\"] = attempt + 1\n                context.metadata[\"max_attempts_for_goal\"] = self.config.engine.max_cycles\n                context.metadata[\"timestamp\"] = datetime.now().isoformat()\n\n                # Execute roles for the current attempt\n                attempt_aborted_by_role = False\n                for role in self.roles:\n                    self.logger.info(\"Executing role: %s\", role.__class__.__name__)\n                    context = role.run(context)\n                    if context.should_abort:\n                        self.logger.warning(\"Role %s requested abort. Stopping current goal attempt.\", role.__class__.__name__)\n                        attempt_aborted_by_role = True\n                        break # Break out of role loop, proceed to snapshot and then next attempt or goal\n                \n                # Record snapshot after each attempt (even if aborted or failed)\n                self.snapshot_store.record(context)\n\n                if context.accepted:\n                    self.goal_manager.mark_done(context.goal.goal_id)\n                    self.logger.info(\"Goal '%s' successfully completed after %s attempts.\", context.goal.goal_id, attempt + 1)\n                    break\n                elif attempt_aborted_by_role:\n                    self.logger.warning(\"Goal '%s' attempt %s aborted due to a role's request. Moving to next goal.\", context.goal.goal_id, attempt + 1)\n                    break # Move to the next pending goal immediately if a role explicitly aborted\n\n\n# File: src\\ai_self_ext_engine\\core\\plugin.py\nfrom abc import abstractmethod\nfrom typing import Any, Optional, Protocol # Import Protocol\n\nclass Plugin(Protocol): # Change to Protocol\n    \"\"\"\n    Protocol for all plugins in the self-extending engine.\n    Plugins provide specific capabilities, such as language support or tool integration.\n    \"\"\"\n    @abstractmethod\n    def detect(self, code: str) -> bool:\n        \"\"\"\n        Detects if the plugin is applicable to the given code.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def execute(self, command: str, **kwargs) -> Any:\n        \"\"\"\n        Executes a command specific to the plugin's capability.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def test(self, code: str, tests: Optional[str] = None) -> Any:\n        \"\"\"\n        Runs tests for the given code using the plugin's testing framework.\n        \"\"\"\n        pass\n\n\n# File: src\\ai_self_ext_engine\\core\\role.py\nfrom abc import abstractmethod\nfrom typing import Any, Dict, List, Optional, TypeVar, Protocol # Import Protocol\nfrom dataclasses import dataclass, field\n\n@dataclass\nclass Context:\n    \"\"\"\n    The central data object passed between roles, containing all relevant\n    information for the current improvement cycle.\n    \"\"\"\n    code_dir: str\n    current_code: Optional[str] = None\n    goal: Optional[Any] = None  # Will be a Goal object from GoalManager\n    todos: List[str] = field(default_factory=list)\n    patch: Optional[str] = None\n    test_results: Optional[Any] = None # Will be a TestResults object\n    accepted: bool = False\n    should_abort: bool = False\n    metadata: Dict[str, Any] = field(default_factory=dict) # For logging additional info\n\nclass Role(Protocol): # Change to Protocol\n    \"\"\"\n    Protocol for all roles in the self-improvement loop.\n    Each role performs a specific task and updates the Context.\n    \"\"\"\n    @abstractmethod\n    def run(self, context: Context) -> Context:\n        \"\"\"\n        Executes the role's logic and returns an updated Context object.\n        \"\"\"\n        pass\n# Define a type variable for roles to enable type hinting for subclasses\nRoleType = TypeVar('RoleType', bound=Role)\n\n\n# File: src\\ai_self_ext_engine\\core\\__init__.py\n# src/core/__init__.py\n\nfrom .engine import Engine\nfrom .role import Context, Role, RoleType # Context is defined in role.py, not engine.py\nfrom .plugin import Plugin\n# GoalManager is not exposed directly in __init__.py, but Goal is\nfrom ..goal_manager import Goal\nfrom ..model_client import ModelClient, ModelCallError\n\n\n# File: src\\ai_self_ext_engine\\plugins\\__init__.py\n# This file makes the 'plugins' directory a Python package.\n\n\n# File: src\\ai_self_ext_engine\\plugins\\python\\PythonPlugin.py\nfrom typing import Any, Optional\nfrom ai_self_ext_engine.core.plugin import Plugin\nfrom ai_self_ext_engine.config import MainConfig # Import MainConfig\n\nclass PythonPlugin(Plugin):\n    \"\"\"\n    A placeholder Python plugin for the AI Self-Extending Engine.\n    This demonstrates how a plugin would be structured and loaded.\n    \"\"\"\n    def __init__(self, config: MainConfig):\n        self.config = config\n        # In a real plugin, you might initialize tools or configurations specific to Python.\n\n    def detect(self, code: str) -> bool:\n        \"\"\"\n        Detects if the plugin is applicable to the given code.\n        For this example, it always returns True, assuming it's a Python project.\n        \"\"\"\n        return True\n\n    def execute(self, command: str, **kwargs) -> Any:\n        \"\"\"\n        Executes a command specific to the plugin's capability.\n        (Placeholder for actual implementation)\n        \"\"\"\n        print(f\"PythonPlugin: Executing command: {command} with kwargs: {kwargs}\")\n        return {\"status\": \"executed\", \"output\": f\"Dummy output for: {command}\"}\n\n    def test(self, code: str, tests: Optional[str] = None) -> Any:\n        \"\"\"\n        Runs tests for the given code using the plugin's testing framework.\n        (Placeholder for actual implementation)\n        \"\"\"\n        print(f\"PythonPlugin: Running tests for code. Tests: {tests}\")\n        return {\"passed\": True, \"details\": \"Dummy test results\"}\n\n\n# File: src\\ai_self_ext_engine\\plugins\\python\\__init__.py\n# This file makes the 'python' directory a Python package.\nfrom .PythonPlugin import PythonPlugin\n\n\n# File: src\\ai_self_ext_engine\\roles\\problem_identification.py\nfrom typing import List\nfrom pathlib import Path # Import Path\nfrom ai_self_ext_engine.core.role import Role, Context\nfrom ai_self_ext_engine.model_client import ModelClient, ModelCallError\nfrom ai_self_ext_engine.config import MainConfig # Use MainConfig\nimport re # Import re for regex\nimport logging # New import\n\nlogger = logging.getLogger(__name__) # New logger\n\nclass ProblemIdentificationRole(Role):\n    def __init__(self, config: MainConfig, model_client: ModelClient):\n        self.config = config\n        self.model_client = model_client\n        # Prompts directory is now part of the engine config, not directly in the root of EngineConfig\n        self.prompt_template_path = Path(config.engine.prompts_dir) / \"problem_identification.tpl\"\n\n    def run(self, context: Context) -> Context:\n        if not context.goal:\n            print(\"ProblemIdentificationRole: No goal in context. Skipping.\")\n            return context\n\n        logger.info(\"ProblemIdentificationRole: Identifying problems for goal '%s'...\", context.goal.goal_id)\n\n        try:\n            # Load prompt template from file\n            if not self.prompt_template_path.exists():\n                raise FileNotFoundError(f\"Prompt template not found at {self.prompt_template_path}\")\n            \n            prompt_template = self.prompt_template_path.read_text(encoding=\"utf-8\")\n            prompt = prompt_template.format(goal_description=context.goal.description)\n            response_text = self.model_client.call_model(\n                self.config.model.model_name, # Use the model name from the new config structure\n                prompt=prompt\n            )\n            \n            # Simple parsing for comma-separated list\n            todos = [todo.strip() for todo in response_text.split(',') if todo.strip()]\n            \n            # Re-apply a minimal set of exclusions for safety, primarily to avoid empty __init__.py files\n            filtered_todos = []\n            excluded_patterns = [\n                r\"create empty `__init__.py`\",\n                r\"verify all directories and `__init__.py` files exist\",\n                r\"create missing core engine subdirectories\",\n                r\"initialize core engine subdirectories with `__init__.py`\"\n            ]\n\n            for todo in todos:\n                if not any(re.search(pattern, todo, re.IGNORECASE) for pattern in excluded_patterns):\n                    filtered_todos.append(todo)\n\n            context.todos = filtered_todos\n            logger.info(\"ProblemIdentificationRole: Identified todos: %s\", filtered_todos)\n\n        except ModelCallError as e:\n            logger.error(\"ProblemIdentificationRole: Model call error: %s\", e)\n            context.should_abort = True # Abort if model call fails\n        except Exception as e:\n            logger.exception(\"ProblemIdentificationRole: An unexpected error occurred: %s\", e)\n            context.should_abort = True # Abort on other errors\n\n        return context\n\n\n# File: src\\ai_self_ext_engine\\roles\\refine.py\nfrom typing import List\nfrom pathlib import Path # Import Path\nfrom ai_self_ext_engine.core.role import Role, Context\nfrom ai_self_ext_engine.model_client import ModelClient, ModelCallError\nfrom ai_self_ext_engine.config import MainConfig # Use MainConfig\nimport subprocess\nimport os # Import os\nimport logging # New import\n\nlogger = logging.getLogger(__name__) # New logger\n\nclass RefineRole(Role):\n    def __init__(self, config: MainConfig, model_client: ModelClient):\n        self.config = config\n        self.model_client = model_client\n        self.prompt_template_path = Path(config.engine.prompts_dir) / \"patch_generation.tpl\"\n\n    def run(self, context: Context) -> Context:\n        if not context.todos:\n            logger.info(\"RefineRole: No todos in context. Skipping.\")\n            return context\n\n        logger.info(\"RefineRole: Generating and applying patch for todos: %s\", context.todos)\n\n        try:\n            current_code = self._read_code_from_dir(self.config.engine.code_dir) # Use config.engine.code_dir\n            context.current_code = current_code # Update context with current code\n\n            # Load prompt template from file\n            if not self.prompt_template_path.exists():\n                raise FileNotFoundError(f\"Prompt template not found at {self.prompt_template_path}\")\n            \n            prompt_template = self.prompt_template_path.read_text(encoding=\"utf-8\")\n            \n            prompt = prompt_template.format(\n                current_code=current_code,\n                todos=\"\\n\".join([f\"- {todo}\" for todo in context.todos])\n            )\n            # The LLM is expected to output only the patch. We might need to strip leading/trailing text.\n            patch = self.model_client.call_model(\n                self.config.model.model_name, # Use model from config\n                prompt=prompt\n            ).strip()\n\n            # Ensure the patch starts with the diff header\n            if not patch.startswith(\"---\"):\n                # Attempt to find the start of the patch if the LLM added preamble\n                patch_start_idx = patch.find(\"--- \")\n                if patch_start_idx != -1:\n                    patch = patch[patch_start_idx:]\n                else:\n                    logger.warning(\"RefineRole: Generated patch does not start with '---'. May be invalid.\")\n\n            context.patch = patch\n            logger.debug(\"RefineRole: Generated patch:\\n%s\", patch)\n            if patch:\n                # Use the actual current working directory as cwd for git apply\n                if self._apply_patch(patch, os.getcwd()): # Use os.getcwd()\n                    logger.info(\"RefineRole: Patch applied successfully.\")\n                else:\n                    logger.error(\"RefineRole: Failed to apply patch. Aborting cycle.\")\n                    context.should_abort = True\n            else:\n                logger.info(\"RefineRole: No patch generated. Skipping application.\")\n\n        except ModelCallError as e:\n            logger.error(\"RefineRole: Model call error: %s\", e)\n            context.should_abort = True\n        except Exception as e:\n            logger.exception(\"RefineRole: An unexpected error occurred: %s\", e)\n            context.should_abort = True\n\n        return context\n\n    def _read_code_from_dir(self, code_dir: str) -> str:\n        \"\"\"\n        Reads all Python files in the specified directory and concatenates them.\n        This is a simplified approach for demonstration.\n        \"\"\"\n        full_code = []\n        # Construct the absolute path to the ai_self_ext_engine package\n        target_code_dir = Path(os.getcwd()) / self.config.engine.code_dir / \"ai_self_ext_engine\"\n        if not target_code_dir.exists():\n            logger.warning(\"Code directory %s does not exist.\", target_code_dir)\n            return \"\"\n\n        # Exclude common temporary/generated directories\n        exclude_dirs = [\"__pycache__\", \"sim_memory\", \"_memory_snapshots\"]\n        \n        # Walk through the target code directory, excluding specified paths\n        for root, dirs, files in os.walk(target_code_dir):\n            # Modify dirs in-place to prune the search\n            dirs[:] = [d for d in dirs if d not in exclude_dirs]\n\n            for file_name in files:\n                if file_name.endswith(\".py\"):\n                    file_path = Path(root) / file_name\n                    try:\n                        # Ensure we don't try to read temp.patch or other non-source files\n                        if file_path.name == \"temp.patch\" or file_path.name.startswith(\"test_\"):\n                            continue\n                        \n                        # Adjust relative path to be relative to the project root for display purposes\n                        relative_path = file_path.relative_to(os.getcwd())\n                        full_code.append(f\"# File: {relative_path}\\n\")\n                        full_code.append(file_path.read_text(encoding=\"utf-8\"))\n                        full_code.append(\"\\n\\n\")\n                    except Exception as e:\n                        logger.warning(\"Could not read %s: %s\", file_path, e)\n        return \"\".join(full_code)\n\n    def _apply_patch(self, patch_text: str, cwd: str) -> bool:\n        \"\"\"\n        Applies a patch to the codebase using git apply.\n        Temporarily always returns True to allow cycle completion for testing.\n        \"\"\"\n        if not patch_text:\n            return False\n        \n        # Temporarily bypass actual git apply for testing full cycle\n        logger.warning(\"Skipping actual patch application for full cycle test. Patch content:\\n%s\", patch_text)\n        return True\n        \n        # Uncomment and re-enable the following for actual patch application:\n        # try:\n        #     # Create a temporary patch file in the current working directory (root)\n        #     patch_file_path = Path(\"./temp.patch\")\n        #     patch_file_path.write_text(patch_text, encoding=\"utf-8\")\n\n        #     # Apply the patch using git from the specified cwd\n        #     subprocess.run(\n        #         [\"git\", \"apply\", str(patch_file_path)],\n        #         check=True,\n        #         cwd=cwd,\n        #         capture_output=True,\n        #     )\n        #     patch_file_path.unlink()  # Delete the temporary patch file\n        #     return True\n        # except subprocess.CalledProcessError as e:\n        #     logger.error(\"Error applying patch: %s\", e)\n        #     if isinstance(e, subprocess.CalledProcessError):\n        #         logger.error(\"Patch stderr:\\n%s\", e.stderr.decode())\n        #     return False\n        # except FileNotFoundError as e:\n        #     logger.error(\"Error: git command not found. %s\", e)\n        #     return False\n\n\n# File: src\\ai_self_ext_engine\\roles\\self_review.py\nfrom typing import Any, Dict, Optional\nfrom ai_self_ext_engine.core.role import Role, Context\nfrom ai_self_ext_engine.model_client import ModelClient, ModelCallError\nfrom ai_self_ext_engine.config import MainConfig\nimport subprocess # For git reset\nimport os # Import os for os.getcwd()\nimport logging # New import\n\nlogger = logging.getLogger(__name__) # New logger\n\nclass SelfReviewRole(Role):\n    \"\"\"\n    Role responsible for evaluating the acceptance of changes based on\n    test results, identified problems, and potentially a meta-critique.\n    \"\"\"\n    def __init__(self, config: MainConfig, model_client: ModelClient):\n        self.config = config\n        self.model_client = model_client\n\n    def run(self, context: Context) -> Context:\n        if context.should_abort:\n            logger.info(\"SelfReviewRole: Context aborted. Skipping self-review.\")\n            return context\n\n        logger.info(\"SelfReviewRole: Evaluating changes...\")\n\n        # Step 1: Evaluate based on test results\n        if context.test_results and context.test_results.get(\"passed\") is False:\n            logger.info(\"SelfReviewRole: Tests failed. Changes are not accepted.\")\n            context.accepted = False\n            self._git_reset_all(os.getcwd()) # Revert changes from project root\n            context.should_abort = True # Abort cycle on failed tests\n            return context\n        elif context.test_results and context.test_results.get(\"passed\") is True:\n            logger.info(\"SelfReviewRole: Tests passed. Provisionally accepted.\")\n            context.accepted = True # Still provisionally, might be overridden by other rules\n        else:\n            logger.info(\"SelfReviewRole: No test results available or tests not run.\")\n            # Decide on default behavior if no tests: provisionally accept or require more info\n            context.accepted = False # Default to not accepted if no tests or inconclusive\n\n        # Step 2: Evaluate acceptance via rules (placeholder for actual RuleEngine integration)\n        # Assuming rule_engine.evaluate would return True/False for acceptance\n        # if self.rule_engine.evaluate(context.goal, context.todos, context.test_results):\n        #     logger.info(\"SelfReviewRole: Rules engine accepted changes.\")\n        #     context.accepted = True\n        # else:\n        #     logger.info(\"SelfReviewRole: Rules engine rejected changes.\")\n        #     context.accepted = False\n        #     self._git_reset_all(context.code_dir)\n        #     context.should_abort = True\n\n        # Step 3: Optional: Meta-critique/Review Analysis (placeholder)\n        # if self.config.meta_critic_model and context.patch:\n        #     try:\n        #         analysis_prompt = self.review_analyzer.get_analysis_prompt(context.patch, context.test_results)\n        #         meta_critique_response = self.model_client.call_model(\n        #             model_name=self.config.meta_critic_model,\n        #             prompt=analysis_prompt\n        #         )\n        #         # Parse meta_critique_response to determine final acceptance\n        #         # For now, let's assume if it contains \"REJECT\" it's rejected\n        #         if \"REJECT\" in meta_critique_response.upper():\n        #             logger.info(\"SelfReviewRole: Meta-critique rejected changes.\")\n        #             context.accepted = False\n        #             self._git_reset_all(context.code_dir)\n        #             context.should_abort = True\n        #         else:\n        #             logger.info(\"SelfReviewRole: Meta-critique accepted changes.\")\n        #             context.accepted = True\n        #     except ModelCallError as e:\n        #         logger.error(f\"SelfReviewRole: Meta-critique model call error: {e}\")\n        #     except Exception as e:\n        #         logger.exception(f\"SelfReviewRole: Error during meta-critique analysis: {e}\")\n\n\n        if context.accepted:\n            logger.info(\"SelfReviewRole: Changes are accepted. Marking goal as completed in next step.\")\n        else:\n            logger.info(\"SelfReviewRole: Changes are NOT accepted. Reverting code and aborting cycle.\")\n            self._git_reset_all(os.getcwd()) # Revert changes from project root\n            context.should_abort = True # Ensure cycle is aborted if not accepted\n            \n        return context\n\n    def _git_reset_all(self, cwd: str):\n        \"\"\"Resets all changes in the git repository.\"\"\"\n        try:\n            logger.info(\"SelfReviewRole: Resetting git repository in %s...\", cwd)\n            subprocess.run(\n                [\"git\", \"reset\", \"--hard\"],\n                check=True,\n                cwd=cwd,\n                capture_output=True,\n            )\n            logger.info(\"SelfReviewRole: Git reset successful.\")\n        except subprocess.CalledProcessError:\n            logger.exception(\"SelfReviewRole: Error resetting git.\")\n        except Exception:\n            logger.exception(\"SelfReviewRole: An unexpected error occurred during git reset.\")\n\n\n# File: src\\ai_self_ext_engine\\roles\\test.py\nimport subprocess\nimport os # Import os\nimport logging # New import\nfrom typing import Any, Dict, Optional\nfrom ai_self_ext_engine.core.role import Role, Context\nfrom ai_self_ext_engine.config import MainConfig # Use MainConfig\n\nlogger = logging.getLogger(__name__) # New logger\n\nclass TestRole(Role):\n    \"\"\"\n    Role responsible for running tests on the codebase and updating the context\n    with the test results.\n    \"\"\"\n    def __init__(self, config: MainConfig, model_client: Any): # model_client not directly used here but passed for consistency\n        self.config = config\n\n    def run(self, context: Context) -> Context:\n        if context.should_abort:\n            logger.info(\"TestRole: Context aborted. Skipping tests.\")\n            return context\n\n        logger.info(\"TestRole: Running tests...\")\n        \n        try:\n            # Assuming pytest is installed and tests are discoverable in code_dir\n            # For a more robust solution, we would use the Plugin interface\n            # to determine the correct test runner for the language/framework.\n            result = subprocess.run(\n                [\"pytest\"],\n                check=True,\n                cwd=os.getcwd(), # Use current working directory\n                capture_output=True,\n                text=True # Capture output as text\n            )\n            context.test_results = {\n                \"passed\": True,\n                \"stdout\": result.stdout,\n                \"stderr\": result.stderr,\n                \"returncode\": result.returncode\n            }\n            context.accepted = True # Tests passed, so changes are provisionally accepted\n            logger.info(\"TestRole: Tests passed successfully.\")\n\n        except subprocess.CalledProcessError as e:\n            context.test_results = {\n                \"passed\": False,\n                \"stdout\": e.stdout,\n                \"stderr\": e.stderr,\n                \"returncode\": e.returncode\n            }\n            context.accepted = False # Tests failed, so changes are not accepted\n            logger.error(\"TestRole: Tests failed. Output:\\n%s\\nErrors:\\n%s\", e.stdout, e.stderr)\n            # Optionally, revert changes if tests fail. This can be handled by SelfReviewRole.\n            # self._git_reset_all(context.code_dir) # Moved to SelfReviewRole for decision making\n        except FileNotFoundError:\n            logger.error(\"TestRole: Pytest command not found. Please ensure pytest is installed.\")\n            context.should_abort = True\n        except Exception as e:\n            logger.exception(\"TestRole: An unexpected error occurred during testing: %s\", e)\n            context.should_abort = True\n\n        return context\n\n\n# File: src\\ai_self_ext_engine\\roles\\__init__.py\n# src/roles/__init__.py\n\nfrom .problem_identification import ProblemIdentificationRole\nfrom .refine import RefineRole\nfrom .test import TestRole\nfrom .self_review import SelfReviewRole\n\n# Expose all roles for dynamic loading\n__all__ = [\n    \"ProblemIdentificationRole\",\n    \"RefineRole\",\n    \"TestRole\",\n    \"SelfReviewRole\",\n]\n\n\n# File: src\\ai_self_ext_engine\\tests\\__init__.py\n# This file makes the 'src/ai_self_ext_engine/tests' directory a Python package.\n\n\n",
  "todos": [
    "Implement the full critique-refine-test-self-review cycle orchestration within the core engine logic."
  ],
  "patch": "--- a/src/ai_self_ext_engine/roles/refine.py\n+++ b/src/ai_self_ext_engine/roles/refine.py\n@@ -107,31 +107,28 @@\n         if not patch_text:\n             return False\n         \n-        # Temporarily bypass actual git apply for testing full cycle\n-        logger.warning(\"Skipping actual patch application for full cycle test. Patch content:\\n%s\", patch_text)\n-        return True\n-        \n-        # Uncomment and re-enable the following for actual patch application:\n-        # try:\n-        #     # Create a temporary patch file in the current working directory (root)\n-        #     patch_file_path = Path(\"./temp.patch\")\n-        #     patch_file_path.write_text(patch_text, encoding=\"utf-8\")\n-\n-        #     # Apply the patch using git from the specified cwd\n-        #     subprocess.run(\n-        #         [\"git\", \"apply\", str(patch_file_path)],\n-        #         check=True,\n-        #         cwd=cwd,\n-        #         capture_output=True,\n-        #     )\n-        #     patch_file_path.unlink()  # Delete the temporary patch file\n-        #     return True\n-        # except subprocess.CalledProcessError as e:\n-        #     logger.error(\"Error applying patch: %s\", e)\n-        #     if isinstance(e, subprocess.CalledProcessError):\n-        #         logger.error(\"Patch stderr:\\n%s\", e.stderr.decode())\n-        #     return False\n-        # except FileNotFoundError as e:\n-        #     logger.error(\"Error: git command not found. %s\", e)\n-        #     return False\n+        try:\n+            # Create a temporary patch file in the current working directory (root)\n+            patch_file_path = Path(\"./temp.patch\")\n+            patch_file_path.write_text(patch_text, encoding=\"utf-8\")\n+\n+            # Apply the patch using git from the specified cwd\n+            subprocess.run(\n+                [\"git\", \"apply\", str(patch_file_path)],\n+                check=True,\n+                cwd=cwd,\n+                capture_output=True,\n+            )\n+            patch_file_path.unlink()  # Delete the temporary patch file\n+            return True\n+        except subprocess.CalledProcessError as e:\n+            logger.error(\"Error applying patch: %s\", e)\n+            if isinstance(e, subprocess.CalledProcessError):\n+                logger.error(\"Patch stderr:\\n%s\", e.stderr.decode())\n+            return False\n+        except FileNotFoundError as e:\n+            logger.error(\"Error: git command not found. %s\", e)\n+            return False\n+\n---",
  "test_results": {
    "passed": false,
    "stdout": "============================= test session starts =============================\nplatform win32 -- Python 3.12.10, pytest-8.4.1, pluggy-1.6.0\nrootdir: C:\\Users\\Michael\\projects\\AI-self-improvement-engine\nconfigfile: pyproject.toml\nplugins: anyio-4.9.0, json-report-1.5.0, metadata-3.1.1, mock-3.14.1\ncollected 12 items\n\nsrc\\ai_self_ext_engine\\tests\\test_goal_manager.py ......F....            [ 91%]\nsrc\\ai_self_ext_engine\\tests\\test_simple_module.py .                     [100%]\n\n================================== FAILURES ===================================\n___________________________ test_next_goal_pending ____________________________\n\npopulated_goals_file = WindowsPath('C:/Users/Michael/AppData/Local/Temp/pytest-of-Michael/pytest-10/test_next_goal_pending0/goals.json')\n\n    def test_next_goal_pending(populated_goals_file):\n        \"\"\"Test next_goal returns pending goals.\"\"\"\n        manager = GoalManager(str(populated_goals_file))\n        next_g = manager.next_goal()\n        assert next_g.goal_id == \"goal1\"\n        next_g = manager.next_goal() # Call again to get next pending\n>       assert next_g.goal_id == \"goal2\"\nE       AssertionError: assert 'goal1' == 'goal2'\nE         \nE         - goal2\nE         ?     ^\nE         + goal1\nE         ?     ^\n\nsrc\\ai_self_ext_engine\\tests\\test_goal_manager.py:82: AssertionError\n============================== warnings summary ===============================\nsrc\\ai_self_ext_engine\\config.py:39\nsrc\\ai_self_ext_engine\\config.py:39\n  C:\\Users\\Michael\\projects\\AI-self-improvement-engine\\src\\ai_self_ext_engine\\config.py:39: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    @validator('engine')\n\n..\\..\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323\n..\\..\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323\n  C:\\Users\\Michael\\.pyenv\\pyenv-win\\versions\\3.12.10\\Lib\\site-packages\\pydantic\\_internal\\_config.py:323: PydanticDeprecatedSince20: Support for class-based `config` is deprecated, use ConfigDict instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n    warnings.warn(DEPRECATION_MESSAGE, DeprecationWarning)\n\n-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n=========================== short test summary info ===========================\nFAILED src/ai_self_ext_engine/tests/test_goal_manager.py::test_next_goal_pending\n================== 1 failed, 11 passed, 4 warnings in 1.75s ===================\n",
    "stderr": "",
    "returncode": 1
  },
  "accepted": false,
  "should_abort": true,
  "metadata": {
    "current_attempt": 1,
    "max_attempts_for_goal": 3,
    "timestamp": "2025-07-13T12:48:56.852043"
  }
}