{
  "cycle": null,
  "goal_id": "auto_goal_2",
  "description": "Improve architecture: Plugin architecture and parallel processing improvements",
  "current_code": "# File: src/ai_self_ext_engine/model_client.py\nimport os\nimport logging # Import logging\nfrom typing import Any, Dict, List, Optional\nimport google.generativeai as genai\nfrom .config import ModelSectionConfig # Import ModelSectionConfig\n\nclass ModelCallError(Exception):\n    \"\"\"Custom exception for errors during model calls.\"\"\"\n    pass\n\nclass ModelClient:\n    \"\"\"\n    Handles interactions with the Gemini API for various model calls.\n    \"\"\"\n    def __init__(self, config: ModelSectionConfig): # Accept ModelSectionConfig\n        self.config = config\n        self.logger = logging.getLogger(__name__) # Get logger for ModelClient\n        try:\n            api_key = os.environ.get(self.config.api_key_env)\n            if not api_key:\n                raise ValueError(f\"Environment variable '{self.config.api_key_env}' not set.\")\n            genai.configure(api_key=api_key)\n            self._configured = True\n        except Exception as e:\n            self.logger.error(\"Error configuring Gemini API: %s\", e)\n            raise ValueError(f\"Error configuring Gemini API: {e}\")\n\n    def call_model(\n        self,\n        model_name: str,\n        prompt: str,\n        system_prompt: Optional[str] = None,\n        dry_run: bool = False,\n        **kwargs # For any other model-specific parameters\n    ) -> str:\n        \"\"\"\n        Makes a call to the specified Gemini model with prompt and system prompt.\n        \"\"\"\n        if dry_run:\n            self.logger.info(f\"Dry run: Model '{model_name}' would be called with prompt:\\n{prompt}\")\n            return \"DRY_RUN_RESPONSE\"\n\n        try:\n            # Create model instance\n            model = genai.GenerativeModel(model_name)\n            \n            # Construct prompt with system prompt if provided\n            full_prompt = prompt\n            if system_prompt:\n                full_prompt = f\"System: {system_prompt}\\n\\nUser: {prompt}\"\n\n            response = model.generate_content(full_prompt, **kwargs)\n            \n            if response.text is None:\n                raise ModelCallError(f\"Model '{model_name}' returned no text response.\")\n            \n            return response.text\n\n        except Exception as e:\n            self.logger.error(\"Failed to call model '%s': %s\", model_name, e)\n            raise ModelCallError(f\"Failed to call model '{model_name}': {e}\")\n\n\n",
  "todos": [
    {
      "file_path": "src/ai_self_ext_engine/model_client.py",
      "change_type": "modify",
      "description": "Refactor the existing model client to support asynchronous API calls, enabling parallel processing of AI requests."
    }
  ],
  "patch": "--- a/src/ai_self_ext_engine/model_client.py\n+++ b/src/ai_self_ext_engine/model_client.py\n@@ -31,7 +31,7 @@\n             self.logger.error(\"Error configuring Gemini API: %s\", e)\n             raise ValueError(f\"Error configuring Gemini API: {e}\")\n \n-    def call_model(\n+    async def call_model(\n         self,\n         model_name: str,\n         prompt: str,\n@@ -40,7 +40,7 @@\n         **kwargs # For any other model-specific parameters\n     ) -> str:\n         \"\"\"\n-        Makes a call to the specified Gemini model with prompt and system prompt.\n+        Makes an asynchronous call to the specified Gemini model with prompt and system prompt.\n         \"\"\"\n         if dry_run:\n             self.logger.info(f\"Dry run: Model '{model_name}' would be called with prompt:\\n{prompt}\")\n@@ -54,7 +54,7 @@\n             if system_prompt:\n                 full_prompt = f\"System: {system_prompt}\\n\\nUser: {prompt}\"\n \n-            response = model.generate_content(full_prompt, **kwargs)\n+            response = await model.generate_content_async(full_prompt, **kwargs)\n             \n             if response.text is None:\n                 raise ModelCallError(f\"Model '{model_name}' returned no text response.\")",
  "test_results": null,
  "accepted": false,
  "should_abort": true,
  "metadata": {}
}