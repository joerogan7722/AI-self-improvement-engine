--- /dev/null
+++ b/src/ai_self_ext_engine/roles/self_analysis.py
@@ -0,0 +1,86 @@
+import logging
+from pathlib import Path
+from typing import List, Dict, Any, Optional
+import json # To parse snapshot files
+
+from ai_self_ext_engine.core.role import Role, Context
+from ai_self_ext_engine.model_client import ModelClient, ModelCallError
+from ai_self_ext_engine.config import MainConfig
+from ai_self_ext_engine.snapshot_store import SnapshotStore # To load historical data
+
+logger = logging.getLogger(__name__)
+
+class SelfAnalysisRole(Role):
+    """
+    Role responsible for analyzing past performance data for the current goal
+    to provide insights for subsequent improvement attempts.
+    """
+    def __init__(self, config: MainConfig, model_client: ModelClient):
+        self.config = config
+        self.model_client = model_client
+        # Instantiate SnapshotStore here to access historical data for the current goal
+        self.snapshot_store = SnapshotStore(config.engine.memory_path)
+        # This role might use a prompt template for more advanced analysis later, but not strictly needed for basic summary.
+        # self.prompt_template_path = Path(config.engine.prompts_dir) / "self_analysis.tpl"
+
+    def run(self, context: Context) -> Context:
+        if not context.goal:
+            logger.info("SelfAnalysisRole: No goal in context. Skipping.")
+            return context
+
+        logger.info("SelfAnalysisRole: Analyzing past performance for goal '%s'...", context.goal.goal_id)
+
+        # Load all snapshots for the current goal
+        goal_snapshots_dir = Path(self.config.engine.memory_path) / context.goal.goal_id
+        historical_data: List[Dict[str, Any]] = []
+
+        if goal_snapshots_dir.exists():
+            # Sort files by modification time to get chronological order of attempts
+            snapshot_files = sorted(
+                [f for f in goal_snapshots_dir.iterdir() if f.suffix == ".json"],
+                key=lambda f: f.stat().st_mtime
+            )
+
+            for snapshot_file in snapshot_files:
+                try:
+                    with open(snapshot_file, 'r', encoding='utf-8') as f:
+                        data = json.load(f)
+                        # Only consider snapshots from previous attempts, not the current one being built
+                        if data.get("cycle") is not None and data["cycle"] < context.metadata.get("current_attempt", 0):
+                            historical_record = {
+                                "cycle": data.get("cycle"),
+                                "patch_generated": bool(data.get("patch")),
+                                "test_results": data.get("test_results"),
+                                "accepted": data.get("accepted"),
+                                "should_abort": data.get("should_abort"),
+                                "todos": data.get("todos", []),
+                                "timestamp": data.get("metadata", {}).get("timestamp")
+                            }
+                            historical_data.append(historical_record)
+                except Exception as e:
+                    logger.warning(f"SelfAnalysisRole: Could not load snapshot from {snapshot_file}: {e}")
+
+        # If this is the first attempt or no relevant historical data, return early
+        if not historical_data:
+            logger.info("SelfAnalysisRole: No relevant historical data found for goal '%s'.", context.goal.goal_id)
+            context.historical_insights = {"summary": "No previous attempts for this goal with available data."}
+            return context
+
+        # Analyze historical data to create insights
+        insights = self._analyze_history(historical_data)
+        
+        context.historical_insights = insights
+        logger.info("SelfAnalysisRole: Generated historical insights: %s", insights)
+
+        return context
+
+    def _analyze_history(self, history: List[Dict[str, Any]]) -> Dict[str, Any]:
+        """
+        Analyzes the historical snapshots to generate insights for the LLM.
+        """
+        if not history:
+            return {"summary_for_llm": "No previous attempts."}
+
+        last_attempt = history[-1]
+        summary_parts = [f"Previous attempts for this goal: {len(history)}."]
+        summary_parts.append(f"The most recent completed attempt (Cycle {last_attempt.get('cycle')}):")
+        summary_parts.append(f"- Outcome: {'Accepted' if last_attempt.get('accepted') else 'Rejected'}.")
+        if last_attempt.get('test_results') is not None:
+            summary_parts.append(f"- Tests: {'Passed' if last_attempt['test_results'].get('passed') else 'Failed'} (Return Code: {last_attempt['test_results'].get('returncode')}).")
+        if last_attempt.get('should_abort'):
+            summary_parts.append("- The attempt was aborted by a previous role.")
+        
+        # Add a note on common failure reasons
+        failure_reasons = set()
+        for attempt in history:
+            if not attempt.get("accepted"):
+                if attempt.get("test_results", {}).get("passed") is False:
+                    failure_reasons.add("Test failures.")
+                elif attempt.get("should_abort"):
+                    failure_reasons.add("Aborted by a role (e.g., model error, patch application failure).")
+                elif not attempt.get("patch_generated"):
+                    failure_reasons.add("Failure to generate a valid patch.")
+
+        if failure_reasons:
+            summary_parts.append("\nCommon issues observed in past attempts:")
+            summary_parts.extend([f"- {reason}" for reason in sorted(list(failure_reasons))])
+
+        return {
+            "summary_for_llm": "\n".join(summary_parts),
+            "raw_historical_data": history # Keep raw data for debugging/more detailed analysis if needed
+        }
+
--- a/src/ai_self_ext_engine/config.py
+++ b/src/ai_self_ext_engine/config.py
@@ -49,3 +49,4 @@
 
     class Config:
         validate_by_name = True # Allow 'class' to be used in RoleConfig
+
--- a/src/ai_self_ext_engine/config/config.py
+++ b/src/ai_self_ext_engine/config/config.py
@@ -16,6 +16,9 @@
   - module: ai_self_ext_engine.roles.problem_identification
     class: ProblemIdentificationRole
     prompt_path: problem_identification.tpl
+  - module: ai_self_ext_engine.roles.self_analysis
+    class: SelfAnalysisRole
+    prompt_path: N/A # SelfAnalysisRole does not directly use a prompt template for its primary function
   - module: ai_self_ext_engine.roles.refine
     class: RefineRole
     prompt_path: patch_generation.tpl
--- a/src/ai_self_ext_engine/core/role.py
+++ b/src/ai_self_ext_engine/core/role.py
@@ -15,6 +15,7 @@
     test_results: Optional[Any] = None # Will be a TestResults object
     accepted: bool = False
     should_abort: bool = False
+    historical_insights: Optional[Dict[str, Any]] = field(default_factory=dict) # New: Insights from past performance
     metadata: Dict[str, Any] = field(default_factory=dict) # For logging additional info
 
 class Role(ABC):
--- a/src/ai_self_ext_engine/roles/refine.py
+++ b/src/ai_self_ext_engine/roles/refine.py
@@ -35,9 +35,16 @@
             
             prompt_template = self.prompt_template_path.read_text(encoding="utf-8")
             
+            # Prepare historical insights for the prompt if available
+            historical_insight_str = ""
+            if context.historical_insights and "summary_for_llm" in context.historical_insights:
+                historical_insight_str = f"\n\nHistorical Context/Insights from Previous Attempts:\n{context.historical_insights['summary_for_llm']}\n"
+                logger.debug("RefineRole: Including historical insights in prompt.")
+
             prompt = prompt_template.format(
                 current_code=current_code,
-                todos="\n".join([f"- {todo}" for todo in context.todos])
+                todos="\n".join([f"- {todo}" for todo in context.todos]),
+                historical_insights=historical_insight_str # Pass the prepared string
             )
             # The LLM is expected to output only the patch. We might need to strip leading/trailing text.
             patch = self.model_client.call_model(
--- a/src/ai_self_ext_engine/roles/__init__.py
+++ b/src/ai_self_ext_engine/roles/__init__.py
@@ -2,9 +2,11 @@
 
 from .problem_identification import ProblemIdentificationRole
 from .refine import RefineRole
+from .self_analysis import SelfAnalysisRole
 from .test import TestRole
 from .self_review import SelfReviewRole
 
 # Expose all roles for dynamic loading
 __all__ = [
     "ProblemIdentificationRole",
     "RefineRole",
+    "SelfAnalysisRole",
     "TestRole",
     "SelfReviewRole",
 ]